{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7080f48-c74c-46f6-8666-673f877b030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import pipeline\n",
    "import datasets\n",
    "import torch\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import psutil\n",
    "from loguru import logger\n",
    "# import ray\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86dd3b1b-8053-459d-be9f-5f20ade01ad3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/data/inet-large-scale-twitter-diffusion/ball4321/data_b/01_raw/data_cleaned_bt_split\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/inet-large-scale-twitter-diffusion/ball4321/venv_legalpythia/lib/python3.11/site-packages/datasets/load.py:2248\u001b[0m, in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_path, fs, keep_in_memory, storage_options)\u001b[0m\n\u001b[1;32m   2244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(path_join(dest_dataset_path, config\u001b[38;5;241m.\u001b[39mDATASET_INFO_FILENAME)) \u001b[38;5;129;01mand\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(\n\u001b[1;32m   2246\u001b[0m     path_join(dest_dataset_path, config\u001b[38;5;241m.\u001b[39mDATASET_STATE_JSON_FILENAME)\n\u001b[1;32m   2247\u001b[0m ):\n\u001b[0;32m-> 2248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(path_join(dest_dataset_path, config\u001b[38;5;241m.\u001b[39mDATASETDICT_JSON_FILENAME)):\n\u001b[1;32m   2250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict\u001b[38;5;241m.\u001b[39mload_from_disk(dataset_path, keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory, storage_options\u001b[38;5;241m=\u001b[39mstorage_options)\n",
      "File \u001b[0;32m/data/inet-large-scale-twitter-diffusion/ball4321/venv_legalpythia/lib/python3.11/site-packages/datasets/arrow_dataset.py:1709\u001b[0m, in \u001b[0;36mDataset.load_from_disk\u001b[0;34m(dataset_path, fs, keep_in_memory, storage_options)\u001b[0m\n\u001b[1;32m   1707\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(dataset_size)\n\u001b[1;32m   1708\u001b[0m table_cls \u001b[38;5;241m=\u001b[39m InMemoryTable \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;28;01melse\u001b[39;00m MemoryMappedTable\n\u001b[0;32m-> 1709\u001b[0m arrow_table \u001b[38;5;241m=\u001b[39m \u001b[43mconcat_tables\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtable_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_join\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdest_dataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_file\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfilename\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_data_files\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1714\u001b[0m split \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_split\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1715\u001b[0m split \u001b[38;5;241m=\u001b[39m Split(split) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m split\n",
      "File \u001b[0;32m/data/inet-large-scale-twitter-diffusion/ball4321/venv_legalpythia/lib/python3.11/site-packages/datasets/table.py:1799\u001b[0m, in \u001b[0;36mconcat_tables\u001b[0;34m(tables, axis)\u001b[0m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcat_tables\u001b[39m(tables: List[Table], axis: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Table:\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;124;03m    Concatenate tables.\u001b[39;00m\n\u001b[1;32m   1785\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1797\u001b[0m \u001b[38;5;124;03m            Otherwise if there's only one table, it is returned as is.\u001b[39;00m\n\u001b[1;32m   1798\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1799\u001b[0m     tables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tables) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1801\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tables[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/data/inet-large-scale-twitter-diffusion/ball4321/venv_legalpythia/lib/python3.11/site-packages/datasets/arrow_dataset.py:1710\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1707\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(dataset_size)\n\u001b[1;32m   1708\u001b[0m table_cls \u001b[38;5;241m=\u001b[39m InMemoryTable \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;28;01melse\u001b[39;00m MemoryMappedTable\n\u001b[1;32m   1709\u001b[0m arrow_table \u001b[38;5;241m=\u001b[39m concat_tables(\n\u001b[0;32m-> 1710\u001b[0m     \u001b[43mtable_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_join\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdest_dataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_file\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfilename\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1711\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data_file \u001b[38;5;129;01min\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_data_files\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1712\u001b[0m )\n\u001b[1;32m   1714\u001b[0m split \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_split\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1715\u001b[0m split \u001b[38;5;241m=\u001b[39m Split(split) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m split\n",
      "File \u001b[0;32m/data/inet-large-scale-twitter-diffusion/ball4321/venv_legalpythia/lib/python3.11/site-packages/datasets/table.py:1059\u001b[0m, in \u001b[0;36mMemoryMappedTable.from_file\u001b[0;34m(cls, filename, replays)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_file\u001b[39m(\u001b[38;5;28mcls\u001b[39m, filename: \u001b[38;5;28mstr\u001b[39m, replays\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1059\u001b[0m     table \u001b[38;5;241m=\u001b[39m \u001b[43m_memory_mapped_arrow_table_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1060\u001b[0m     table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_replays(table, replays)\n\u001b[1;32m   1061\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(table, filename, replays)\n",
      "File \u001b[0;32m/data/inet-large-scale-twitter-diffusion/ball4321/venv_legalpythia/lib/python3.11/site-packages/datasets/table.py:66\u001b[0m, in \u001b[0;36m_memory_mapped_arrow_table_from_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_memory_mapped_arrow_table_from_file\u001b[39m(filename: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pa\u001b[38;5;241m.\u001b[39mTable:\n\u001b[1;32m     65\u001b[0m     opened_stream \u001b[38;5;241m=\u001b[39m _memory_mapped_record_batch_reader_from_file(filename)\n\u001b[0;32m---> 66\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[43mopened_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pa_table\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = datasets.load_from_disk('/data/inet-large-scale-twitter-diffusion/ball4321/data_b/01_raw/data_cleaned_bt_split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f96112-bcac-4d27-be91-e7361ae9291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acd981ba-de6c-4800-876e-c67c3d317c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/inet-large-scale-twitter-diffusion/ball4321/data_b/04_models/roberta-large-NER were not used when initializing XLMRobertaForTokenClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    task = 'ner',\n",
    "    model='/data/inet-large-scale-twitter-diffusion/ball4321/data_b/04_models/roberta-large-NER',\n",
    "    aggregation_strategy='simple',\n",
    "    batch_size=250,\n",
    "    device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d528558d-79e5-48ac-b68f-5523d565cf23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['News broke this week of the surrender of Dominic Ongwen, a top commander in the Lord’s Resistance Army, to US forces in the Central African Republic.',\n",
       " 'One of the five LRA commanders indicted by the ICC in 2005, Ongwen is also the most controversial of the Uganda indictments.',\n",
       " 'While his surrender deals a serious blow to the LRA, whether the LRA’s victims will see justice remains unclear.',\n",
       " 'Ongwen was a child when he was abducted by the LRA and grew up to become a notorious killer.',\n",
       " 'But what are the legal and moral implications of trying a man for crimes against humanity when that man was conscripted as a child?',\n",
       " 'The most notable aspect of the conflict is the LRA’s widespread use of abducted children as child soldiers; Ongwen himself is one of these child abductees.',\n",
       " 'Abducted by the LRA in 1990 at the age of 10 while walking to school, over the years he rose through the ranks of LRA, ultimately becoming one of the senior commanders of the rebel group.',\n",
       " 'There is little question that he ordered and participated in the murder, rape, torture and abduction of civilians over the last 24 years but critics of his indictment also point to the physical and psychological process the LRA uses – finely tuned after decades of practice – to turn ordinary children into weapons of war.',\n",
       " 'Under international law, any acts committed by child soldiers prior to their 18th birthday are not eligible for prosecution but for those who do not escape until they are adults, the law holds them responsible for their acts regardless of how they came to be fighters.',\n",
       " 'This is true for all conflicts that use child soldiers, but Ongwen’s case is the most high profile with the involvement of the highest criminal court in the world.',\n",
       " 'As a victim of the same crimes he is now indicted for, many feel Ongwen should not be held responsible at the same level as other LRA leaders.',\n",
       " 'The Justice and Reconciliation Project released a report in 2008 that summed up the problems with accepting Ongwen’s responsibility while ignoring the events and trauma that led to his transformation, concluding that international justice may not be the mechanism best suited to address such a complicated narrative.',\n",
       " 'That Ongwen was forced into war crimes for eight years before he gained his first leadership role at 18 does not matter legally, but sits uneasily with human rights and child soldier advocates.',\n",
       " 'It also sits uneasily with many Ugandans who still see former LRA fighters as abused children despite their criminal actions.',\n",
       " 'Most surveys show Ugandans want LRA fighters to be held accountable in some way for their actions, but there is also widespread support for the blanket amnesty passed by Uganda in 2000.',\n",
       " 'This may complicate things for the ICC who is already reeling from several recent developments, including the collapse of the court’s case against Kenyan president Uhuru Kenyatta and the official hibernation of the investigation into atrocities committed in Darfur.',\n",
       " 'Of all the LRA commanders indicted by the court, Ongwen has always been the most contentious and raises complicated legal and moral questions with no easy answers for the court.',\n",
       " 'Politically there is also the question of how the Ugandan government will respond.',\n",
       " 'Although Uganda was the first state to refer a country – itself – to the ICC in 2003, relations have soured in recent years.',\n",
       " 'The involvement of the ICC became a sticking point during Juba peace talks with the Ugandan government and the LRA in 2008, as the LRA demanded full amnesty from the ICC.',\n",
       " 'Although it will never be known what would have happened if the ICC agreed to suspend the cases, the collapse of the peace talks in the face of non-cooperation by the court created the appearance of the ICC choosing to prolong the conflict in Northern Uganda to preserve its own principles at the detriment of Ugandans.',\n",
       " 'It also created the perfect political target for Museveni as the conflict continued.',\n",
       " 'As a result, Museveni is now one of the most outspoken critics of the court, especially regarding the court’s alleged African-bias.',\n",
       " 'Last month, he declared his relations with the court over and called for a mass departure from the court by African governments, even after the ICC dropped the controversial case against Kenyatta.',\n",
       " 'Even though it remains a member of the court, Uganda is in no mood to cooperate with the ICC.',\n",
       " 'Unlike with Bosco Ntaganda, the Congolese rebel leader who was turned over to the ICC after surrendering to the US embassy in Kigali in 2013, US forces opted to turn over Ongwen to Ugandan officials.',\n",
       " 'That means the decision of what will happen to Ongwen will rest with Uganda, who has not confirmed that the open ICC arrest warrant against him will be honored.',\n",
       " 'Instead, Uganda could choose to prosecute Ongwen in its own courts, or even grant him amnesty under the Amnesty Act of 2000.',\n",
       " 'All of this leads to numerous unanswered questions about what the surrender of the first LRA leader will mean for Uganda and the ICC as the first situation investigated by the court finally bears a defendant.',\n",
       " 'As the conflict continues to drag on, gaining justice for the LRA’s victims may be possible here, but it will be far from an easy task to achieve.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:30]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc74d71f-cebb-4f3f-9410-733307407fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = ['News broke this week of the surrender of Dominic Ongwen, a top commander in the Lord’s Resistance Army, to US forces in the Central African Republic.',\n",
    " 'One of the five LRA commanders indicted by the ICC in 2005, Ongwen is also the most controversial of the Uganda indictments.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b970f8b-02e9-4e84-a1b6-d4d98feb9659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# out = pipe(data[0:10]['text'])\n",
    "out = pipe(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db9361ba-aaa5-49df-b6ff-ab332e8422e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9999879,\n",
       "  'word': 'Dominic Ongwen',\n",
       "  'start': 41,\n",
       "  'end': 55},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9999915,\n",
       "  'word': 'Lord’s Resistance Army',\n",
       "  'start': 80,\n",
       "  'end': 102},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.9921078,\n",
       "  'word': 'US',\n",
       "  'start': 107,\n",
       "  'end': 109},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.99999636,\n",
       "  'word': 'Central African Republic',\n",
       "  'start': 124,\n",
       "  'end': 148}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2d033f8-0fa0-455e-a459-fd229aaf88f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42909472"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdbe8acf-9911-4e76-b143-e8c477ce13a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2461379/3146181148.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_start in tqdm(range(0,1000000,batch_size)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "144dc135a1334d36b8f6388b798a1376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m batch_end \u001b[38;5;241m=\u001b[39m batch_start\u001b[38;5;241m+\u001b[39mbatch_size\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# print(batch_start, batch_end)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m out\u001b[38;5;241m.\u001b[39mextend(\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_start\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_end\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/data/inet-large-scale-twitter-diffusion/ball4321/venv_legalpythia/lib/python3.11/site-packages/transformers/pipelines/token_classification.py:249\u001b[0m, in \u001b[0;36mTokenClassificationPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offset_mapping:\n\u001b[1;32m    247\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffset_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m offset_mapping\n\u001b[0;32m--> 249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/inet-large-scale-twitter-diffusion/ball4321/venv_legalpythia/lib/python3.11/site-packages/transformers/pipelines/base.py:1121\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1118\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1119\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1120\u001b[0m     )\n\u001b[0;32m-> 1121\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(final_iterator)\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/data/inet-large-scale-twitter-diffusion/ball4321/venv_legalpythia/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m/data/inet-large-scale-twitter-diffusion/ball4321/venv_legalpythia/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:266\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[0;32m--> 266\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m/data/inet-large-scale-twitter-diffusion/ball4321/venv_legalpythia/lib/python3.11/site-packages/transformers/pipelines/base.py:1047\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1045\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   1046\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m-> 1047\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_tensor_on_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFramework \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/data/inet-large-scale-twitter-diffusion/ball4321/venv_legalpythia/lib/python3.11/site-packages/transformers/pipelines/base.py:948\u001b[0m, in \u001b[0;36mPipeline._ensure_tensor_on_device\u001b[0;34m(self, inputs, device)\u001b[0m\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ModelOutput(\n\u001b[1;32m    945\u001b[0m         {name: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(tensor, device) \u001b[38;5;28;01mfor\u001b[39;00m name, tensor \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    946\u001b[0m     )\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 948\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m{\u001b[49m\u001b[43mname\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_tensor_on_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, UserDict):\n\u001b[1;32m    950\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserDict({name: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(tensor, device) \u001b[38;5;28;01mfor\u001b[39;00m name, tensor \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()})\n",
      "File \u001b[0;32m/data/inet-large-scale-twitter-diffusion/ball4321/venv_legalpythia/lib/python3.11/site-packages/transformers/pipelines/base.py:948\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ModelOutput(\n\u001b[1;32m    945\u001b[0m         {name: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(tensor, device) \u001b[38;5;28;01mfor\u001b[39;00m name, tensor \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    946\u001b[0m     )\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 948\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {name: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_tensor_on_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m name, tensor \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, UserDict):\n\u001b[1;32m    950\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserDict({name: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(tensor, device) \u001b[38;5;28;01mfor\u001b[39;00m name, tensor \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()})\n",
      "File \u001b[0;32m/data/inet-large-scale-twitter-diffusion/ball4321/venv_legalpythia/lib/python3.11/site-packages/transformers/pipelines/base.py:958\u001b[0m, in \u001b[0;36mPipeline._ensure_tensor_on_device\u001b[0;34m(self, inputs, device)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01min\u001b[39;00m {torch\u001b[38;5;241m.\u001b[39mfloat16, torch\u001b[38;5;241m.\u001b[39mbfloat16}:\n\u001b[1;32m    957\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m--> 958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    960\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "out = []\n",
    "\n",
    "batch_size=250\n",
    "\n",
    "for batch_start in tqdm(range(0,1000000,batch_size)):\n",
    "    batch_end = batch_start+batch_size\n",
    "    # print(batch_start, batch_end)\n",
    "\n",
    "    out.extend(pipe(data[batch_start:batch_end]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1de24699-18d4-4d7b-8178-fd42e4fb7add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.99998134,\n",
       "  'word': 'Carina Coetzee',\n",
       "  'start': 11,\n",
       "  'end': 25},\n",
       " {'entity_group': 'PER',\n",
       "  'score': 0.99999124,\n",
       "  'word': 'Judy Sheehan',\n",
       "  'start': 37,\n",
       "  'end': 49}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afa0c48-4275-49c9-94e3-1d9596c22a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Ray cluster\n",
    "ray.init(num_cpus=psutil.cpu_count(logical=True), ignore_reinit_error=True)\n",
    "\n",
    "\"\"\"\n",
    "The command ray.put(x) would be run by a worker process or by the driver process (the driver process is the one running your script). \n",
    "It takes a Python object and copies it to the local object store (here local means on the same node). \n",
    "Once the object has been stored in the object store, its value cannot be changed.\n",
    "In addition, ray.put(x) returns an object ID, which is essentially an ID that can be used to refer to the newly created remote object. \n",
    "If we save the object ID in a variable with x_id = ray.put(x), \n",
    "then we can pass x_id into remote functions, \n",
    "and those remote functions will operate on the corresponding remote object.\n",
    "\"\"\"\n",
    "pipe_id = ray.put(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942b824e-8968-4c84-84ba-69137abb0f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ray.remote decorator enables to use this \n",
    "# function in distributed setting\n",
    "@ray.remote\n",
    "def predict(pipeline, text_data, label_names):\n",
    "    return pipeline(text_data, label_names)\n",
    "\n",
    "# Predict multipe texts on all available CPUs and time the inference duration\n",
    "start = time.time()\n",
    "\n",
    "# Run the function using multiple cores and gather the results\n",
    "predictions = ray.get([predict.remote(pipe_id, text, ['motorcycle', 'baseball']) for text in test_data])\n",
    "\n",
    "end = time.time()\n",
    "print('Prediction time:', str(timedelta(seconds=end-start)))\n",
    "\n",
    "# Stop running Ray cluster\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61dba09-0db1-482e-ad06-2b03fcc2267a",
   "metadata": {},
   "source": [
    "## NER Collate Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa7a0421-8e50-48ae-976a-a7607c7a7e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_number(filename):\n",
    "    # use only last part of filename\n",
    "    filename=Path(filename).stem\n",
    "    # Extract the number using regular expressions (regex)\n",
    "    match = re.search(r'\\d+', filename)\n",
    "    # If a number is found, return it as an integer\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42cb740f-9819-4517-8244-33154f48b6fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_number('/data/inet-large-scale-twitter-diffusion/ball4321/data_c/YT/raw_ner/raw_ner_0-999999.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "147460b9-cee8-4853-8079-aaddc23878b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'raw_ner_0-999999'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path('/data/inet-large-scale-twitter-diffusion/ball4321/data_c/YT/raw_ner/raw_ner_0-999999.pkl').stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "807cc9d4-7ef2-47e8-9528-c3433bd7f1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-03 13:46:58.676\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mProcessing 0 of 12: 0.0%\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e8aac24d04d4ad08258e0fe54b93b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-03 13:47:05.402\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mProcessing 1 of 12: 8.3%\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "007b65cca31641c0a4ffa5d1f32ee510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-03 13:47:29.001\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mProcessing 2 of 12: 16.7%\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "629de995e88648ed92ffd00994c723a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-03 13:47:49.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mProcessing 3 of 12: 25.0%\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d08d62e6cc97421c8f10c72d7b5efb2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-03 13:48:13.846\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mProcessing 4 of 12: 33.3%\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5716269e86e499888c1d5317460bb93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-03 13:48:43.179\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mProcessing 5 of 12: 41.7%\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c35c98a1739b46f2b7f1512178f332c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-03 13:49:07.434\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mProcessing 6 of 12: 50.0%\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77509d85e85a4be6a40a6c56f3375e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-03 13:49:32.523\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mProcessing 7 of 12: 58.3%\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a8234a5f8cd4aafae2e8652a3fd6fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-03 13:50:04.113\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mProcessing 8 of 12: 66.7%\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88c0cc92fe3496c96e724846c2e2802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-03 13:50:30.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mProcessing 9 of 12: 75.0%\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17da51d34b064ceb85d89938a5093f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-03 13:51:02.560\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mProcessing 10 of 12: 83.3%\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b25ce50faee49498ae2c3a640a2041b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-03 13:51:29.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mProcessing 11 of 12: 91.7%\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "950cf697cab6415f862e509dc08f591d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1909472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filenames = glob.glob(os.path.join('/data/inet-large-scale-twitter-diffusion/ball4321/data_c/YT/raw_ner/', 'raw_ner_*.pkl'))\n",
    "\n",
    "# function to sort glob data\n",
    "def extract_number(filename):\n",
    "    # use only last part of filename\n",
    "    filename=Path(filename).stem\n",
    "    # Extract the number using regular expressions (regex)\n",
    "    match = re.search(r'\\d+', filename)\n",
    "    # If a number is found, return it as an integer\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Sort the filenames based on the number\n",
    "filenames_sorted = sorted(filenames, key=extract_number)\n",
    "\n",
    "combined_result = defaultdict(set)\n",
    "for counter, file in enumerate(filenames_sorted):\n",
    "    # if counter % 100 == 0:\n",
    "    logger.info(f'Processing {counter} of {len(filenames_sorted)}: {100*counter/len(filenames_sorted):.1f}%')\n",
    "    with open(file,'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    for result, part_id in tqdm(data):\n",
    "        processed_stories_id = part_id.split('_')[0]\n",
    "        for item in result:\n",
    "            if item['entity_group']=='PER':\n",
    "                combined_result[processed_stories_id].add(item['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "93bcb122-f377-4882-9251-750172b772af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-03 13:57:50.712\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mSaved to ner_id_to_ent.pkl\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "outpath = 'ner_id_to_ent.pkl'\n",
    "\n",
    "with open(outpath, 'wb') as f:\n",
    "    pickle.dump(combined_result, f)\n",
    "\n",
    "logger.info(f'Saved to {outpath}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "324de27f-68ed-4f8f-946d-f5c68edc73a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_dict = {}\n",
    "\n",
    "# Iterating over the original dictionary\n",
    "for key, values in combined_result.items():\n",
    "    for value in values:\n",
    "        # Append the key to the list of keys for this value\n",
    "        if value in inverted_dict:\n",
    "            inverted_dict[value].append(key)\n",
    "        else:\n",
    "            inverted_dict[value] = [key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "acd978ab-e09d-4dbe-992d-12e2e521668b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "820798"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ef776a2b-16f4-4764-8b4a-183396e1c711",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-03 14:06:13.564\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mSaved to ner_ent_to_id.pkl\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "outpath = 'ner_ent_to_id.pkl'\n",
    "\n",
    "with open(outpath, 'wb') as f:\n",
    "    pickle.dump(inverted_dict, f)\n",
    "\n",
    "logger.info(f'Saved to {outpath}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ccf20e4-1e80-4e73-9fe2-d6d90e98b1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting thefuzz\n",
      "  Downloading thefuzz-0.20.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.0.0 (from thefuzz)\n",
      "  Downloading rapidfuzz-3.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Downloading thefuzz-0.20.0-py3-none-any.whl (15 kB)\n",
      "Downloading rapidfuzz-3.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, thefuzz\n",
      "Successfully installed rapidfuzz-3.6.1 thefuzz-0.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip install thefuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c4b18c-b2a2-42c5-9b9a-c4fe7c5028b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
