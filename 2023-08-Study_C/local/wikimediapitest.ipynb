{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import os\n",
    "from collections import defaultdict, Counter\n",
    "import pickle\n",
    "\n",
    "with open('wmapicred.json', 'r') as f:\n",
    "    credentials = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pageid_strings(dict_list, max_count=50, max_length=300):\n",
    "    combined_string = ''\n",
    "    count = 0\n",
    "\n",
    "    for item in dict_list:\n",
    "        pageid = str(item['pageid'])\n",
    "        if combined_string:\n",
    "            # Check if adding this pageid will exceed the length limit or count limit\n",
    "            if len(combined_string) + len(pageid) + 1 > max_length or count >= max_count:\n",
    "                yield combined_string\n",
    "                combined_string = pageid\n",
    "                count = 1  # Reset count for the new string\n",
    "            else:\n",
    "                combined_string += '|' + pageid\n",
    "                count += 1\n",
    "        else:\n",
    "            combined_string = pageid\n",
    "            count = 1\n",
    "\n",
    "    # Yield the last combined string if it's not empty\n",
    "    if combined_string:\n",
    "        yield combined_string\n",
    "\n",
    "\n",
    "def generate_titles_strings(inlist, max_count=50, max_length=300):\n",
    "    combined_string = ''\n",
    "    count = 0\n",
    "\n",
    "    for title in inlist:\n",
    "        if combined_string:\n",
    "            # Check if adding this pageid will exceed the length limit or count limit\n",
    "            if len(combined_string) + len(title) + 1 > max_length or count >= max_count:\n",
    "                yield combined_string\n",
    "                combined_string = title\n",
    "                count = 1  # Reset count for the new string\n",
    "            else:\n",
    "                combined_string += '|' + title\n",
    "                count += 1\n",
    "        else:\n",
    "            combined_string = title\n",
    "            count = 1\n",
    "\n",
    "    # Yield the last combined string if it's not empty\n",
    "    if combined_string:\n",
    "        yield combined_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3\n",
    "\n",
    "# Get today's date in YYYY/MM/DD format.\n",
    "\n",
    "today = datetime.datetime.now()\n",
    "date = today.strftime('%Y/%m/%d')\n",
    "\n",
    "# Choose your language, and get today's featured content.\n",
    "language_code = 'en' # English\n",
    "headers = {\n",
    "  'Authorization': f'Bearer {credentials[\"access_token\"]}',\n",
    "  # 'User-Agent': 'YOUR_APP_NAME (YOUR_EMAIL_OR_CONTACT_PAGE)'\n",
    "}\n",
    "\n",
    "base_url = 'https://api.wikimedia.org/feed/v1/wikipedia/'\n",
    "url = base_url + language_code + '/featured/' + date\n",
    "response = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = requests.Session()\n",
    "\n",
    "URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "SEARCHPAGE = \"#MeToo\"\n",
    "\n",
    "PARAMS = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"list\": \"search\",\n",
    "    \"srsearch\": SEARCHPAGE,\n",
    "    \"srlimit\": 500,\n",
    "    \"sroffset\": 0\n",
    "}\n",
    "\n",
    "R = S.get(url=URL, params=PARAMS)\n",
    "DATA = R.json()\n",
    "\n",
    "# if DATA['query']['search'][0]['title'] == SEARCHPAGE:\n",
    "    # print(\"Your search page '\" + SEARCHPAGE + \"' exists on English Wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA['query']['searchinfo']['totalhits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pagination\n",
    "S = requests.Session()\n",
    "\n",
    "URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "SEARCHPAGE = \"#MeToo\"\n",
    "PARAMS = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"list\": \"search\",\n",
    "    \"srsearch\": SEARCHPAGE,\n",
    "    \"srlimit\": 500,\n",
    "    \"sroffset\": 0\n",
    "}\n",
    "TO_CONTINUE = True\n",
    "RESULTS = []\n",
    "\n",
    "while TO_CONTINUE:\n",
    "    print(f\"Processing Offset Num {PARAMS['sroffset']}\")\n",
    "    R = S.get(url=URL, params=PARAMS)\n",
    "    DATA = R.json()\n",
    "    RESULTS.extend(DATA['query']['search'])\n",
    "    if 'error' in DATA:\n",
    "        raise Exception(DATA['error'])\n",
    "    if 'warnings' in DATA:\n",
    "        print(DATA['warnings'])\n",
    "    if 'continue' not in DATA:\n",
    "        break\n",
    "    elif '||' in DATA['continue']['continue']:\n",
    "        TO_CONTINUE=True\n",
    "        PARAMS['sroffset']=DATA['continue']['sroffset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = '/Users/hubert/Drive/DPhil/DPhil_Studies/2023-08-Study_C/Data/Wikipedia/metoo_base.json'\n",
    "\n",
    "with open(savepath, 'w') as file:\n",
    "    json.dump(RESULTS, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Query with changed sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srsorts = [\n",
    "    'create_timestamp_asc',\n",
    "    'create_timestamp_desc',\n",
    "    'incoming_links_asc',\n",
    "    'incoming_links_desc',\n",
    "    'just_match',\n",
    "    'last_edit_asc',\n",
    "    'last_edit_desc',\n",
    "    'none',\n",
    "    'random',\n",
    "    'relevance',\n",
    "    'user_random'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = []\n",
    "\n",
    "# with pagination\n",
    "S = requests.Session()\n",
    "\n",
    "for srsort in srsorts:\n",
    "    print(f'SORT TYPE: {srsort}')\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "    SEARCHPAGE = \"#MeToo\"\n",
    "    PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"list\": \"search\",\n",
    "        \"srsearch\": SEARCHPAGE,\n",
    "        \"srlimit\": 500,\n",
    "        \"sroffset\": 0,\n",
    "        \"srsort\": srsort\n",
    "    }\n",
    "    TO_CONTINUE = True\n",
    "\n",
    "    while TO_CONTINUE:\n",
    "        print(f\"Processing Offset Num {PARAMS['sroffset']}\")\n",
    "        R = S.get(url=URL, params=PARAMS)\n",
    "        DATA = R.json()\n",
    "        RESULTS.extend(DATA['query']['search'])\n",
    "        if 'error' in DATA:\n",
    "            raise Exception(DATA['error'])\n",
    "        if 'warnings' in DATA:\n",
    "            print(DATA['warnings'])\n",
    "        if 'continue' not in DATA:\n",
    "            break\n",
    "        elif '||' in DATA['continue']['continue']:\n",
    "            TO_CONTINUE=True\n",
    "            PARAMS['sroffset']=DATA['continue']['sroffset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard duplicate results\n",
    "def remove_duplicates(dict_list):\n",
    "    unique_pageids = set()\n",
    "    unique_dicts = []\n",
    "\n",
    "    for d in dict_list:\n",
    "        pageid = d['pageid']\n",
    "        if pageid not in unique_pageids:\n",
    "            unique_pageids.add(pageid)\n",
    "            unique_dicts.append(d)\n",
    "\n",
    "    return unique_dicts\n",
    "\n",
    "\n",
    "RESULTS_DEDUPED = remove_duplicates(RESULTS)\n",
    "print(len(RESULTS))\n",
    "print(len(RESULTS_DEDUPED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = '/Users/hubert/Drive/DPhil/DPhil_Studies/2023-08-Study_C/Data/Wikipedia/metoo_base_multisort.json'\n",
    "\n",
    "with open(savepath, 'w') as file:\n",
    "    json.dump(RESULTS, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about just random?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = '/Users/hubert/Drive/DPhil/DPhil_Studies/2023-08-Study_C/Data/Wikipedia/metoo_base_multisort_random_aug.json'\n",
    "\n",
    "with open(savepath, 'r') as file:\n",
    "    INRES = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in max results collected\n",
    "\n",
    "counter = 0\n",
    "max_tries = 500\n",
    "while True:\n",
    "    print(f'Call number {counter}')\n",
    "    counter += 1\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "    SEARCHPAGE = \"#MeToo\"\n",
    "    PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"list\": \"search\",\n",
    "        \"srsearch\": SEARCHPAGE,\n",
    "        \"srlimit\": 500,\n",
    "        \"sroffset\": 0,\n",
    "        \"srsort\": 'random'\n",
    "    }\n",
    "    TO_CONTINUE = True\n",
    "\n",
    "    # while TO_CONTINUE:\n",
    "        # print(f\"Processing Offset Num {PARAMS['sroffset']}\")\n",
    "    R = S.get(url=URL, params=PARAMS)\n",
    "    DATA = R.json()\n",
    "    original_uniq = len(INRES)\n",
    "    INRES.extend(DATA['query']['search'])\n",
    "    INRES = remove_duplicates(INRES)\n",
    "    new_uniq = len(INRES)\n",
    "    if 'error' in DATA:\n",
    "        raise Exception(DATA['error'])\n",
    "    if 'warnings' in DATA:\n",
    "        print(DATA['warnings'])\n",
    "    print(f'Unique Count: Before - {original_uniq}; After - {new_uniq}; Diff - {new_uniq-original_uniq}')\n",
    "    time.sleep(2)\n",
    "    if counter >= max_tries or len(INRES) > 100234:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = '/Users/hubert/Drive/DPhil/DPhil_Studies/2023-08-Study_C/Data/Wikipedia/metoo_base_multisort_random_aug.json'\n",
    "\n",
    "with open(savepath, 'w') as file:\n",
    "    json.dump(INRES, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using workaround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_list = [\n",
    "    \"#MeToo intitle:/[A-G]/ -intitle:/[H-Z]/\",\n",
    "    \"#MeToo intitle:/[H-Z]/ -intitle:/[A-G]/\",\n",
    "    \"#MeToo intitle:/[A-G]/\",\n",
    "    \"#MeToo intitle:/[H-Z]/\",\n",
    "    \"#MeToo intitle:/[A-B]/\",\n",
    "    \"#MeToo intitle:/[B-C]/\",\n",
    "    \"#MeToo intitle:/[C-D]/\",\n",
    "    \"#MeToo intitle:/[D-E]/\",\n",
    "    \"#MeToo intitle:/[E-F]/\",\n",
    "    \"#MeToo intitle:/[F-G]/\",\n",
    "    \"#MeToo intitle:/[G-H]/\",\n",
    "    \"#MeToo intitle:/[H-I]/\",\n",
    "    \"#MeToo intitle:/[I-J]/\",\n",
    "    \"#MeToo intitle:/[J-K]/\",\n",
    "    \"#MeToo intitle:/[K-L]/\",\n",
    "    \"#MeToo intitle:/[L-M]/\",\n",
    "    \"#MeToo intitle:/[M-N]/\",\n",
    "    \"#MeToo intitle:/[N-O]/\",\n",
    "    \"#MeToo intitle:/[O-P]/\",\n",
    "    \"#MeToo intitle:/[P-Q]/\",\n",
    "    \"#MeToo intitle:/[Q-R]/\",\n",
    "    \"#MeToo intitle:/[R-S]/\",\n",
    "    \"#MeToo intitle:/[S-T]/\",\n",
    "    \"#MeToo intitle:/[T-U]/\",\n",
    "    \"#MeToo intitle:/[U-V]/\",\n",
    "    \"#MeToo intitle:/[V-W]/\",\n",
    "    \"#MeToo intitle:/[W-X]/\",\n",
    "    \"#MeToo intitle:/[X-Y]/\",\n",
    "    \"#MeToo intitle:/[Y-Z]/\",\n",
    "]\n",
    "\n",
    "individual_letters = [\n",
    "    \"#MeToo intitle:/[A]/\",\n",
    "    \"#MeToo intitle:/[B]/\",\n",
    "    \"#MeToo intitle:/[C]/\",\n",
    "    \"#MeToo intitle:/[D]/\",\n",
    "    \"#MeToo intitle:/[E]/\",\n",
    "    \"#MeToo intitle:/[F]/\",\n",
    "    \"#MeToo intitle:/[G]/\",\n",
    "    \"#MeToo intitle:/[H]/\",\n",
    "    \"#MeToo intitle:/[I]/\",\n",
    "    \"#MeToo intitle:/[J]/\",\n",
    "    \"#MeToo intitle:/[K]/\",\n",
    "    \"#MeToo intitle:/[L]/\",\n",
    "    \"#MeToo intitle:/[M]/\",\n",
    "    \"#MeToo intitle:/[N]/\",\n",
    "    \"#MeToo intitle:/[O]/\",\n",
    "    \"#MeToo intitle:/[P]/\",\n",
    "    \"#MeToo intitle:/[Q]/\",\n",
    "    \"#MeToo intitle:/[R]/\",\n",
    "    \"#MeToo intitle:/[S]/\",\n",
    "    \"#MeToo intitle:/[T]/\",\n",
    "    \"#MeToo intitle:/[U]/\",\n",
    "    \"#MeToo intitle:/[V]/\",\n",
    "    \"#MeToo intitle:/[W]/\",\n",
    "    \"#MeToo intitle:/[X]/\",\n",
    "    \"#MeToo intitle:/[Y]/\",\n",
    "    \"#MeToo intitle:/[Z]/\",\n",
    "]\n",
    "\n",
    "extended_query_list = [\n",
    "    \"#MeToo intitle:/[A]/ intitle:/[B-N]/ -intitle:/[O-Z]/\",\n",
    "    \"#MeToo intitle:/[A]/ -intitle:/[B-N]/ intitle:/[O-Z]/\",\n",
    "    \"#MeToo intitle:/[B]/ intitle:/[A,C-N]/ -intitle:/[O-Z]/\",\n",
    "    \"#MeToo intitle:/[B]/ -intitle:/[A,C-N]/ intitle:/[O-Z]/\",\n",
    "    \"#MeToo intitle:/[C]/ intitle:/[A-B,D-N]/ -intitle:/[O-Z]/\",\n",
    "    \"#MeToo intitle:/[C]/ -intitle:/[A-B,D-N]/ intitle:/[O-Z]/\",\n",
    "    \"#MeToo intitle:/[D]/ intitle:/[A-C,E-N]/ -intitle:/[O-Z]/\",\n",
    "    \"#MeToo intitle:/[D]/ -intitle:/[A-C,E-N]/ intitle:/[O-Z]/\",\n",
    "    \"#MeToo intitle:/[E]/ intitle:/[A-D,F-N]/ -intitle:/[O-Z]/\",\n",
    "    \"#MeToo intitle:/[E]/ -intitle:/[A-D,F-N]/ intitle:/[O-Z]/\",\n",
    "    \"#MeToo intitle:/[F]/ intitle:/[A-E,G-N]/ -intitle:/[O-Z]/\",\n",
    "    \"#MeToo intitle:/[F]/ -intitle:/[A-E,G-N]/ intitle:/[O-Z]/\",\n",
    "    \"#MeToo intitle:/[G]/ intitle:/[A-F,H-N]/ -intitle:/[O-Z]/\",\n",
    "    \"#MeToo intitle:/[G]/ -intitle:/[A-F,H-N]/ intitle:/[O-Z]/\",\n",
    "    \"#MeToo intitle:/[H]/ intitle:/[A-G,I-N]/ -intitle:/[O-Z]/\",\n",
    "    \"#MeToo intitle:/[H]/ -intitle:/[A-G,I-N]/ intitle:/[O-Z]/\",\n",
    "    \"#MeToo intitle:/[I]/ intitle:/[A-H,J-N]/ -intitle:/[O-Z]/\",\n",
    "    \"#MeToo intitle:/[I]/ -intitle:/[A-H,J-N]/ intitle:/[O-Z]/\",\n",
    "    \"#MeToo intitle:/[J]/ intitle:/[A-I,K-N]/ -intitle:/[O-Z]/\",\n",
    "    \"#MeToo intitle:/[J]/ -intitle:/[A-I,K-N]/ intitle:/[O-Z]/\",\n",
    "    \"#MeToo intitle:/[K]/ intitle:/[A-J,L-N]/ -intitle:/[O-Z]/\",\n",
    "    \"#MeToo intitle:/[K]/ -intitle:/[A-J,L-N]/ intitle:/[O-Z]/\",\n",
    "    \"#MeToo intitle:/[L]/ intitle:/[A-K,M-N]/ -intitle:/[O-Z]/\",\n",
    "    \"#MeToo intitle:/[L]/ -intitle:/[A-K,M-N]/ intitle:/[O-Z]/\",\n",
    "    \"#MeToo intitle:/[M]/ intitle:/[A-L,N]/ -intitle:/[O-Z]/\",\n",
    "    \"#MeToo intitle:/[M]/ -intitle:/[A-L,N]/ intitle:/[O-Z]/\",\n",
    "    \"#MeToo intitle:/[N]/ intitle:/[A-M]/ -intitle:/[O-Z]/\",\n",
    "    \"#MeToo intitle:/[N]/ -intitle:/[A-M]/ intitle:/[O-Z]/\",\n",
    "    \"#MeToo intitle:/[O]/ intitle:/[A-N]/ -intitle:/[P-Z]/\",\n",
    "    \"#MeToo intitle:/[O]/ -intitle:/[A-N]/ intitle:/[P-Z]/\",\n",
    "    \"#MeToo intitle:/[P]/ intitle:/[A-N]/ -intitle:/[Q-Z]/\",\n",
    "    \"#MeToo intitle:/[P]/ -intitle:/[A-N]/ intitle:/[Q-Z]/\",\n",
    "    \"#MeToo intitle:/[Q]/ intitle:/[A-N]/ -intitle:/[R-Z]/\",\n",
    "    \"#MeToo intitle:/[Q]/ -intitle:/[A-N]/ intitle:/[R-Z]/\",\n",
    "    \"#MeToo intitle:/[R]/ intitle:/[A-N]/ -intitle:/[S-Z]/\",\n",
    "]\n",
    "\n",
    "multi_ht_search = [\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "half1 = \"A-M\"\n",
    "half2 = \"N-Z\"\n",
    "\n",
    "query_list = []\n",
    "\n",
    "for letter in alphabet:\n",
    "    # Case 1: The letter is the only capital letter in the title\n",
    "    query_list.append(f\"#MeToo intitle:/{letter}/ -intitle:/[A-Z&&[^{letter}]]/\")\n",
    "\n",
    "    # Case 2: The letter + at least one letter from one half of the alphabet\n",
    "    if letter in half1:\n",
    "        query_list.append(f\"#MeToo intitle:/{letter}/ intitle:/[{half2}]/\")\n",
    "    else:\n",
    "        query_list.append(f\"#MeToo intitle:/{letter}/ intitle:/[{half1}]/\")\n",
    "\n",
    "    # Case 3: The letter + at least one letter from the other half of the alphabet\n",
    "    if letter in half1:\n",
    "        query_list.append(f\"#MeToo intitle:/{letter}/ intitle:/[{half1}&&[^{letter}]]/\")\n",
    "    else:\n",
    "        query_list.append(f\"#MeToo intitle:/{letter}/ intitle:/[{half2}&&[^{letter}]]/\")\n",
    "\n",
    "# Print or use the query_list as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_one_query(\"#MeToo intitle:/^(.*[A-Z]){0,2}.*$/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"list\": \"search\",\n",
    "    \"srsearch\": 'MeToo OR BalanceTonPorc OR MoiAussi OR نه_یعنی_نه OR 米兔 OR 我也是 OR وأنا كمان OR GamAni OR TôiCũngVậy OR 私も OR WatashiMo OR 나도 OR 나도당했다 OR גםאנחנו OR Ятоже OR RiceBunny OR EnaZeda OR AnaKaman OR YoTambien OR SendeAnlat OR KuToo OR WithYou OR WeToo OR cuentalo OR QuellaVoltaChe',\n",
    "    \"srlimit\": 500,\n",
    "    \"sroffset\": 0\n",
    "}\n",
    "# TO_CONTINUE = True\n",
    "# RESULTS = []\n",
    "\n",
    "# while TO_CONTINUE:\n",
    "    # print(f\"Processing Offset Num {PARAMS['sroffset']}\")\n",
    "R = S.get(url=URL, params=PARAMS)\n",
    "DATA = R.json()\n",
    "# return DATA['query']['searchinfo']['totalhits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_one_query(\"#MeToo intitle:/A/ intitle:/[N-Z]/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_one_query(\"#MeToo intitle:/A/ intitle:/[A-M&&[^A]]/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pagination\n",
    "\n",
    "S = requests.Session()\n",
    "\n",
    "URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "def process_one_query(query):\n",
    "\n",
    "\n",
    "    PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"list\": \"search\",\n",
    "        \"srsearch\": query,\n",
    "        \"srlimit\": 500,\n",
    "        \"sroffset\": 0\n",
    "    }\n",
    "    # TO_CONTINUE = True\n",
    "    # RESULTS = []\n",
    "\n",
    "    # while TO_CONTINUE:\n",
    "        # print(f\"Processing Offset Num {PARAMS['sroffset']}\")\n",
    "    R = S.get(url=URL, params=PARAMS)\n",
    "    DATA = R.json()\n",
    "    return DATA['query']['searchinfo']['totalhits']\n",
    "    # RESULTS.extend(DATA['query']['search'])\n",
    "        # break\n",
    "        # if 'error' in DATA:\n",
    "        #     raise Exception(DATA['error'])\n",
    "        # if 'warnings' in DATA:\n",
    "        #     print(DATA['warnings'])\n",
    "        # if 'continue' not in DATA:\n",
    "        #     break\n",
    "        # elif '||' in DATA['continue']['continue']:\n",
    "        #     TO_CONTINUE=True\n",
    "        #     PARAMS['sroffset']=DATA['continue']['sroffset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not query\n",
    "query_hits = []\n",
    "for query in individual_letters:\n",
    "    print(f'Processing Query: {query}')\n",
    "    query_hits.append(process_one_query(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.DataFrame.from_dict({\n",
    "    'Query': individual_letters,\n",
    "    'Total Hits': query_hits\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all of query lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pagination\n",
    "\n",
    "S = requests.Session()\n",
    "\n",
    "URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "def save_one_query(savename, query):\n",
    "\n",
    "    PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"list\": \"search\",\n",
    "        \"srsearch\": query,\n",
    "        \"srlimit\": 500,\n",
    "        \"sroffset\": 0\n",
    "    }\n",
    "    TO_CONTINUE = True\n",
    "    RESULTS = []\n",
    "\n",
    "    while TO_CONTINUE:\n",
    "        print(f\"Processing Offset Num {PARAMS['sroffset']}\")\n",
    "        R = S.get(url=URL, params=PARAMS)\n",
    "        DATA = R.json()\n",
    "        RESULTS.extend(DATA['query']['search'])\n",
    "        if 'error' in DATA:\n",
    "            raise Exception(DATA['error'])\n",
    "        if 'warnings' in DATA:\n",
    "            print(DATA['warnings'])\n",
    "        if 'continue' not in DATA:\n",
    "            break\n",
    "        elif '||' in DATA['continue']['continue']:\n",
    "            TO_CONTINUE=True\n",
    "            PARAMS['sroffset']=DATA['continue']['sroffset']\n",
    "    print(f\"Total Hits: {DATA['query']['searchinfo']['totalhits']}\")\n",
    "\n",
    "    savepath = f'/Users/hubert/Drive/DPhil/DPhil_Studies/2023-08-Study_C/Data/Wikipedia/{savename}.json'\n",
    "\n",
    "    with open(savepath, 'w') as file:\n",
    "        json.dump(RESULTS, file)\n",
    "\n",
    "    print(f'Saved to {savepath}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not query\n",
    "hashtags = [\n",
    "    \"MeToo\", \"BalanceTonPorc\", \"MoiAussi\", \"نه_یعنی_نه\", \"米兔\", \"我也是\", \n",
    "    \"وأنا كمان\", \"GamAni\", \"TôiCũngVậy\", \"私も\", \"WatashiMo\", \"나도\", \n",
    "    \"나도당했다\", \"גםאנחנו\", \"Ятоже\", \"RiceBunny\", \"EnaZeda\", \"AnaKaman\", \n",
    "    \"YoTambien\", \"SendeAnlat\", \"KuToo\", \"WithYou\", \"WeToo\", \"cuentalo\", \n",
    "    \"QuellaVoltaChe\", \"NiUnaMenos\", \"WoYeShi\", \"MyHarveyWeinstein\", \n",
    "    \"NousToutes\", \"stilleforopptak\", \"nårdansenstopper\", \"nårmusikkenstilner\", \n",
    "    \"memyös\", \"timesup\", \"NiEre\", \"JoTambe\", \"미투\", \"운동\"\n",
    "]\n",
    "\n",
    "query_hits = []\n",
    "for savename, query in enumerate(hashtags):\n",
    "    print(f'Processing Query: {query}')\n",
    "    save_one_query(savename, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Wikidata Id from Pageid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savename=0\n",
    "with open(f'/Users/hubert/Drive/DPhil/DPhil_Studies/2023-08-Study_C/Data/Wikipedia/{savename}.json', 'r') as f:\n",
    "    x = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_generator = list(generate_pageid_strings(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(query_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = []\n",
    "for query in tqdm(query_generator):\n",
    "    PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"pageids\": query,\n",
    "        # \"srsearch\": query,\n",
    "        # \"srlimit\": 500,\n",
    "        # \"sroffset\": 0,\n",
    "        \"prop\": \"pageprops\"\n",
    "    }\n",
    "    TO_CONTINUE = True\n",
    "\n",
    "    # while TO_CONTINUE:\n",
    "        # print(f\"Processing Offset Num {PARAMS['sroffset']}\")\n",
    "    R = S.get(url=URL, params=PARAMS)\n",
    "    DATA = R.json()\n",
    "\n",
    "    RESULTS.append(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'/Users/hubert/Drive/DPhil/DPhil_Studies/2023-08-Study_C/Data/Wikipedia/{savename}_wikidata.json', 'w') as f:\n",
    "    json.dump(RESULTS, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(DATA['query']['pages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full pipeline:\n",
    "\n",
    "for savename in range(1, 38):\n",
    "    with open(f'/Users/hubert/Drive/DPhil/DPhil_Studies/2023-08-Study_C/Data/Wikipedia/{savename}.json', 'r') as f:\n",
    "        x = json.load(f)\n",
    "\n",
    "    query_generator = list(generate_pageid_strings(x))\n",
    "\n",
    "    RESULTS = []\n",
    "    for query in tqdm(query_generator):\n",
    "        PARAMS = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"pageids\": query,\n",
    "            # \"srsearch\": query,\n",
    "            # \"srlimit\": 500,\n",
    "            # \"sroffset\": 0,\n",
    "            \"prop\": \"pageprops\"\n",
    "        }\n",
    "        TO_CONTINUE = True\n",
    "\n",
    "        # while TO_CONTINUE:\n",
    "            # print(f\"Processing Offset Num {PARAMS['sroffset']}\")\n",
    "        R = S.get(url=URL, params=PARAMS)\n",
    "        DATA = R.json()\n",
    "\n",
    "        RESULTS.append(DATA)\n",
    "\n",
    "    with open(f'/Users/hubert/Drive/DPhil/DPhil_Studies/2023-08-Study_C/Data/Wikipedia/{savename}_wikidata.json', 'w') as f:\n",
    "        json.dump(RESULTS, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full pipeline for select files\n",
    "\n",
    "files = [\n",
    "    'metoo_base',\n",
    "    'metoo_base_multisort',\n",
    "    'metoo_base_multisort_random_aug'\n",
    "]\n",
    "\n",
    "for savename in files:\n",
    "    with open(f'/Users/hubert/Drive/DPhil/DPhil_Studies/2023-08-Study_C/Data/Wikipedia/{savename}.json', 'r') as f:\n",
    "        x = json.load(f)\n",
    "\n",
    "    query_generator = list(generate_pageid_strings(x))\n",
    "\n",
    "    RESULTS = []\n",
    "    for query in tqdm(query_generator):\n",
    "        PARAMS = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"pageids\": query,\n",
    "            # \"srsearch\": query,\n",
    "            # \"srlimit\": 500,\n",
    "            # \"sroffset\": 0,\n",
    "            \"prop\": \"pageprops\"\n",
    "        }\n",
    "        TO_CONTINUE = True\n",
    "\n",
    "        # while TO_CONTINUE:\n",
    "            # print(f\"Processing Offset Num {PARAMS['sroffset']}\")\n",
    "        R = S.get(url=URL, params=PARAMS)\n",
    "        DATA = R.json()\n",
    "\n",
    "        RESULTS.append(DATA)\n",
    "\n",
    "    with open(f'/Users/hubert/Drive/DPhil/DPhil_Studies/2023-08-Study_C/Data/Wikipedia/{savename}_wikidata.json', 'w') as f:\n",
    "        json.dump(RESULTS, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Langlinks + Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same thing but with revisions and langlinks\n",
    "\n",
    "#Full pipeline for select fiels\n",
    "\n",
    "files = [\n",
    "    'metoo_base',\n",
    "    # 'metoo_base_multisort',\n",
    "    'metoo_base_multisort_random_aug'\n",
    "]\n",
    "\n",
    "S = requests.Session()\n",
    "\n",
    "URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "for savename in files[1:]:\n",
    "# for savename in range(0,38):\n",
    "    with open(f'/Users/hubert/Drive/DPhil/DPhil_Studies/2023-08-Study_C/Data/Wikipedia/{savename}.json', 'r') as f:\n",
    "        x = json.load(f)\n",
    "    # print(len(x))\n",
    "\n",
    "    query_generator = list(generate_pageid_strings(x))\n",
    "\n",
    "    RESULTS = []\n",
    "    for index, query in enumerate(tqdm(query_generator)):\n",
    "        PARAMS = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"pageids\": query,\n",
    "            \"lllimit\": 500,\n",
    "            \"llprop\": \"url|langname|autonym\",\n",
    "            \"plnamespace\":0,\n",
    "            \"prop\": \"pageprops|langlinks|links\",\n",
    "            \"pllimit\": 500,\n",
    "        }\n",
    "        TO_CONTINUE = True\n",
    "\n",
    "        # while TO_CONTINUE:\n",
    "            # print(f\"Processing Offset Num {PARAMS['sroffset']}\")\n",
    "        R = S.get(url=URL, params=PARAMS)\n",
    "        DATA = R.json()\n",
    "        RESULTS.append(DATA)\n",
    "        if \"continue\" in DATA:\n",
    "            # print(f'WARNING: \"continue\" key present in query index {index}.')\n",
    "            # # print(DATA)\n",
    "            while True:\n",
    "                PARAMS = {\n",
    "                    \"action\": \"query\",\n",
    "                    \"format\": \"json\",\n",
    "                    \"pageids\": query,\n",
    "                    \"lllimit\": 500,\n",
    "                    \"llcontinue\": DATA['continue'].get('llcontinue', None),\n",
    "                    \"llprop\": \"url|langname|autonym\",\n",
    "                    \"plnamespace\":0,\n",
    "                    \"prop\": \"pageprops|langlinks|links\",\n",
    "                    \"plcontinue\": DATA['continue'].get('plcontinue', None),\n",
    "                    \"pllimit\": 500,\n",
    "                }\n",
    "                R = S.get(url=URL, params=PARAMS)\n",
    "                DATA = R.json()\n",
    "                # print(DATA)\n",
    "                if 'error' in DATA:\n",
    "                    print(DATA)\n",
    "                    raise ValueError\n",
    "                RESULTS.append(DATA)\n",
    "                if \"continue\" not in DATA:\n",
    "                    break\n",
    "\n",
    "        #REMOVE LATER\n",
    "        # break\n",
    "\n",
    "\n",
    "    with open(f'/Users/hubert/Drive/DPhil/DPhil_Studies/2023-08-Study_C/Data/Wikipedia/{savename}_lang+pageprops.json', 'w') as f:\n",
    "        json.dump(RESULTS, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Revisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same thing but with revisions and langlinks\n",
    "\n",
    "#Full pipeline for select fiels\n",
    "\n",
    "files = [\n",
    "    'metoo_base',\n",
    "    'metoo_base_multisort_random_aug'\n",
    "]\n",
    "import os\n",
    "\n",
    "S = requests.Session()\n",
    "\n",
    "URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "IDS_IN_RESULTS = []\n",
    "\n",
    "for savename in files[:1]:\n",
    "    path = f'/Users/hubert/Drive/DPhil/DPhil_Studies/2023-08-Study_C/Data/Wikipedia/{savename}_rev+pageprops.json'\n",
    "    if os.path.isfile(path):\n",
    "        with open(path, 'r') as f:\n",
    "            RESULTS = json.load(f) \n",
    "        print('Data Loaded In')\n",
    "    else:\n",
    "        RESULTS = []\n",
    "    with open(f'/Users/hubert/Drive/DPhil/DPhil_Studies/2023-08-Study_C/Data/Wikipedia/{savename}.json', 'r') as f:\n",
    "        x = json.load(f)\n",
    "\n",
    "    PAGEIDS = [i['pageid'] for i in x]\n",
    "    # PAGEIDS = [i for i in PAGEIDS if ]\n",
    "    for index, query in enumerate(tqdm(PAGEIDS[:200])):\n",
    "        PARAMS = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"pageids\": str(query),\n",
    "            \"prop\": \"pageprops|revisions\",\n",
    "            \"rvprop\": \"ids|timestamp|flags|comment|user|content|tags|userid\",\n",
    "            \"rvlimit\": 25,\n",
    "            \"rvslots\": \"main\"\n",
    "        }\n",
    "\n",
    "        R = S.get(url=URL, params=PARAMS)\n",
    "        DATA = R.json()\n",
    "        RESULTS.append(DATA)\n",
    "        if \"continue\" in DATA:\n",
    "            while True:\n",
    "                PARAMS = {\n",
    "                    \"action\": \"query\",\n",
    "                    \"format\": \"json\",\n",
    "                    \"pageids\": str(query),\n",
    "                    \"prop\": \"pageprops|revisions\",\n",
    "                    \"rvprop\": \"ids|timestamp|flags|comment|user|content|tags|userid\",\n",
    "                    \"rvlimit\": 25,\n",
    "                    \"rvslots\": \"main\",\n",
    "                    \"rvcontinue\": DATA['continue'].get('rvcontinue', ''),\n",
    "                }\n",
    "                R = S.get(url=URL, params=PARAMS)\n",
    "                DATA = R.json()\n",
    "                RESULTS.append(DATA)\n",
    "                if \"continue\" not in DATA or \"batchcomplete\" in DATA:\n",
    "                    break\n",
    "\n",
    "        with open(f'/Users/hubert/Drive/DPhil/DPhil_Studies/2023-08-Study_C/Data/Wikipedia/{savename}_rev+pageprops.json', 'w') as f:\n",
    "            json.dump(RESULTS, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'/Users/hubert/Drive/DPhil/DPhil_Studies/2023-08-Study_C/Data/Wikipedia/metoo_base_rev+pageprops.json'\n",
    "if os.path.isfile(path):\n",
    "    with open(path, 'r') as f:\n",
    "        RESULTS = json.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS[0]['query']['pages'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = []\n",
    "PARAMS = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"pageids\": '55551931',\n",
    "    \"prop\": \"pageprops|revisions\",\n",
    "    \"rvprop\": \"ids|timestamp|flags|comment|user|content|tags|userid\",\n",
    "    \"rvlimit\": 25,\n",
    "    \"rvslots\": \"main\"\n",
    "}\n",
    "\n",
    "R = S.get(url=URL, params=PARAMS)\n",
    "DATA = R.json()\n",
    "while True:\n",
    "    PARAMS = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"pageids\": query,\n",
    "        \"prop\": \"pageprops|revisions\",\n",
    "        \"rvprop\": \"ids|timestamp|flags|comment|user|content|tags|userid\",\n",
    "        \"rvlimit\": 25,\n",
    "        \"rvcontinue\": DATA['continue'].get('rvcontinue', ''),\n",
    "        \"rvslots\": \"main\",\n",
    "    }\n",
    "    R = S.get(url=URL, params=PARAMS)\n",
    "    DATA = R.json()\n",
    "    RESULTS.append(DATA)\n",
    "    if 'warnings' in DATA:\n",
    "        print(DATA['warnings'])\n",
    "    if \"continue\" not in DATA:\n",
    "        print('nocont')\n",
    "        break\n",
    "    if \"batchcomplete\" in DATA:\n",
    "        print('batchcomplete')\n",
    "        break\n",
    "    if \"rvcontinue\" not in DATA['continue']:\n",
    "        print('no rvcontinue')\n",
    "    else:\n",
    "        print(DATA['continue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Page Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same thing but with revisions and langlinks\n",
    "overwrite = False\n",
    "\n",
    "#Full pipeline for select fiels\n",
    "\n",
    "files = [\n",
    "    'metoo_base',\n",
    "    # 'metoo_base_multisort',\n",
    "    'metoo_base_multisort_random_aug'\n",
    "]\n",
    "\n",
    "files = files + [f'{i}' for i in range(38)]\n",
    "\n",
    "S = requests.Session()\n",
    "\n",
    "URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "\n",
    "\n",
    "for index, savename in enumerate(tqdm(files)):\n",
    "# for savename in range(0,38):\n",
    "    with open(f'/Users/hubert/Drive/DPhil/DPhil_Studies/2023-08-Study_C/Data/Wikipedia/{savename}.json', 'r') as f:\n",
    "        x = json.load(f)\n",
    "    # print(len(x))\n",
    "\n",
    "    outfile = f'/Users/hubert/Drive/DPhil/DPhil_Studies/2023-08-Study_C/Data/Wikipedia/{savename}_categories.json'\n",
    "    if os.path.isfile(outfile) and not overwrite:\n",
    "        print(f'Skipping {savename}')\n",
    "        continue\n",
    "\n",
    "    print(f'Processing -> {savename}. {index+1} of {len(files)}')\n",
    "\n",
    "    query_generator = list(generate_pageid_strings(x))\n",
    "\n",
    "    RESULTS = []\n",
    "    for index, query in enumerate(tqdm(query_generator)):\n",
    "        PARAMS = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"pageids\": query,\n",
    "            \"prop\": \"categories|pageprops\",\n",
    "            \"cl\": \"sortkey|timestamp|hidden\",\n",
    "            \"cllimit\": 500,\n",
    "        }\n",
    "        TO_CONTINUE = True\n",
    "\n",
    "        # while TO_CONTINUE:\n",
    "            # print(f\"Processing Offset Num {PARAMS['sroffset']}\")\n",
    "        R = S.get(url=URL, params=PARAMS)\n",
    "        DATA = R.json()\n",
    "        if 'error' in DATA:\n",
    "            print(DATA)\n",
    "            raise ValueError\n",
    "        RESULTS.append(DATA)\n",
    "        if \"continue\" in DATA:\n",
    "            # print(f'WARNING: \"continue\" key present in query index {index}.')\n",
    "            # print(DATA)\n",
    "            while True:\n",
    "                PARAMS = {\n",
    "                    \"action\": \"query\",\n",
    "                    \"format\": \"json\",\n",
    "                    \"pageids\": query,\n",
    "                    \"prop\": \"categories|pageprops\",\n",
    "                    \"cl\": \"sortkey|timestamp|hidden\",\n",
    "                    \"cllimit\": 500,\n",
    "                    \"clcontinue\": DATA['continue'].get('clcontinue', None),\n",
    "                }\n",
    "                R = S.get(url=URL, params=PARAMS)\n",
    "                DATA = R.json()\n",
    "                if 'error' in DATA:\n",
    "                    print(DATA)\n",
    "                    raise ValueError\n",
    "                RESULTS.append(DATA)\n",
    "                if \"continue\" not in DATA:\n",
    "                    break\n",
    "\n",
    "    with open(outfile, 'w') as f:\n",
    "        json.dump(RESULTS, f)\n",
    "\n",
    "    print(f'Saved to {outfile}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Then get Corresponding Category Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAT = defaultdict(set)\n",
    "ALLCATS = Counter()\n",
    "\n",
    "files = [\n",
    "    # 'metoo_base',\n",
    "    # 'metoo_base_multisort',\n",
    "    'metoo_base_multisort_random_aug'\n",
    "]\n",
    "\n",
    "# omit 0 == 'MeToo'\n",
    "files = files + [f'{i}' for i in range(1, 38)]\n",
    "\n",
    "PAGEID_ALREADY_SEEN = set()\n",
    "for file in tqdm(files):\n",
    "\n",
    "    with open(f'./Data/Wikipedia/{file}_categories.json', 'r') as f:\n",
    "        RESULTS = json.load(f)\n",
    "\n",
    "    for count, i in enumerate(RESULTS):\n",
    "\n",
    "        if 'query' not in i:\n",
    "            print(count, i)\n",
    "            continue\n",
    "        for k, v in i['query']['pages'].items():\n",
    "\n",
    "            assert k == str(v['pageid'])\n",
    "            if 'categories' in v:\n",
    "                for cat in v['categories']:\n",
    "                    if cat['title'] in CAT[k]:\n",
    "                        continue\n",
    "                    else:\n",
    "                        CAT[k].add(cat['title'])\n",
    "                        ALLCATS[cat['title']]+=1\n",
    "\n",
    "print(f'Total number of unique categories collected: {len(ALLCATS)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALLCATS.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(CAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Data/Wikipedia/all_cat_desc.json', 'w') as f:\n",
    "    json.dump(ALLCATS, f)\n",
    "with open('./Data/Wikipedia/cat_per_article.pkl', 'wb') as f:\n",
    "    pickle.dump(CAT, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_generator = list(generate_titles_strings(ALLCATS.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb2f476258241fd813e0b2ad01539b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44460 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 23\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, query \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(query_generator)):\n\u001b[1;32m     17\u001b[0m     PARAMS \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitles\u001b[39m\u001b[38;5;124m\"\u001b[39m: query,\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprop\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategoryinfo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m     }\n\u001b[0;32m---> 23\u001b[0m     R \u001b[38;5;241m=\u001b[39m \u001b[43mS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mURL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPARAMS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     DATA \u001b[38;5;241m=\u001b[39m R\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m DATA:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dphil3/lib/python3.11/site-packages/requests/sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dphil3/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dphil3/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dphil3/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dphil3/lib/python3.11/site-packages/urllib3/connectionpool.py:791\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 791\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    807\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dphil3/lib/python3.11/site-packages/urllib3/connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dphil3/lib/python3.11/site-packages/urllib3/connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dphil3/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dphil3/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dphil3/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dphil3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dphil3/lib/python3.11/ssl.py:1311\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1308\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1309\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1310\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dphil3/lib/python3.11/ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# same thing but with revisions and langlinks\n",
    "overwrite = False\n",
    "\n",
    "S = requests.Session()\n",
    "\n",
    "URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "\n",
    "outfile = './Data/Wikipedia/all_cat_info.json'\n",
    "query_generator = list(generate_titles_strings(ALLCATS.keys()))\n",
    "\n",
    "if os.path.isfile(outfile) and not overwrite:\n",
    "    print(f'{outfile} already exists. Ending.')\n",
    "else:\n",
    "    RESULTS = []\n",
    "    for index, query in enumerate(tqdm(query_generator)):\n",
    "        PARAMS = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": query,\n",
    "            \"prop\": \"categoryinfo\",\n",
    "        }\n",
    "        R = S.get(url=URL, params=PARAMS)\n",
    "        DATA = R.json()\n",
    "        if 'error' in DATA:\n",
    "            print(DATA)\n",
    "            raise ValueError\n",
    "        RESULTS.append(DATA)\n",
    "        if \"continue\" in DATA:\n",
    "            while True:\n",
    "                PARAMS = {\n",
    "                    \"action\": \"query\",\n",
    "                    \"format\": \"json\",\n",
    "                    \"titles\": query,\n",
    "                    \"prop\": \"categoryinfo\",\n",
    "                    \"clcontinue\": DATA['continue'].get('clcontinue', None),\n",
    "                }\n",
    "                R = S.get(url=URL, params=PARAMS)\n",
    "                DATA = R.json()\n",
    "                if 'error' in DATA:\n",
    "                    print(DATA)\n",
    "                    raise ValueError\n",
    "                RESULTS.append(DATA)\n",
    "                if \"continue\" not in DATA:\n",
    "                    break\n",
    "\n",
    "    with open(outfile, 'w') as f:\n",
    "        json.dump(RESULTS, f)\n",
    "\n",
    "    print(f'Saved to {outfile}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27652"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outfile, 'w') as f:\n",
    "    json.dump(RESULTS, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dphil3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
