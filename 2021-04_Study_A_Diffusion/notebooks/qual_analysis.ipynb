{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qual Analysis and Exploratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tqdm\n",
    "import json\n",
    "import glob\n",
    "import jsonlines\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "import sys\n",
    "from collections import Counter\n",
    "import subprocess\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "import datetime\n",
    "import h5py\n",
    "from typing import NamedTuple\n",
    "import re\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import bernoulli \n",
    "from collections import defaultdict\n",
    "from operator import attrgetter\n",
    "import statsmodels.formula.api as smf\n",
    "from pandas.plotting import scatter_matrix\n",
    "import networkx as nx\n",
    "\n",
    "#load in search hashtags\n",
    "with open('../references/search_hashtags.txt', 'r') as f:\n",
    "    search_hashtags = f.readlines()\n",
    "    search_hashtags = [i.replace('\\n', '') for i in search_hashtags]\n",
    "    search_hashtags = [i.replace('#', '') for i in search_hashtags]\n",
    "    search_hashtags = [i.lower() for i in search_hashtags]\n",
    "    search_hashtags.remove('وأناكمان')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kutoo': datetime.datetime(2020, 2, 28, 0, 0),\n",
       " 'metoo': datetime.datetime(2020, 2, 24, 0, 0),\n",
       " 'moiaussi': datetime.datetime(2020, 2, 5, 0, 0),\n",
       " 'niunamenos': datetime.datetime(2020, 3, 8, 0, 0),\n",
       " 'tôicũngvậy': datetime.datetime(2020, 3, 8, 0, 0),\n",
       " 'watashimo': datetime.datetime(2020, 2, 28, 0, 0),\n",
       " 'ятоже': datetime.datetime(2020, 2, 8, 0, 0),\n",
       " '나도당했다': datetime.datetime(2020, 3, 8, 0, 0)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_num = 3\n",
    "hashtag_split = True\n",
    "ngram_range = '34'\n",
    "min_user = 100\n",
    "\n",
    "plot_save_path = f'/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/results/0{group_num}_group/'\n",
    "\n",
    "overwrite = True\n",
    "read_in = True\n",
    "\n",
    "def unit_conv(val):\n",
    "    return datetime.datetime.strptime('2017-10-16', '%Y-%m-%d') + datetime.timedelta(days=int(val))\n",
    "\n",
    "def reverse_unit_conv(date):\n",
    "    return (datetime.datetime.strptime(date, '%Y-%m-%d') - datetime.datetime.strptime('2017-10-16', '%Y-%m-%d')).days\n",
    "\n",
    "class daterange(NamedTuple):\n",
    "    start: str\n",
    "    end: str\n",
    "\n",
    "#obtain peak times again\n",
    "with h5py.File('/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/02_intermediate/FAS_peak_analysis.hdf5', 'r') as f:\n",
    "    FAS_peaks = f['peak_detections']\n",
    "    x = f['segments']['selected_ranges'][int(group_num)-1]\n",
    "    group_date_range = daterange(\n",
    "        start = x[0].decode(),\n",
    "        end = x[1].decode()\n",
    "    )\n",
    "\n",
    "    group_start_index = reverse_unit_conv(group_date_range.start)\n",
    "    group_end_index = reverse_unit_conv(group_date_range.end)\n",
    "\n",
    "    most_prominent_peaks = {}\n",
    "    for name, h5obj in FAS_peaks.items():\n",
    "\n",
    "        peak_locations = h5obj['peak_locations']\n",
    "        peak_locations = [(i,e) for i,e in enumerate(h5obj['peak_locations']) if (unit_conv(e) > datetime.datetime.strptime(group_date_range.start, '%Y-%m-%d')) and (unit_conv(e) < datetime.datetime.strptime(group_date_range.end, '%Y-%m-%d'))]\n",
    "        peak_indices = [i[0] for i in peak_locations]\n",
    "        prominences = [element for index, element in enumerate(h5obj['prominences']) if index in peak_indices]\n",
    "        if len(prominences) == 0:\n",
    "            continue\n",
    "        max_prominence = np.argmax(prominences)\n",
    "        most_prominent_peaks[name] = unit_conv(peak_locations[max_prominence][1])\n",
    "\n",
    "most_prominent_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read df raw for ABM\n",
    "stats_df_save_dir = '/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/06_reporting/'\n",
    "df_filename = os.path.join(stats_df_save_dir, f'ABM_raw_df_group_{group_num}.obj')\n",
    "stats_filename = os.path.join(stats_df_save_dir, f'ABM_stats_df_group_{group_num}.obj')\n",
    "\n",
    "if os.path.isfile(df_filename):\n",
    "    print('reading in df')\n",
    "    with open(df_filename, 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "if os.path.isfile(stats_filename):\n",
    "    print('reading in stats_df')\n",
    "    with open(stats_filename, 'rb') as f:\n",
    "        stats_df = pickle.load(f)\n",
    "\n",
    "print('N.B. users are not included in stats df because in creating the activity counts users were split into before and after peak interactions')\n",
    "\n",
    "print(f'Length of df: {len(df)}')\n",
    "unique_author_stats_df_count = len(stats_df['author_id'].unique())\n",
    "print(f'Number of unique authors in df: {unique_author_stats_df_count}')\n",
    "unique_author_df_count = len(df['author_id'].unique())\n",
    "print(f'Number of unique authors in stats_df: {unique_author_df_count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate ht column - sort of reverse of \n",
    "df_colnames = df.columns\n",
    "vocab_colnames = [i for i in df_colnames if i.startswith('vocab')][::-1]\n",
    "def process_row_ht(row):\n",
    "    for col in vocab_colnames:\n",
    "        if col == 'vocab:#timesup':\n",
    "            continue\n",
    "        if row[col] == 1:\n",
    "            return col.split('#')[-1]\n",
    "    return None\n",
    "\n",
    "df['ht'] = df.apply(process_row_ht, axis=1)\n",
    "df['ht'] = df['ht'].fillna('metoo')\n",
    "\n",
    "df = df.merge(stats_df, on=['author_id','ht'], how='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert Primary HT into full df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAS_volume_df = pd.read_hdf('/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/02_intermediate/FAS_peak_analysis.hdf5', key='plot_data')\n",
    "\n",
    "num_users_group = len(glob.glob(f'/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/01_raw/0{group_num}_group/timeline*.jsonl'))\n",
    "\n",
    "expected_counts = FAS_volume_df[(FAS_volume_df['created_at']<group_date_range.end) & (FAS_volume_df['created_at']>group_date_range.start)].groupby('hashtag').sum()/num_users_group\n",
    "\n",
    "# incorporate primary ht\n",
    "with open(f'/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/03_processed/primary_ht_global.obj', 'rb') as f:\n",
    "    user_order, res = pickle.load(f)\n",
    "\n",
    "unknown_count = 0\n",
    "def process_primary_res(author_id):\n",
    "    global unknown_count\n",
    "    if author_id not in user_order:\n",
    "        # print(f'{author_id} not in users')\n",
    "        unknown_count += 1\n",
    "        return 'metoo'\n",
    "    return search_hashtags[np.argmax(res[user_order.index(author_id),:])]\n",
    "\n",
    "df['primary_ht'] = df['author_id'].map(df.groupby('author_id').apply(lambda x: process_primary_res(x.name)))\n",
    "print(f'Number of unknown primary hashtags for users: {unknown_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory\n",
    "\n",
    "What would be informative?\n",
    "* Which users show the strongest signals?\n",
    "* PCA on the metrics in the scatter matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats_df.columns)\n",
    "\n",
    "pca_colnames = [\n",
    "    'int_pre_peak',\n",
    "    'act_pre_peak',\n",
    "    'norm_act_pre_peak',\n",
    "    'likes_std',\n",
    "    'percent_reciprocal'\n",
    "]\n",
    "\n",
    "pca_df = stats_df.loc[:,pca_colnames]\n",
    "pca_df = pca_df.fillna(0)\n",
    "pca_df = StandardScaler().fit_transform(pca_df)\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(pca_df)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf = pd.concat([principalDf, stats_df['act_post_peak']>5], axis = 1)\n",
    "finalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "\n",
    "targets = ['participated', 'not_participated']\n",
    "colors = ['r', 'g']\n",
    "for target, color in zip(targets,colors):\n",
    "    if target == 'participated':\n",
    "        indicesToKeep = finalDf['act_post_peak']\n",
    "    else:\n",
    "        indicesToKeep = finalDf['act_post_peak']==False\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()\n",
    "plt.savefig(f'{plot_save_path}pca_numerical_{group_num}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = stats_df['act_post_peak']>5\n",
    "\n",
    "def pca_with_var_plot(score,coeff,labels=None):\n",
    "\n",
    "    # score = pca result\n",
    "    # coeff = pca.components_\n",
    "\n",
    "    xs = score[:,0]\n",
    "    ys = score[:,1]\n",
    "    n = coeff.shape[0]\n",
    "\n",
    "    sns.scatterplot(xs,ys, hue=y) #without scaling\n",
    "    for i in range(n):\n",
    "        plt.arrow(0, 0, coeff[i,0], coeff[i,1], color = 'r',alpha = 1, width=0.2)\n",
    "        if labels is None:\n",
    "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color = 'g', ha = 'center', va = 'center')\n",
    "        else:\n",
    "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = 'g', ha = 'center', va = 'center')\n",
    "\n",
    "fig = plt.figure(figsize = (15,15))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "sns.color_palette(\"Set2\")\n",
    "plt.xlabel(\"PC{}\".format(1))\n",
    "plt.ylabel(\"PC{}\".format(2))\n",
    "plt.grid()\n",
    "\n",
    "#Call the function. \n",
    "pca_with_var_plot(principalComponents, pca.components_, labels=pca_colnames)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "famd_df_cols = [\n",
    "    'int_pre_peak',\n",
    "    'act_pre_peak',\n",
    "    'norm_act_pre_peak',\n",
    "    'likes_std',\n",
    "    'percent_reciprocal',\n",
    "    'ht',\n",
    "    'org',\n",
    "    'age',\n",
    "    'gender',\n",
    "    'lang_diff'\n",
    "]\n",
    "\n",
    "famd_df = stats_df.loc[:,famd_df_cols]\n",
    "\n",
    "#creating instance of one-hot-encoder\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform one-hot encoding on 'team' column \n",
    "famd_df_final=stats_df.loc[:,pca_colnames]\n",
    "for col in famd_df.columns:\n",
    "    if famd_df[col].dtype == 'int64' or famd_df[col].dtype == 'float64':\n",
    "        pass\n",
    "    else:\n",
    "        encoder_df = pd.DataFrame(encoder.fit_transform(famd_df[[col]]).toarray())\n",
    "        encoder_df = encoder_df.div(np.sqrt(encoder_df.sum(axis=0)/len(encoder_df)), axis=1)\n",
    "        encoder_df.columns = [f'{col}_{i}' for i in encoder_df.columns]\n",
    "        famd_df_final = famd_df_final.join(encoder_df, how='left', rsuffix=col)\n",
    "\n",
    "# #merge one-hot encoded columns back with original DataFrame\n",
    "# final_df = df.join(encoder_df)\n",
    "\n",
    "famd_df_final=famd_df_final.fillna(0)\n",
    "famd_df_final = StandardScaler().fit_transform(famd_df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(famd_df_final)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of act_post_peak\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Post peak activity', fontsize = 15)\n",
    "ax.hist(stats_df['act_post_peak'], bins = 200);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df['act_post_peak'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf = pd.concat([principalDf, stats_df['act_post_peak']>2], axis = 1)\n",
    "finalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "\n",
    "targets = ['participated', 'not_participated']\n",
    "colors = ['r', 'g']\n",
    "for target, color in zip(targets,colors):\n",
    "    if target == 'participated':\n",
    "        indicesToKeep = finalDf['act_post_peak']\n",
    "    else:\n",
    "        indicesToKeep = finalDf['act_post_peak']==False\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()\n",
    "plt.savefig(f'{plot_save_path}pca_combined_{group_num}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = stats_df['act_post_peak']>5\n",
    "\n",
    "fig = plt.figure(figsize = (15,15))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "sns.color_palette(\"Set2\")\n",
    "plt.xlabel(\"PC{}\".format(1))\n",
    "plt.ylabel(\"PC{}\".format(2))\n",
    "plt.grid()\n",
    "\n",
    "#Call the function. \n",
    "pca_with_var_plot(principalComponents, pca.components_, labels=pca_colnames)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KModes/KPrototypes\n",
    "\n",
    "[HUANG97]\t(1, 2) Huang, Z.: Clustering large data sets with mixed numeric and categorical values, Proceedings of the First Pacific Asia Knowledge Discovery and Data Mining Conference, Singapore, pp. 21-34, 1997.\n",
    "[HUANG98]\tHuang, Z.: Extensions to the k-modes algorithm for clustering large data sets with categorical values, Data Mining and Knowledge Discovery 2(3), pp. 283-304, 1998.\n",
    "[CAO09]\tCao, F., Liang, J, Bai, L.: A new initialization method for categorical data clustering, Expert Systems with Applications 36(7), pp. 10223-10228., 2009.\n",
    "\n",
    "Methods of combining clustering for both numerical and categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "famd_df = famd_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "famd_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kmodes.kprototypes import KPrototypes\n",
    "\n",
    "kproto = KPrototypes(n_clusters=2, verbose=2, max_iter=20)\n",
    "kproto.fit(famd_df, categorical=[5,6,7,8,9])\n",
    "\n",
    "# clusters = KPrototypes().fit_predict(X, categorical=[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = kproto.predict(famd_df, categorical=[5,6,7,8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "famd_df['clusters'] = list(clusters)\n",
    "famd_df[famd_df['clusters']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full df exploratory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* get a sense that majority of people don't do much\n",
    "* get an idea of that 95%/5% split.\n",
    "* what do they look like behaviourally speaking?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter df to detectable signal output\n",
    "activated_idx = stats_df['act_post_peak'] > 10\n",
    "\n",
    "stats_df[activated_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot distribution of interactions for int_pre_peak for the filter on act_post_peak\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Interaction Histogram for High Post Peak Activity', fontsize = 15)\n",
    "# grouped_activated = df[activated_idx].groupby('author_id').\n",
    "ax.hist(stats_df[activated_idx]['int_pre_peak'], bins = 50)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compared to histogram for interaction activity for all users:\n",
    "\n",
    "# # plot distribution of interactions for int_pre_peak for the filter on act_post_peak\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Interaction Histogram for All Users', fontsize = 15)\n",
    "# grouped_activated = df[activated_idx].groupby('author_id').\n",
    "ax.hist(stats_df['int_pre_peak'], bins = 100)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how about the density of interactions up to their peaks?\n",
    "\n",
    "df['peak_time'] = df['ht'].apply(lambda x: most_prominent_peaks[x].date())\n",
    "df['peak_time_delta'] = df['peak_time']-df['created_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of peak time delta\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Peak time delta for all users', fontsize = 15)\n",
    "# grouped_activated = df[activated_idx].groupby('author_id').\n",
    "ax.hist(df['peak_time_delta'].dt.total_seconds()/(24 * 60 * 60), bins = 50)\n",
    "plt.yscale('log')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So on average each user does keep interacting after the peak they participate in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot interaction strength in each language\n",
    "# fig = plt.figure(figsize = (8,8))\n",
    "# ax = fig.add_subplot(1,1,1)\n",
    "# for ht in df['ht'].unique():\n",
    "#     ht_indices = df['ht']==ht\n",
    "#     daily_counts = df[ht_indices].groupby('created_at').count()\n",
    "#     ax.scatter(finalDf.loc[indicesToKeep, \n",
    "#                , finalDf.loc[indicesToKeep, 'principal component 2']\n",
    "#                , c = color\n",
    "#                , s = 50)\n",
    "#     temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby(['created_at','ht']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (16,9))\n",
    "\n",
    "sns_df = df.groupby(['created_at','ht']).count()\n",
    "sns_df['tweet_id'] = np.log(sns_df['tweet_id'])\n",
    "\n",
    "# Draw line plot of size and total_bill with parameters and hue \"day\"\n",
    "sns.lineplot(\n",
    "    x = \"created_at\", y = \"tweet_id\", data = sns_df, hue = \"ht\",\n",
    "            style = \"ht\", palette = \"hot\", dashes = False,  legend=\"brief\",)\n",
    " \n",
    "plt.title(\"Interactions per day per hashtag\", fontsize = 20)\n",
    "plt.xlabel(\"Date\", fontsize = 15)\n",
    "plt.ylabel(\"Interactions\", fontsize = 15)\n",
    "plt.savefig(f'{plot_save_path}overall_interactions_{group_num}.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot interactions per hashtag and subsequent activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find users with max signal\n",
    "stats_df.sort_values(by=['act_post_peak'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_to_examine = '101381642'\n",
    "usertype = 'low'\n",
    "\n",
    "class daterange(NamedTuple):\n",
    "    start: str\n",
    "    end: str\n",
    "\n",
    "activity_file = '/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/03_processed/activity_counts.hdf5'\n",
    "\n",
    "# obtain user activity\n",
    "with h5py.File(activity_file, 'r') as f:\n",
    "    activity = f[f'group_{group_num}'][user_to_examine]['hashtagged'][:]\n",
    "    feature_order = f[f'group_{group_num}'][user_to_examine]['hashtagged'].attrs['feature_order']\n",
    "    feature_order = feature_order.split(';')\n",
    "\n",
    "with h5py.File('/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/02_intermediate/FAS_peak_analysis.hdf5', 'r') as f:\n",
    "    FAS_peaks = f['peak_detections']\n",
    "    x = f['segments']['selected_ranges'][int(group_num)-1]\n",
    "    group_date_range = daterange(\n",
    "        start = x[0].decode(),\n",
    "        end = x[1].decode()\n",
    "    )\n",
    "\n",
    "daterange = pd.date_range(start=group_date_range.start, end=group_date_range.end)\n",
    "if group_num == 1:\n",
    "    daterange = pd.date_range(start=group_date_range.start, end='2018-05-30')\n",
    "\n",
    "plot_user_df = pd.DataFrame({\n",
    "    'created_at' : daterange})\n",
    "\n",
    "for index, feature in enumerate(feature_order):\n",
    "    plot_user_df[f'activity_{feature}'] = activity[index,:]\n",
    "\n",
    "plot_user_df=pd.wide_to_long(plot_user_df, stubnames='activity', i='created_at', j='ht', sep = '_', suffix='.*').reset_index()\n",
    "\n",
    "user_df = df[df['author_id']==user_to_examine]\n",
    "user_df = user_df.groupby(['created_at', 'ht']).count()['tweet_id'].reset_index()\n",
    "user_df['created_at'] = pd.to_datetime(user_df['created_at'])\n",
    "final_user_plot_df = user_df.merge(plot_user_df, on = ['created_at','ht'], how='left')\n",
    "final_user_plot_df.columns = ['created_at', 'Hashtag', 'Interaction Count', 'Hashtag Protest Network Activity']\n",
    "\n",
    "# final_user_plot_df['interaction_count'] = final_user_plot_df['activity']-final_user_plot_df['tweet_id']\n",
    "# final_user_plot_df = final_user_plot_df.drop(['tweet_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['author_id']==user_to_examine]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df.groupby(['author_id','ht']).count().reset_index()\n",
    "temp = temp[['author_id', 'ht']].groupby('author_id').count()\n",
    "temp[temp['ht']>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_user_plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.melt(final_user_plot_df, ['created_at','Hashtag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (16,9))\n",
    "\n",
    "# Draw line plot of size and total_bill with parameters and hue \"day\"\n",
    "sns.lineplot(\n",
    "    x = \"created_at\", y = \"value\", hue='variable',data=pd.melt(final_user_plot_df, ['created_at','Hashtag']))\n",
    "plt.title(\"Interactions per day per hashtag\", fontsize = 20)\n",
    "plt.xlabel(\"Date\", fontsize = 15)\n",
    "plt.ylabel(\"Interactions\", fontsize = 15)\n",
    "plt.savefig(f'{plot_save_path}user_{usertype}_output_{group_num}.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Progress for ABM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABM_ready_filename = f'/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/03_processed/ABM_df_group_{group_num}.obj'\n",
    "\n",
    "with open(ABM_ready_filename, 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
