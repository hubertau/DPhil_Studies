{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Analysis: Generate Stats DF\n",
    "\n",
    "Beginnings of statistical analysis based on the timeline stats of users, NOT the clustering word data yet. Also should I get the follows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tqdm\n",
    "import json\n",
    "import glob\n",
    "import jsonlines\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from collections import Counter\n",
    "import subprocess\n",
    "import datetime\n",
    "import h5py\n",
    "from typing import NamedTuple\n",
    "import re\n",
    "import statsmodels.api as sm\n",
    "from collections import defaultdict\n",
    "from operator import attrgetter\n",
    "import statsmodels.formula.api as smf\n",
    "from pandas.plotting import scatter_matrix\n",
    "import networkx as nx\n",
    "\n",
    "#load in search hashtags\n",
    "with open('../references/search_hashtags.txt', 'r') as f:\n",
    "    search_hashtags = f.readlines()\n",
    "    search_hashtags = [i.replace('\\n', '') for i in search_hashtags]\n",
    "    search_hashtags = [i.replace('#', '') for i in search_hashtags]\n",
    "    search_hashtags = [i.lower() for i in search_hashtags]\n",
    "    search_hashtags.remove('وأناكمان')\n",
    "\n",
    "class daterange(NamedTuple):\n",
    "    start: str\n",
    "    end: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_num = 1\n",
    "hashtag_split = True\n",
    "ngram_range = '34'\n",
    "min_user = 100\n",
    "\n",
    "overwrite = True\n",
    "read_in = True\n",
    "for_abm = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in data and basic preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['interactions_group_1_part_0', 'interactions_group_1_part_1']\n",
      "Multi-part interactions read-in\n",
      "converting data format for in_reply_to\n",
      "removing self_replies\n",
      "Extracting quote ids\n",
      "saving to file\n",
      "Number of non-null: 7939920\n",
      "getting internal counts...\n",
      "Internal replies (not self replies): 848558\n",
      "Total df length: 7939920\n",
      "           tweet_id\n",
      "count   4216.000000\n",
      "mean    1883.282732\n",
      "std     2837.788446\n",
      "min        1.000000\n",
      "25%      148.750000\n",
      "50%      763.000000\n",
      "75%     2498.250000\n",
      "max    26390.000000\n"
     ]
    }
   ],
   "source": [
    "# Load in data\n",
    "\n",
    "# set paths\n",
    "interactions_hdf5_file = '/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/03_processed/interactions.hdf5'\n",
    "\n",
    "group_folder = f'/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/01_raw/0{group_num}_group/'\n",
    "\n",
    "FAS_volume_df = pd.read_hdf('/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/02_intermediate/FAS_peak_analysis.hdf5', key='plot_data')\n",
    "\n",
    "num_users_group = len(glob.glob(f'/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/01_raw/0{group_num}_group/timeline*.jsonl'))\n",
    "\n",
    "# define functions\n",
    "def load_in_data(interactions_hdf5_file):\n",
    "\n",
    "    # check if data was saved in multiple parts\n",
    "    with h5py.File(interactions_hdf5_file, 'r') as f:\n",
    "        interactions_hdf5_keys = f.keys()\n",
    "        interactions_hdf5_keys = [i for i in interactions_hdf5_keys if re.search(f'interactions_group_{group_num}_part',i)]\n",
    "        print(interactions_hdf5_keys)\n",
    "\n",
    "    if len(interactions_hdf5_keys) > 1:\n",
    "        print('Multi-part interactions read-in')\n",
    "        list_df = []\n",
    "        for i in interactions_hdf5_keys:\n",
    "            list_df.append(pd.read_hdf(interactions_hdf5_file,i))\n",
    "        df = pd.concat([i for i in list_df], axis=0)\n",
    "    else:\n",
    "        df = pd.read_hdf(interactions_hdf5_file,f'interactions_group_{group_num}')\n",
    "        print('data read in')\n",
    "\n",
    "    def convert_in_reply_to(x):\n",
    "        if x:\n",
    "            return x[0]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    print('converting data format for in_reply_to')\n",
    "    df['in_reply_to'] = df['in_reply_to'].apply(convert_in_reply_to)\n",
    "\n",
    "    # remove self replies\n",
    "    print('removing self_replies')\n",
    "    df = df[df['author_id']!=df['in_reply_to']]\n",
    "\n",
    "    def combine_quotes(row):\n",
    "        if isinstance(row, list):\n",
    "            return row\n",
    "        if isinstance(row, str):\n",
    "            return [str]\n",
    "\n",
    "    # extract quoted tweet ids\n",
    "    print('Extracting quote ids')\n",
    "    df['quotes']= df['quotes'].apply(combine_quotes)\n",
    "    quoted_tweet_ids = []\n",
    "    for row in df.itertuples():\n",
    "        for tweet_id in row.quotes:\n",
    "            quoted_tweet_ids.append(tweet_id)\n",
    "    quoted_tweet_ids = list(set(quoted_tweet_ids))\n",
    "\n",
    "    print('saving to file')\n",
    "    # save quotes to group location\n",
    "    assert os.path.isdir(group_folder)\n",
    "    hydration_filename = os.path.join(group_folder, 'quoted_hydration.txt')\n",
    "    with open(hydration_filename, 'w') as f:\n",
    "        for line in quoted_tweet_ids:\n",
    "            f.write(line)\n",
    "            f.write('\\n')\n",
    "\n",
    "    return df\n",
    "\n",
    "def convert_quotes(quote_list, dictionary):\n",
    "\n",
    "    res = []\n",
    "    for i in quote_list:\n",
    "        if i not in dictionary:\n",
    "            pass\n",
    "        else:\n",
    "            res.append(dictionary[i])\n",
    "    return res\n",
    "\n",
    "def process_hydrated(group_folder, hydrated_filename):\n",
    "    assert os.path.isfile(hydrated_filename)\n",
    "    # collect usernames from hydrated:\n",
    "    quoted_to_user_id = defaultdict(None)\n",
    "    with jsonlines.open(hydrated_filename, 'r') as reader:\n",
    "        for line in reader:\n",
    "            for tweet in line['data']:\n",
    "                quoted_to_user_id[tweet['id']] = tweet['author_id']\n",
    "    return quoted_to_user_id\n",
    "\n",
    "def get_group_daterange():\n",
    "    with h5py.File('/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/02_intermediate/FAS_peak_analysis.hdf5', 'r') as f:\n",
    "        FAS_peaks = f['peak_detections']\n",
    "        x = f['segments']['selected_ranges'][int(group_num)-1]\n",
    "        group_date_range = daterange(\n",
    "            start = x[0].decode(),\n",
    "            end = x[1].decode()\n",
    "        )\n",
    "    return group_date_range\n",
    "\n",
    "def unit_conv(val):\n",
    "    return datetime.datetime.strptime('2017-10-16', '%Y-%m-%d') + datetime.timedelta(days=int(val))\n",
    "\n",
    "def reverse_unit_conv(date):\n",
    "    return (datetime.datetime.strptime(date, '%Y-%m-%d') - datetime.datetime.strptime('2017-10-16', '%Y-%m-%d')).days\n",
    "\n",
    "def get_peaks_for_group(group_date_range):\n",
    "    #obtain peak times again\n",
    "\n",
    "    group_start_index = reverse_unit_conv(group_date_range.start)\n",
    "    group_end_index = reverse_unit_conv(group_date_range.end)\n",
    "\n",
    "\n",
    "    with h5py.File('/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/02_intermediate/FAS_peak_analysis.hdf5', 'r') as f:\n",
    "        FAS_peaks = f['peak_detections']\n",
    "        most_prominent_peaks = {}\n",
    "        for name, h5obj in FAS_peaks.items():\n",
    "\n",
    "            peak_locations = h5obj['peak_locations']\n",
    "            peak_locations = [(i,e) for i,e in enumerate(h5obj['peak_locations']) if (unit_conv(e) > datetime.datetime.strptime(group_date_range.start, '%Y-%m-%d')) and (unit_conv(e) < datetime.datetime.strptime(group_date_range.end, '%Y-%m-%d'))]\n",
    "            peak_indices = [i[0] for i in peak_locations]\n",
    "            prominences = [element for index, element in enumerate(h5obj['prominences']) if index in peak_indices]\n",
    "            if len(prominences) == 0:\n",
    "                continue\n",
    "            max_prominence = np.argmax(prominences)\n",
    "            most_prominent_peaks[name] = unit_conv(peak_locations[max_prominence][1])\n",
    "\n",
    "    return most_prominent_peaks\n",
    "\n",
    "def process_interaction(df):\n",
    "    # get unique counts\n",
    "    df_authors = set(df['author_id'].unique())\n",
    "\n",
    "    print('getting internal counts...')\n",
    "    # generate counts of internal mentions/replies\n",
    "    df['internal'] = (df['in_reply_to'].isin(df['author_id'])) | (df['mentions'].apply(lambda x: any([i in df_authors for i in x]))) | (df['quoted_user_id'].apply(lambda x: any([i in df_authors for i in x])))\n",
    "\n",
    "    internal = df['internal'].sum()\n",
    "    print(f'Internal replies (not self replies): {internal}')\n",
    "    print(f'Total df length: {len(df)}')\n",
    "    user_interaction_counts = df[['author_id','tweet_id']].groupby('author_id').count()\n",
    "\n",
    "    print(user_interaction_counts.describe())\n",
    "\n",
    "    return df\n",
    "\n",
    "df = load_in_data(interactions_hdf5_file)\n",
    "\n",
    "hydrated_filename = os.path.join(group_folder, 'hydration.jsonl')\n",
    "quoted_to_user_id = process_hydrated(group_folder, hydrated_filename)\n",
    "df['quoted_user_id'] = df['quotes'].apply(convert_quotes, args=(quoted_to_user_id,))\n",
    "non_null = len(df[~df['quoted_user_id'].isnull()])\n",
    "print(f'Number of non-null: {non_null}')\n",
    "\n",
    "\n",
    "group_date_range = get_group_daterange()\n",
    "expected_counts = FAS_volume_df[(FAS_volume_df['created_at']<group_date_range.end) & (FAS_volume_df['created_at']>group_date_range.start)].groupby('hashtag').sum()/num_users_group\n",
    "\n",
    "most_prominent_peaks = get_peaks_for_group(group_date_range)\n",
    "\n",
    "df = process_interaction(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet_id', 'author_id', 'tweet_lang', 'text', 'likes', 'reply_count',\n",
       "       'quote_count', 'retweet_count', 'created_at', 'in_reply_to', 'mentions',\n",
       "       'quotes', 'replies', 'contains_hashtags', 'quoted_user_id', 'internal'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hashtag in search_hashtags:\n",
    "    df['vocab:#'+hashtag] = df['contains_hashtags'].apply(lambda x: any(hashtag.lower() == item.lower() for item in x))\n",
    "\n",
    "#columns with vocab: in them\n",
    "vocab_colnames = [i for i in list(df.columns) if 'vocab:#' in i]\n",
    "\n",
    "# df['network_participation'] = df[vocab_colnames].sum(axis=1)\n",
    "# df['multiple_participation'] = df['network_participation']>1\n",
    "\n",
    "# df_participate = df[['author_id','multiple_participation']].groupby('author_id').sum()\n",
    "\n",
    "# print(df_participate.describe())\n",
    "\n",
    "# above_one = (df_participate >= 1).sum()\n",
    "# # print(f'{above_one} users participate in more than one protest network in interactions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To match on activity, let's collect their base timelines and see how much they tweeted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the output variables from this continuous time scope we need the peaks again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start creating the df needed for stats. Each user needs to have\n",
    "* normal activity before and after\n",
    "* hashtagged activity before and after\n",
    "* number of interactions with (each) protest network\n",
    "* protest networks participated in (organise by pairs?)\n",
    "* before and after peaks of each network participation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of hashtags in search ones\n",
    "df['author_total_hashtags'] = df['contains_hashtags'].apply(lambda x: [i for i in x if i.lower() in search_hashtags])\n",
    "# get peaks for each hashtag into the pandas\n",
    "for ht in most_prominent_peaks.keys():\n",
    "    if ht != 'وأناكمان':\n",
    "        with_sym_ht = f'#{ht}'\n",
    "        df[f'peak_{ht}'] = df['created_at'].apply(lambda x: x > most_prominent_peaks[ht].date())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacted users for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_mention_replies(row):\n",
    "    res = []\n",
    "    if row['mentions'] is not None:\n",
    "        for i in row['mentions']:\n",
    "            res.append(i)\n",
    "    if row['quoted_user_id'] is not None:\n",
    "        for i in row['quoted_user_id']:\n",
    "            res.append(i)\n",
    "    if row['in_reply_to'] is not None:\n",
    "        assert isinstance(row['in_reply_to'],str)\n",
    "        res.append(row['in_reply_to'])\n",
    "    return res\n",
    "\n",
    "# collect users interacted with\n",
    "df['interacted_users'] = df.apply(combine_mention_replies,axis=1)\n",
    "\n",
    "# res = []\n",
    "# for ht_target in most_prominent_peaks.keys():\n",
    "#     # print(ht_target)\n",
    "#     # take only the rows for before the peak. Group by \n",
    "#     filtered =  df[df[f'vocab:#{ht_target}']&df['internal']]\n",
    "#     filtered = filtered.groupby('author_id').agg(interacted_total_users = pd.NamedAgg(column=\"interacted_users\", aggfunc=\"sum\"))\n",
    "#     filtered['ht'] = ht_target\n",
    "#     res.append(filtered)\n",
    "\n",
    "# test_df_int = pd.concat([i for i in res], axis=0)\n",
    "# int_df = test_df_int.reset_index()\n",
    "\n",
    "# # only keep rows where users did in fact interact with others\n",
    "# int_df = int_df[int_df['interacted_total_users'].apply(len)>0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about reciprocal interactions as a variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode the rows of interacted users. purely for the purpose of getting these \n",
    "# in their own rows\n",
    "df_exploded = df.explode('interacted_users', ignore_index=True)\n",
    "\n",
    "# only get the ones that are internal interactions\n",
    "df_exploded = df_exploded[df_exploded['internal']]\n",
    "\n",
    "# iterate over hashtags because we want to group by user ids and hashtags.\n",
    "res = []\n",
    "for ht_target in most_prominent_peaks.keys():\n",
    "\n",
    "    # for each hashtag, filter out the hashtag\n",
    "    filtered =  df_exploded[df_exploded[f'vocab:#{ht_target}']]\n",
    "\n",
    "    # then groupby author id and count the number of interactions with each other user.\n",
    "    filtered = filtered.groupby(['author_id','interacted_users']).agg(weight = pd.NamedAgg(column=\"tweet_id\", aggfunc=\"count\"), interacted_total_users = pd.NamedAgg(column=\"interacted_users\", aggfunc=lambda x: list(set(x))))\n",
    "\n",
    "    # add back the hashtag iterating over to the resulting df\n",
    "    filtered['ht'] = ht_target\n",
    "\n",
    "    # append result\n",
    "    res.append(filtered)\n",
    "\n",
    "# concatenate to collect results\n",
    "df_exploded_int = pd.concat([i for i in res], axis=0).reset_index()\n",
    "\n",
    "# create graph from resulting edge list\n",
    "G = nx.from_pandas_edgelist(df_exploded_int, \"author_id\", \"interacted_users\", edge_attr = ['weight', 'ht'], create_using = nx.MultiDiGraph())\n",
    "\n",
    "# nx.draw(G, with_labels = False)\n",
    "\n",
    "# define function to search for reciprocal in the edgelist\n",
    "def get_reciprocal(row, G):\n",
    "\n",
    "    try:\n",
    "\n",
    "        # data=True allows accessing edge weight\n",
    "        edgelist = G.edges(row['interacted_users'], data=True)\n",
    "        res = 0\n",
    "\n",
    "        # search through edgelist to find the REVERSE, i.e. reciprocal. That is, where target is the source of the initial edgelist\n",
    "        for _, target, attributes in edgelist:\n",
    "            if target==row['author_id'] and attributes['ht']==row['ht']:\n",
    "                res+=attributes['weight']\n",
    "        return res\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "# run reciprocal collection\n",
    "df_exploded_int['reciprocal'] = df_exploded_int.apply(get_reciprocal, args = (G,), axis=1)\n",
    "\n",
    "# group by author and hashtag as originally planned\n",
    "int_df = df_exploded_int.groupby(['author_id', 'ht']).agg(\n",
    "    weight = pd.NamedAgg(column='weight', aggfunc='sum'),\n",
    "    reciprocal = pd.NamedAgg(column='reciprocal', aggfunc='sum'),\n",
    "    interacted_total_users = pd.NamedAgg(column=\"interacted_total_users\", aggfunc=\"sum\")\n",
    ").reset_index()\n",
    "\n",
    "# calculate percentages\n",
    "int_df['percent_reciprocal'] = int_df['reciprocal']/int_df['weight']\n",
    "\n",
    "# # remove duplicate\n",
    "# df_exploded_int_grouped['interacted_total_users'] = df_exploded_int_grouped['interacted_total_users'].apply(lambda x: list(set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>ht</th>\n",
       "      <th>weight</th>\n",
       "      <th>reciprocal</th>\n",
       "      <th>interacted_total_users</th>\n",
       "      <th>percent_reciprocal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1005675566</td>\n",
       "      <td>timesup</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[15907183]</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101142807</td>\n",
       "      <td>metoo</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[780302207325310977, 925039712783306752]</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101142807</td>\n",
       "      <td>timesup</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>[780302207325310977, 925039712783306752]</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101142807</td>\n",
       "      <td>wetoo</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>[101142807, 947423877277171712]</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10118982</td>\n",
       "      <td>metoo</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[15754281, 290755398, 941782988819521536]</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3293</th>\n",
       "      <td>987050151159255041</td>\n",
       "      <td>timesup</td>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "      <td>[11744152, 11855772, 1198406491, 119972954, 13...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3294</th>\n",
       "      <td>988406744</td>\n",
       "      <td>metoo</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[807095]</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3295</th>\n",
       "      <td>992431609</td>\n",
       "      <td>timesup</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[15588104]</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3296</th>\n",
       "      <td>995054521</td>\n",
       "      <td>metoo</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[16012783]</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3297</th>\n",
       "      <td>99784623</td>\n",
       "      <td>metoo</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[17525171, 24216951]</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3298 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               author_id       ht  weight  reciprocal  \\\n",
       "0             1005675566  timesup       1           0   \n",
       "1              101142807    metoo       6           0   \n",
       "2              101142807  timesup       7           0   \n",
       "3              101142807    wetoo      13          11   \n",
       "4               10118982    metoo       3           0   \n",
       "...                  ...      ...     ...         ...   \n",
       "3293  987050151159255041  timesup     104           0   \n",
       "3294           988406744    metoo       1           0   \n",
       "3295           992431609  timesup       1           0   \n",
       "3296           995054521    metoo       1           0   \n",
       "3297            99784623    metoo       2           0   \n",
       "\n",
       "                                 interacted_total_users  percent_reciprocal  \n",
       "0                                            [15907183]            0.000000  \n",
       "1              [780302207325310977, 925039712783306752]            0.000000  \n",
       "2              [780302207325310977, 925039712783306752]            0.000000  \n",
       "3                       [101142807, 947423877277171712]            0.846154  \n",
       "4             [15754281, 290755398, 941782988819521536]            0.000000  \n",
       "...                                                 ...                 ...  \n",
       "3293  [11744152, 11855772, 1198406491, 119972954, 13...            0.000000  \n",
       "3294                                           [807095]            0.000000  \n",
       "3295                                         [15588104]            0.000000  \n",
       "3296                                         [16012783]            0.000000  \n",
       "3297                               [17525171, 24216951]            0.000000  \n",
       "\n",
       "[3298 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "      <th>reciprocal</th>\n",
       "      <th>percent_reciprocal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3298.000000</td>\n",
       "      <td>3298.000000</td>\n",
       "      <td>3298.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.933899</td>\n",
       "      <td>1.341722</td>\n",
       "      <td>0.179460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>37.142446</td>\n",
       "      <td>6.658394</td>\n",
       "      <td>1.424806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1969.000000</td>\n",
       "      <td>229.000000</td>\n",
       "      <td>76.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            weight   reciprocal  percent_reciprocal\n",
       "count  3298.000000  3298.000000         3298.000000\n",
       "mean      6.933899     1.341722            0.179460\n",
       "std      37.142446     6.658394            1.424806\n",
       "min       1.000000     0.000000            0.000000\n",
       "25%       1.000000     0.000000            0.000000\n",
       "50%       2.000000     0.000000            0.000000\n",
       "75%       6.000000     1.000000            0.041667\n",
       "max    1969.000000   229.000000           76.333333"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Input: Text in interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea: use the text in interactions to see if frequency of phrases increase?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Input: Python Clustering Eval Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for vocabulary input. First load in the relevant files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bsc model, features names, csr, and user ('document') ids\n",
    "bsc_output_dir = '/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/05_model_output/'\n",
    "bsc_eval_hdf5 = '/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/05_model_output/bsc_cluster_eval.hdf5'\n",
    "mapping_dir = '/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/02_intermediate/'\n",
    "\n",
    "if hashtag_split:\n",
    "    bsc_model_list_dict = {'before':{}, 'after':{}}\n",
    "    feature_mapping_dict = {'before':{}, 'after':{}}\n",
    "    csr_file_dict = {'before':{}, 'after':{}}\n",
    "\n",
    "    for before_str in ['before','after']:\n",
    "        for ht in most_prominent_peaks.keys():\n",
    "            bsc_model_list_dict[before_str][ht] = glob.glob(os.path.join(bsc_output_dir, f'0{group_num}_group',f'*{ht}_{before_str}.obj'))\n",
    "\n",
    "            if not os.path.isfile(bsc_model_list_dict[before_str][ht][0]):\n",
    "                print(bsc_model_list_dict[before_str][ht][0])\n",
    "\n",
    "            # feature_mapping_dict[before_str][ht] = os.path.join(mapping_dir,f'0{group_num}_group',f'mapping_ngram_{ngram_range}_{ht}_{before_str}.obj')\n",
    "\n",
    "            # # sanity check\n",
    "            # if not os.path.isfile(feature_mapping_dict[before_str][ht]):\n",
    "            #     print(feature_mapping_dict[before_str][ht])\n",
    "\n",
    "            # csr_file_dict[before_str][ht] = os.path.join(mapping_dir,f'0{group_num}_group/user_count_mat_ngram_{ngram_range}_{ht}_{before_str}.obj')\n",
    "\n",
    "            # # sanity check\n",
    "            # if not os.path.isfile(feature_mapping_dict[before_str][ht]):\n",
    "            #     print(feature_mapping_dict[before_str][ht])\n",
    "\n",
    "else:\n",
    "    bsc_model_list = glob.glob(os.path.join(bsc_output_dir,f'0{group_num}_group','*.obj'))\n",
    "    feature_mapping_file = os.path.join(mapping_dir,f'0{group_num}_group',f'mapping_ngram_{ngram_range}.obj')\n",
    "    csr_file = os.path.join(mapping_dir,f'0{group_num}_group/user_count_mat_ngram_{ngram_range}.obj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with h5py.File(bsc_eval_hdf5, 'r') as f:\n",
    "#     print(f['group_1']['ngram_34']['min_100']['stilleforopptak']['after']['10'][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class single_cluster(NamedTuple):\n",
    "    n_clusters: int\n",
    "    n_invalid: int\n",
    "    total_ncut: float\n",
    "    norm_ncut: float\n",
    "    hashtag: str\n",
    "    before: str\n",
    "\n",
    "if hashtag_split:\n",
    "\n",
    "    cluster_evals = {'before':{}, 'after':{}}\n",
    "\n",
    "    for before_str in ['before','after']:\n",
    "        for ht in most_prominent_peaks.keys():\n",
    "            cluster_evals[before_str][ht] = []\n",
    "\n",
    "            with h5py.File(bsc_eval_hdf5, 'r') as f:\n",
    "                x = f[f'group_{group_num}'][f'ngram_{ngram_range}'][f'min_{min_user}'][ht][before_str]\n",
    "                try:\n",
    "                    for key, h5_data in x.items():\n",
    "\n",
    "                        # numpy data shape\n",
    "                        h5_data = h5_data[:,1]\n",
    "\n",
    "                        total_ncut = np.sum([i for i in h5_data if i > 0])\n",
    "                        n_invalid = np.sum([i < 0 for i in h5_data])\n",
    "                        cluster_evals[before_str][ht].append(single_cluster(\n",
    "                            n_clusters=int(key),\n",
    "                            n_invalid=n_invalid,\n",
    "                            total_ncut=total_ncut,\n",
    "                            norm_ncut=total_ncut/int(key),\n",
    "                            hashtag=ht,\n",
    "                            before=before_str\n",
    "                        ))\n",
    "                except:\n",
    "                    print(ht, before_str,key)\n",
    "                    continue\n",
    "\n",
    "else:\n",
    "\n",
    "    cluster_evals = []\n",
    "    with h5py.File(bsc_eval_hdf5, 'r') as f:\n",
    "        x = f[f'group_{group_num}'][f'ngram_{ngram_range}'][f'min_{min_user}']\n",
    "        for key, h5_data in x.items():\n",
    "            h5_data = h5_data[:,1]\n",
    "            total_ncut = np.sum([i for i in h5_data if i > 0])\n",
    "            n_invalid = np.sum([i < 0 for i in h5_data])\n",
    "            cluster_evals.append(single_cluster(\n",
    "                n_clusters=int(key),\n",
    "                n_invalid=n_invalid,\n",
    "                total_ncut=total_ncut,\n",
    "                norm_ncut=total_ncut/int(key),\n",
    "                hashtag=\"None\",\n",
    "                before=\"None\"\n",
    "            ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise cluster evaluation, or save the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_best_cluster_dict = {'before':{},'after':{}}\n",
    "if hashtag_split:\n",
    "    for ht in most_prominent_peaks.keys():\n",
    "        try:\n",
    "            for i in ['before', 'after']:\n",
    "\n",
    "                c = cluster_evals[i][ht]\n",
    "\n",
    "                c = sorted(c, key=lambda x: x.n_clusters)\n",
    "\n",
    "                x_data = [i.n_clusters for i in c]\n",
    "                y_data = [i.norm_ncut for i in c]\n",
    "\n",
    "                y_invalid = [i.n_invalid for i in c]\n",
    "\n",
    "                # f, ax1 = plt.subplots(figsize=(15,10))\n",
    "                # ax1.plot(x_data, y_data, color = 'blue')\n",
    "                # ax2 = ax1.twinx()\n",
    "                # ax2.plot(x_data, y_invalid, color='red')\n",
    "                # ax1.set_title(f'NCut evaluation for max clusters on ngram range {ngram_range} and min {min_user}')\n",
    "                # ax1.set_xlabel('k=cluster hyperparameter')\n",
    "                # ax1.set_ylabel('Normalised Cut')\n",
    "                # ax2.set_ylabel('Number of empty clusters')\n",
    "                best_cluster = sorted(c, key=attrgetter('norm_ncut'), reverse=True)[0]\n",
    "                # ax1.annotate(\n",
    "                #     f'({best_cluster.n_clusters}, {best_cluster.norm_ncut:.2f}), n_invalid = {best_cluster.n_invalid}',\n",
    "                #     (best_cluster.n_clusters, best_cluster.norm_ncut),\n",
    "                #     textcoords = 'offset points',\n",
    "                #     xytext = (10,0),\n",
    "                #     ha = 'left'\n",
    "                # );\n",
    "\n",
    "                selected_model = best_cluster.n_clusters\n",
    "                ht_best_cluster_dict[i][ht]=selected_model\n",
    "        except:\n",
    "            print(ht)\n",
    "\n",
    "else:\n",
    "    cluster_evals = sorted(cluster_evals, key=lambda x: x.n_clusters)\n",
    "\n",
    "    x_data = [i.n_clusters for i in cluster_evals]\n",
    "    y_data = [i.norm_ncut for i in cluster_evals]\n",
    "\n",
    "    y_invalid = [i.n_invalid for i in cluster_evals]\n",
    "\n",
    "    f, ax1 = plt.subplots(figsize=(15,10))\n",
    "    ax1.plot(x_data, y_data, color = 'blue')\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(x_data, y_invalid, color='red')\n",
    "    ax1.set_title(f'NCut evaluation for max clusters on ngram range {ngram_range} and min {min_user}')\n",
    "    ax1.set_xlabel('k=cluster hyperparameter')\n",
    "    ax1.set_ylabel('Normalised Cut')\n",
    "    ax2.set_ylabel('Number of empty clusters')\n",
    "    best_cluster = sorted(cluster_evals, key=attrgetter('norm_ncut'), reverse=True)[0]\n",
    "    ax1.annotate(\n",
    "        f'({best_cluster.n_clusters}, {best_cluster.norm_ncut:.2f}), n_invalid = {best_cluster.n_invalid}',\n",
    "        (best_cluster.n_clusters, best_cluster.norm_ncut),\n",
    "        textcoords = 'offset points',\n",
    "        xytext = (10,0),\n",
    "        ha = 'left'\n",
    "    );\n",
    "\n",
    "    selected_model = best_cluster.n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/06_reporting/0{group_num}_group/best_clusters.obj', 'wb') as f:\n",
    "    pickle.dump(ht_best_cluster_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anakaman': datetime.datetime(2018, 5, 24, 0, 0),\n",
      " 'balancetonporc': datetime.datetime(2018, 1, 10, 0, 0),\n",
      " 'cuentalo': datetime.datetime(2018, 4, 30, 0, 0),\n",
      " 'gamani': datetime.datetime(2018, 1, 26, 0, 0),\n",
      " 'jotambe': datetime.datetime(2017, 11, 24, 0, 0),\n",
      " 'memyös': datetime.datetime(2017, 12, 12, 0, 0),\n",
      " 'metoo': datetime.datetime(2018, 3, 9, 0, 0),\n",
      " 'moiaussi': datetime.datetime(2018, 1, 15, 0, 0),\n",
      " 'niere': datetime.datetime(2018, 2, 6, 0, 0),\n",
      " 'niunamenos': datetime.datetime(2017, 11, 25, 0, 0),\n",
      " 'nårdansenstopper': datetime.datetime(2017, 12, 12, 0, 0),\n",
      " 'nårmusikkenstilner': datetime.datetime(2017, 11, 22, 0, 0),\n",
      " 'quellavoltache': datetime.datetime(2018, 3, 8, 0, 0),\n",
      " 'ricebunny': datetime.datetime(2018, 3, 31, 0, 0),\n",
      " 'stilleforopptak': datetime.datetime(2017, 11, 16, 0, 0),\n",
      " 'timesup': datetime.datetime(2018, 1, 8, 0, 0),\n",
      " 'tôicũngvậy': datetime.datetime(2017, 12, 3, 0, 0),\n",
      " 'watashimo': datetime.datetime(2018, 4, 6, 0, 0),\n",
      " 'wetoo': datetime.datetime(2018, 4, 13, 0, 0),\n",
      " 'withyou': datetime.datetime(2018, 4, 6, 0, 0),\n",
      " 'woyeshi': datetime.datetime(2018, 1, 9, 0, 0),\n",
      " 'yotambien': datetime.datetime(2018, 1, 25, 0, 0),\n",
      " 'ятоже': datetime.datetime(2018, 1, 11, 0, 0),\n",
      " '我也是': datetime.datetime(2018, 2, 14, 0, 0),\n",
      " '私も': datetime.datetime(2017, 11, 30, 0, 0),\n",
      " '米兔': datetime.datetime(2018, 2, 7, 0, 0),\n",
      " '나도': datetime.datetime(2018, 3, 23, 0, 0),\n",
      " '나도당했다': datetime.datetime(2018, 2, 27, 0, 0)}\n",
      "daterange(start='2017-11-15', end='2018-05-29')\n"
     ]
    }
   ],
   "source": [
    "pp=pprint.PrettyPrinter()\n",
    "pp.pprint(most_prominent_peaks)\n",
    "pp.pprint(group_date_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_doc_ids = sorted(glob.glob(os.path.join(f'/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/01_raw/0{group_num}_group/','timeline*.jsonl')))\n",
    "user_doc_ids = [re.split('[_.]',i)[-2] for i in user_doc_ids]\n",
    "assert len(user_doc_ids)>0\n",
    "\n",
    "\n",
    "def match_index(search_val, doc_ids):\n",
    "    try:\n",
    "        return np.where(doc_ids == search_val)[0].item()\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "class user_interact_dict(NamedTuple):\n",
    "    author_id: str\n",
    "    author_dict_before: dict\n",
    "    author_dict_after: dict\n",
    "    ht: str\n",
    "\n",
    "author_interacted_res=[]\n",
    "\n",
    "for ht in most_prominent_peaks.keys():\n",
    "\n",
    "    # open bsc model files:\n",
    "    try:\n",
    "        best_cluster_before = ht_best_cluster_dict['before'][ht]\n",
    "        best_cluster_after  = ht_best_cluster_dict['after'][ht]\n",
    "    except KeyError:\n",
    "        print(ht)\n",
    "        continue\n",
    "\n",
    "\n",
    "    bsc_before = os.path.join(bsc_output_dir,f'0{group_num}_group/', f'bsc_python_cluster_{best_cluster_before}_ngram_{ngram_range}_min_{min_user}_{ht}_before.obj')\n",
    "    assert os.path.isfile(bsc_before)\n",
    "    # logging.debug(f'BSC before is {bsc_before}')\n",
    "    bsc_after = os.path.join(bsc_output_dir,f'0{group_num}_group/', f'bsc_python_cluster_{best_cluster_after}_ngram_{ngram_range}_min_{min_user}_{ht}_after.obj')\n",
    "    assert os.path.isfile(bsc_before)\n",
    "    # logging.debug(f'BSC after is {bsc_after}')\n",
    "\n",
    "\n",
    "    # logging.debug('Attempting to load in bsc before')\n",
    "    with open(bsc_before, 'rb') as f:\n",
    "        model_before = pickle.load(f)\n",
    "    # logging.debug('Done. Attempting to load in bsc after')\n",
    "    with open(bsc_after, 'rb') as f:\n",
    "        model_after = pickle.load(f)\n",
    "\n",
    "    mask_file = os.path.join(bsc_output_dir,f'0{group_num}_group/',f'mask_bsc_python_cluster_ngram_{ngram_range}_min_{min_user}_{ht}_before.obj')\n",
    "\n",
    "    with open(mask_file, 'rb') as f:\n",
    "        mask = pickle.load(f)\n",
    "\n",
    "    masked_user_doc_ids = np.array(user_doc_ids)[mask]\n",
    "\n",
    "    # rows_before = 0\n",
    "    # for i in range(int(best_cluster_before)):\n",
    "    #     rows_ind, _ = model_before.get_indices(i)\n",
    "    #     rows_before += rows_ind.shape[0]\n",
    "\n",
    "    # print(rows_before)\n",
    "    # print(len(masked_user_doc_ids))\n",
    "\n",
    "\n",
    "\n",
    "    # logging.debug(f'Filter only to this ht: {ht}')\n",
    "    int_df_sub = int_df[int_df['ht']==ht]\n",
    "\n",
    "    for row in int_df_sub.itertuples():\n",
    "\n",
    "        # logging.debug('gathering indices of where author is in features')\n",
    "        author_index_before = np.where(masked_user_doc_ids == row.author_id)[0]\n",
    "        indices_interacted_before = [match_index(i, masked_user_doc_ids) for i in row.interacted_total_users]\n",
    "        interacted_before = zip(row.interacted_total_users, indices_interacted_before)\n",
    "\n",
    "        author_index_after = np.where(masked_user_doc_ids == row.author_id)[0]\n",
    "        indices_interacted_after = [match_index(i, masked_user_doc_ids) for i in row.interacted_total_users]\n",
    "        interacted_after = zip(row.interacted_total_users, indices_interacted_after)\n",
    "\n",
    "        author_interacted_before = defaultdict(list)\n",
    "        author_interacted_after = defaultdict(list)\n",
    "        if author_index_before.shape[0] > 0:\n",
    "            for i in range(int(best_cluster_before)):\n",
    "                model_before_rows = model_before.get_indices(i)[0]\n",
    "                if author_index_before not in model_before_rows:\n",
    "                    continue\n",
    "                else:\n",
    "                    for user, index in interacted_before:\n",
    "                        if index in model_before_rows:\n",
    "                            author_interacted_before[user] = True\n",
    "                        else:\n",
    "                            author_interacted_before[user] = False\n",
    "\n",
    "        if author_index_after.shape[0] > 0:\n",
    "            for i in range(int(best_cluster_after)):\n",
    "                model_after_rows = model_after.get_indices(i)[0]\n",
    "                if author_index_after not in model_after_rows:\n",
    "                    continue\n",
    "                else:\n",
    "                    for user, index in interacted_after:\n",
    "                        if index in model_after_rows:\n",
    "                            author_interacted_after[user] = True\n",
    "                        else:\n",
    "                            author_interacted_after[user] = False\n",
    "\n",
    "        author_interacted_res.append(\n",
    "            user_interact_dict(\n",
    "                author_id=row.author_id,\n",
    "                author_dict_before=author_interacted_before,\n",
    "                author_dict_after=author_interacted_after,\n",
    "                ht=row.ht\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # for i in author_interacted_before.keys():\n",
    "        #     if not author_interacted_before[i]:\n",
    "        #         if author_interacted_after[i]:\n",
    "        #             print(row.author_id, i, author_interacted_before[i], author_interacted_after[i])\n",
    "\n",
    "        # int_df.loc[row.author_id,'res_before']=author_interacted_before\n",
    "        # int_df.loc[row.author_id,'res_after']=author_interacted_after\n",
    "\n",
    "# combine results into dataframe\n",
    "\n",
    "int_cluster_df = pd.DataFrame(columns = ['author_id', 'interact_user', 'ht', 'same_cluster_before', 'same_cluster_after'])\n",
    "\n",
    "def get_author_dict_bool(d,key):\n",
    "    if d[key] == []:\n",
    "        return False\n",
    "    else:\n",
    "        return d[key]\n",
    "\n",
    "for i in author_interacted_res:\n",
    "    for j,k in i.author_dict_before.items():\n",
    "        int_cluster_df.loc[len(int_cluster_df.index)] = [i.author_id, j, i.ht, k, get_author_dict_bool(i.author_dict_after,j)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in selected model data, csr, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_params_from_file(filename):\n",
    "#     return re.split('[_.]',filename)\n",
    "\n",
    "# selected_model_file = [i for i in bsc_model_list if int(model_params_from_file(i)[-6])==selected_model and model_params_from_file(i)[-4] == ngram_range][0]\n",
    "\n",
    "# with open(selected_model_file, 'rb') as f:\n",
    "#     model = pickle.load(f)\n",
    "\n",
    "# # open mapping also get relevent features names and user ids\n",
    "# with open(feature_mapping_file, 'rb') as f:\n",
    "#     feature_names = pickle.load(f)\n",
    "\n",
    "# # also load in csr\n",
    "# with open(csr_file,'rb') as f:\n",
    "#     csr = pickle.load(f)\n",
    "\n",
    "# # get also the userlist that was used to generate this for 'document' names\n",
    "# user_doc_ids = sorted(glob.glob(os.path.join(f'/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/01_raw/0{group_num}_group/','timeline*.jsonl')))\n",
    "# user_doc_ids = [re.split('[_.]',i)[-2] for i in user_doc_ids]\n",
    "# assert len(user_doc_ids)>0\n",
    "# # function to extract top phrases from cluster\n",
    "# def extract_cluster_words(feature_names, user_ids, csr, model, cluster_num, top_n = 20):\n",
    "#     row_ind, col_ind = model.get_indices(cluster_num)\n",
    "\n",
    "#     csr_cluster = csr[row_ind][:,col_ind]\n",
    "#     feature_csr_sums = csr_cluster.sum(axis=0)\n",
    "#     top_features = np.argsort(feature_csr_sums)\n",
    "#     top_features = np.asarray(top_features).flatten()\n",
    "#     return [feature_names[i] for i in top_features[-1:-top_n:-1]]\n",
    "\n",
    "# cluster_words_dict = {}\n",
    "\n",
    "# for i in range(best_cluster.n_clusters):\n",
    "\n",
    "#     top_n = 50\n",
    "\n",
    "#     cluster_words_dict[i] = extract_cluster_words(\n",
    "#         feature_names,\n",
    "#         user_doc_ids,\n",
    "#         csr,\n",
    "#         model,\n",
    "#         i,\n",
    "#         top_n = top_n\n",
    "#     )\n",
    "\n",
    "#     if len(cluster_words_dict[i]) < top_n-1:\n",
    "#         cluster_words_dict[i] = cluster_words_dict[i] + [None]*(top_n-1-len(cluster_words_dict[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cluster_words = pd.DataFrame.from_dict(cluster_words_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, a statistical model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAS analysis file\n",
    "FAS_peak_analysis_file = '/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/02_intermediate/FAS_peak_analysis.hdf5'\n",
    "\n",
    "# insert activity of users\n",
    "def date_to_array_index(date, daterange):\n",
    "    return (date - datetime.datetime.strptime(daterange.start, '%Y-%m-%d')).days\n",
    "\n",
    "activity_file = '/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/03_processed/activity_counts.hdf5'\n",
    "\n",
    "with h5py.File(activity_file, 'r') as f:\n",
    "    # print(f.keys())\n",
    "    x = f['group_1']['1005675566']['hashtagged'].attrs['feature_order']\n",
    "    ht_row_mapping = x.split(';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do by hashtag. group by users and then whether it was before peak\n",
    "class stats_df_tuple(NamedTuple):\n",
    "    ht_target: str\n",
    "    data: pd.DataFrame\n",
    "\n",
    "def retrieve_activity_user(row, daterange=None, activity_file = None, activity='hashtagged', hashtag=None, group_num = 1, ht_row_mapping = None, averaged = False):\n",
    "\n",
    "    try:\n",
    "        if activity == 'hashtagged':\n",
    "            if row[f'peak_{hashtag}']:\n",
    "                after = date_to_array_index(most_prominent_peaks[hashtag],daterange)\n",
    "                # get ht row index\n",
    "                assert hashtag in ht_row_mapping\n",
    "                ht_index = ht_row_mapping.index(hashtag)\n",
    "                # print(f'group_{group_num}', row['author_id'], after)\n",
    "                result = activity_file[f'group_{group_num}'][row['author_id']][activity][ht_index,after:]\n",
    "            else:\n",
    "                up_to = date_to_array_index(most_prominent_peaks[hashtag], daterange)\n",
    "                # get ht row index\n",
    "                assert hashtag in ht_row_mapping\n",
    "                ht_index = ht_row_mapping.index(hashtag)\n",
    "                # print(f'group_{group_num}', row['author_id'], ht_index, up_to)\n",
    "                result = activity_file[f'group_{group_num}'][row['author_id']][activity][ht_index,:up_to]\n",
    "        else:\n",
    "            if row[f'peak_{hashtag}']:\n",
    "                after = date_to_array_index(most_prominent_peaks[hashtag], daterange)\n",
    "                # print(f'group_{group_num}', row['author_id'], after)\n",
    "                result = activity_file[f'group_{group_num}'][row['author_id']][activity][after:]\n",
    "            else:\n",
    "                up_to = date_to_array_index(most_prominent_peaks[hashtag], daterange)\n",
    "                # print(f'group_{group_num}', row['author_id'], up_to)\n",
    "                result = activity_file[f'group_{group_num}'][row['author_id']][activity][:up_to]\n",
    "\n",
    "        if len(result)>0:\n",
    "            if averaged:\n",
    "                s = np.sum(result)/len(result)\n",
    "                return s\n",
    "            else:\n",
    "                s = np.sum(result)\n",
    "                # print(s)\n",
    "                return s\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        print('not found')\n",
    "        return 0\n",
    "\n",
    "def generate_stats_df_components(df, for_abm = False,\n",
    "        most_prominent_peaks = most_prominent_peaks,\n",
    "        activity_file=activity_file,\n",
    "        group_date_range=group_date_range,\n",
    "        group_num = group_num,\n",
    "        ht_row_mapping = ht_row_mapping,\n",
    "    ):\n",
    "    res = []\n",
    "    with h5py.File(activity_file, 'r') as f:\n",
    "        for ht_target in most_prominent_peaks.keys():\n",
    "            # print(ht_target)\n",
    "            # take only the rows for before the peak. Group by \n",
    "            if for_abm:\n",
    "                filtered =  df[~df[f'peak_{ht_target}']]\n",
    "            else:\n",
    "                filtered =  df[~df[f'peak_{ht_target}'] & df[f'vocab:#{ht_target}']]\n",
    "            num_interacts = filtered.groupby(['author_id']).agg(int_pre_peak = pd.NamedAgg(column=\"tweet_id\", aggfunc=\"count\"))\n",
    "            act_pre_peak = filtered.sort_values('created_at', ascending=False).groupby('author_id').agg(lambda x: x.iloc[0]).reset_index()\n",
    "\n",
    "            if act_pre_peak.empty:\n",
    "                num_interacts['act_pre_peak'] = 0\n",
    "                num_interacts['act_pre_peak_avg'] = 0\n",
    "                num_interacts['norm_act_pre_peak'] = 0\n",
    "                num_interacts['norm_act_pre_peak_avg'] = 0\n",
    "            else:\n",
    "                act_pre_peak['act_pre_peak'] = act_pre_peak.apply(\n",
    "                    retrieve_activity_user,\n",
    "                    daterange=group_date_range,\n",
    "                    activity_file=f,\n",
    "                    hashtag=ht_target,\n",
    "                    group_num=group_num,\n",
    "                    ht_row_mapping = ht_row_mapping,\n",
    "                    axis=1\n",
    "                )\n",
    "\n",
    "                act_pre_peak['act_pre_peak_avg'] = act_pre_peak.apply(\n",
    "                    retrieve_activity_user,\n",
    "                    daterange=group_date_range,\n",
    "                    activity_file=f,\n",
    "                    hashtag=ht_target,\n",
    "                    group_num=group_num,\n",
    "                    ht_row_mapping = ht_row_mapping,\n",
    "                    averaged=True,\n",
    "                    axis=1\n",
    "                )\n",
    "\n",
    "                # add normal activity too\n",
    "                act_pre_peak['norm_act_pre_peak'] = act_pre_peak.apply(\n",
    "                    retrieve_activity_user,\n",
    "                    daterange=group_date_range,\n",
    "                    activity_file=f,\n",
    "                    activity='normal',\n",
    "                    hashtag=ht_target,\n",
    "                    group_num=group_num,\n",
    "                    axis=1\n",
    "                )\n",
    "\n",
    "                act_pre_peak['norm_act_pre_peak_avg'] = act_pre_peak.apply(\n",
    "                    retrieve_activity_user,\n",
    "                    daterange=group_date_range,\n",
    "                    activity_file=f,\n",
    "                    activity='normal',\n",
    "                    hashtag=ht_target,\n",
    "                    group_num=group_num,\n",
    "                    averaged=True,\n",
    "                    axis=1\n",
    "                )\n",
    "\n",
    "                num_interacts = num_interacts.join(act_pre_peak[['author_id','act_pre_peak', 'norm_act_pre_peak','act_pre_peak_avg', 'norm_act_pre_peak_avg']].set_index('author_id'))\n",
    "\n",
    "            if for_abm:\n",
    "                act_post_peak = df[df[f'peak_{ht_target}']]\n",
    "            else:\n",
    "                act_post_peak = df[df[f'peak_{ht_target}'] & df[f'vocab:#{ht_target}']]\n",
    "            act_post_peak = act_post_peak.sort_values('created_at', ascending=False).groupby('author_id').agg(lambda x: x.iloc[0]).reset_index()\n",
    "\n",
    "            if act_post_peak.empty:\n",
    "                num_interacts['act_post_peak'] = 0\n",
    "                num_interacts['act_post_peak_avg'] = 0\n",
    "            else:\n",
    "                act_post_peak['act_post_peak'] = act_post_peak.apply(\n",
    "                    retrieve_activity_user,\n",
    "                    daterange=group_date_range,\n",
    "                    activity_file=f,\n",
    "                    hashtag=ht_target,\n",
    "                    group_num=group_num,\n",
    "                    ht_row_mapping = ht_row_mapping,\n",
    "                    axis=1\n",
    "                )\n",
    "\n",
    "                act_post_peak['act_post_peak_avg'] = act_post_peak.apply(\n",
    "                    retrieve_activity_user,\n",
    "                    daterange=group_date_range,\n",
    "                    activity_file=f,\n",
    "                    hashtag=ht_target,\n",
    "                    group_num=group_num,\n",
    "                    ht_row_mapping = ht_row_mapping,\n",
    "                    averaged=True,\n",
    "                    axis=1\n",
    "                )\n",
    "\n",
    "                temp = act_post_peak[['author_id','act_post_peak', 'act_post_peak_avg']].set_index('author_id')\n",
    "\n",
    "                num_interacts = num_interacts.join(temp)\n",
    "            num_interacts = num_interacts.reset_index().fillna(0)\n",
    "            num_interacts['ht'] = ht_target\n",
    "            res.append(stats_df_tuple(ht_target=ht_target, data=num_interacts))\n",
    "    return res\n",
    "\n",
    "res = generate_stats_df_components(df, for_abm = for_abm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = pd.concat([i.data for i in res], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add primary ht back into stats df\n",
    "# stats_df['primary_ht'] = df['author_id'].apply(process_primary_ht, args = (primary_ht,expected_counts,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>int_pre_peak</th>\n",
       "      <th>act_pre_peak</th>\n",
       "      <th>act_pre_peak_avg</th>\n",
       "      <th>norm_act_pre_peak</th>\n",
       "      <th>norm_act_pre_peak_avg</th>\n",
       "      <th>act_post_peak</th>\n",
       "      <th>act_post_peak_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2726.000000</td>\n",
       "      <td>2726.000000</td>\n",
       "      <td>2726.000000</td>\n",
       "      <td>2726.000000</td>\n",
       "      <td>2726.000000</td>\n",
       "      <td>2726.000000</td>\n",
       "      <td>2726.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>29.019076</td>\n",
       "      <td>83.849963</td>\n",
       "      <td>0.774565</td>\n",
       "      <td>2352.147836</td>\n",
       "      <td>23.098917</td>\n",
       "      <td>32.505503</td>\n",
       "      <td>0.329629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>255.639394</td>\n",
       "      <td>434.945339</td>\n",
       "      <td>3.825799</td>\n",
       "      <td>2786.310440</td>\n",
       "      <td>26.280607</td>\n",
       "      <td>104.381649</td>\n",
       "      <td>1.162335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>19.250000</td>\n",
       "      <td>0.219298</td>\n",
       "      <td>508.000000</td>\n",
       "      <td>5.342105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>0.412281</td>\n",
       "      <td>1415.000000</td>\n",
       "      <td>14.460526</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.144578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>0.703216</td>\n",
       "      <td>3101.250000</td>\n",
       "      <td>30.922515</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.337349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>12260.000000</td>\n",
       "      <td>20189.000000</td>\n",
       "      <td>177.096491</td>\n",
       "      <td>25099.000000</td>\n",
       "      <td>220.166667</td>\n",
       "      <td>3372.000000</td>\n",
       "      <td>40.626506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       int_pre_peak  act_pre_peak  act_pre_peak_avg  norm_act_pre_peak  \\\n",
       "count   2726.000000   2726.000000       2726.000000        2726.000000   \n",
       "mean      29.019076     83.849963          0.774565        2352.147836   \n",
       "std      255.639394    434.945339          3.825799        2786.310440   \n",
       "min        1.000000      0.000000          0.000000           0.000000   \n",
       "25%        1.000000     19.250000          0.219298         508.000000   \n",
       "50%        4.000000     44.000000          0.412281        1415.000000   \n",
       "75%       17.000000     77.000000          0.703216        3101.250000   \n",
       "max    12260.000000  20189.000000        177.096491       25099.000000   \n",
       "\n",
       "       norm_act_pre_peak_avg  act_post_peak  act_post_peak_avg  \n",
       "count            2726.000000    2726.000000        2726.000000  \n",
       "mean               23.098917      32.505503           0.329629  \n",
       "std                26.280607     104.381649           1.162335  \n",
       "min                 0.000000       0.000000           0.000000  \n",
       "25%                 5.342105       0.000000           0.000000  \n",
       "50%                14.460526      13.000000           0.144578  \n",
       "75%                30.922515      33.000000           0.337349  \n",
       "max               220.166667    3372.000000          40.626506  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2120"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stats_df['author_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporate Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # first attempt 2021-12-01: Simply assign cluster assignment as categorical variable\n",
    "\n",
    "# # get rows that correspond to the users\n",
    "# author_model_indices = np.searchsorted(user_doc_ids,stats_df['author_id'])\n",
    "# cluster_labels = model.row_labels_[author_model_indices]\n",
    "\n",
    "# stats_df['cluster'] = cluster_labels\n",
    "# stats_df['cluster'] = stats_df['cluster'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporate likes\n",
    "\n",
    "Though experimentation is not easy to operationalise, perhaps one way of realising a small experimentation is the feedback on individual tries ('tweets') with substantial likes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['likes_deviation'] = df.groupby('author_id')['likes'].transform(lambda x: abs(x - x.mean()) / x.std())\n",
    "df['likes_std_2'] = df['likes_deviation'] > 2\n",
    "\n",
    "likes_res = []\n",
    "for ht_target in most_prominent_peaks.keys():\n",
    "    # print(ht_target)\n",
    "    # take only the rows for before the peak. Group by \n",
    "    if for_abm:\n",
    "        filtered =  df[~df[f'peak_{ht_target}']]\n",
    "    else:\n",
    "        filtered =  df[~df[f'peak_{ht_target}'] & df[f'vocab:#{ht_target}']]\n",
    "    likes_std = filtered.groupby(['author_id']).agg(likes_std = pd.NamedAgg(column=\"likes_std_2\", aggfunc=\"sum\"))\n",
    "    likes_std['ht'] = ht_target\n",
    "    likes_res.append(likes_std)\n",
    "\n",
    "likes_df = pd.concat([i for i in likes_res], axis=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4211"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(likes_df['author_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = stats_df.merge(likes_df, on=['author_id','ht'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4211"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stats_df['author_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process logarithmic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats_df['log_act_post_peak'] = np.log(stats_df['act_post_peak']+1)\n",
    "# stats_df['log_act_pre_peak'] = np.log(stats_df['act_pre_peak']+1)\n",
    "# stats_df['log_int_pre_peak'] = np.log(stats_df['int_pre_peak']+1)\n",
    "# stats_df['log_norm_act_pre_peak'] = np.log(stats_df['norm_act_pre_peak']+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporate m3 inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3_file = f'../data/03_processed/m3inferred_group_{group_num}.json'\n",
    "with open(m3_file,'r') as f:\n",
    "    m3_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df['org'] = stats_df['author_id'].apply(lambda x: m3_results[x]['org']['non-org']<=0.5)\n",
    "\n",
    "def age_m3(id, m3_res):\n",
    "    d = m3_res[id]['age']\n",
    "    return max(d, key=d.get)\n",
    "\n",
    "stats_df['age'] = stats_df['author_id'].apply(age_m3, args=(m3_results,))\n",
    "stats_df['gender'] = stats_df['author_id'].apply(lambda x: m3_results[x]['gender']['female']>=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorporate predicted language on bio as well\n",
    "m3_transformed_file = f'../data/01_raw/0{group_num}_group/m3_users_transformed.jsonl'\n",
    "m3_lang = {}\n",
    "with jsonlines.open(m3_transformed_file, 'r') as f:\n",
    "    for line in f:\n",
    "        m3_lang[line['id']] = line['lang']\n",
    "\n",
    "stats_df['lang'] = stats_df['author_id'].apply(lambda x: m3_lang[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporate hashtag_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_lang_dict = {\n",
    "    'metoo': 'en',\n",
    "    'balancetonporc': 'fr',\n",
    "    'moiaussi': 'fr',\n",
    "    'نه_یعنی_نه': 'ar',\n",
    "    '米兔': 'zh',\n",
    "    '我也是': 'zh',\n",
    "    'gamani': 'he',\n",
    "    'tôicũngvậy': 'vi',\n",
    "    '私も': 'ja',\n",
    "    'watashimo': 'ja',\n",
    "    '나도': 'ko',\n",
    "    '나도당했다': 'ko',\n",
    "    'גםאנחנו' : 'he',\n",
    "    'ятоже': \"ru\",\n",
    "    'ricebunny': 'zh',\n",
    "    'enazeda': \"ar\",\n",
    "    'anakaman': \"ar\",\n",
    "    'yotambien': 'es',\n",
    "    'sendeanlat': 'tr' ,\n",
    "    'kutoo': 'ja',\n",
    "    'withyou': 'ja',\n",
    "    'wetoo': 'ja',\n",
    "    'cuentalo': 'es',\n",
    "    'quellavoltache': 'it',\n",
    "    'niunamenos': 'es',\n",
    "    'woyeshi': 'zh',\n",
    "    'myharveyweinstein': 'en',\n",
    "    'noustoutes': 'fr',\n",
    "    'stilleforopptak': 'no',\n",
    "    'nårdansenstopper': 'no',\n",
    "    'nårmusikkenstilner': 'no',\n",
    "    'memyös': 'fi',\n",
    "    'timesup': 'en',\n",
    "    'niere': 'fr',\n",
    "    'jotambe': 'en'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df['hashtag_lang'] = stats_df.apply(lambda x: hashtag_lang_dict[x['ht']], axis=1)\n",
    "stats_df['lang_diff'] = stats_df.apply(lambda x: x['lang'] != hashtag_lang_dict[x['ht']], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependent Variable: Time before tweet in a different language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perhaps filter out people without lang diff?\n",
    "\n",
    "def user_lang_diff_interval(user_id, timeline_dir, peak_date, ht, user_lang, ht_lang, num=1):\n",
    "\n",
    "    if user_lang == ht_lang:\n",
    "        return np.nan\n",
    "\n",
    "    # get user file\n",
    "    filename = os.path.join(timeline_dir, f'timeline_{user_id}.jsonl')\n",
    "    assert os.path.isfile(filename)\n",
    "\n",
    "    current_best = datetime.timedelta(days=10000)\n",
    "\n",
    "    # read through file\n",
    "    with jsonlines.open(filename, 'r') as reader:\n",
    "        for tweet_jsonl in reader:\n",
    "            tweet_list_in_file = tweet_jsonl['data']\n",
    "            for tweet_data in tweet_list_in_file:\n",
    "                tweet_created_at = datetime.datetime.fromisoformat(tweet_data['created_at'][:-1])\n",
    "                if tweet_created_at < peak_date:\n",
    "                    continue\n",
    "                elif tweet_data['lang'] != user_lang and tweet_data['lang'] == ht_lang and tweet_created_at - peak_date < current_best:\n",
    "                    if 'entities' in tweet_data and 'hashtags' in tweet_data['entities']:\n",
    "                        # print([i['tag'].lower() for i in tweet_data['entities']['hashtags']])\n",
    "                        # if ht.lower() in [i['tag'].lower() for i in tweet_data['entities']['hashtags']]:\n",
    "                            # print('bdas')\n",
    "                        current_best = tweet_created_at - peak_date\n",
    "\n",
    "    if current_best == datetime.timedelta(days=10000):\n",
    "        return np.nan\n",
    "\n",
    "    return current_best\n",
    "\n",
    "if not for_abm:\n",
    "    stats_df['interval'] = stats_df.apply(lambda x: user_lang_diff_interval(\n",
    "        x['author_id'],\n",
    "        f'../data/01_raw/0{group_num}_group',\n",
    "        most_prominent_peaks[x['ht']],\n",
    "        x['ht'],\n",
    "        x['lang'],\n",
    "        x['hashtag_lang']\n",
    "    ), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                          744\n",
       "mean     2 days 21:12:47.325268817\n",
       "std      4 days 04:55:15.855892271\n",
       "min                0 days 00:00:00\n",
       "25%         0 days 09:52:55.500000\n",
       "50%                1 days 00:53:29\n",
       "75%         3 days 13:18:03.250000\n",
       "max               24 days 14:45:33\n",
       "Name: interval, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_df['interval'][~stats_df['interval'].isnull()].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interval in minutes\n",
    "stats_df['interval_minutes'] = stats_df['interval'].apply(lambda x: x.total_seconds()/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3010\n",
      "9674\n"
     ]
    }
   ],
   "source": [
    "print(len(stats_df['author_id'].unique()))\n",
    "print(len(df['author_id'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependent Variable: Language Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_cluster_df['same_cluster_change'] = int_cluster_df['same_cluster_after']-int_cluster_df['same_cluster_before']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_cluster_df_by_author = int_cluster_df.groupby(['ht','author_id'])['same_cluster_change'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>same_cluster_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3248.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.215165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.430269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       same_cluster_change\n",
       "count          3248.000000\n",
       "mean             -0.215165\n",
       "std               0.430269\n",
       "min              -1.000000\n",
       "25%              -0.500000\n",
       "50%               0.000000\n",
       "75%               0.000000\n",
       "max               1.000000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_cluster_df_by_author.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>int_pre_peak</th>\n",
       "      <th>act_pre_peak</th>\n",
       "      <th>norm_act_pre_peak</th>\n",
       "      <th>act_pre_peak_avg</th>\n",
       "      <th>norm_act_pre_peak_avg</th>\n",
       "      <th>act_post_peak</th>\n",
       "      <th>act_post_peak_avg</th>\n",
       "      <th>ht</th>\n",
       "      <th>likes_std</th>\n",
       "      <th>org</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>lang</th>\n",
       "      <th>hashtag_lang</th>\n",
       "      <th>lang_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1005675566</td>\n",
       "      <td>1229</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2972</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.642105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>anakaman</td>\n",
       "      <td>20</td>\n",
       "      <td>False</td>\n",
       "      <td>&gt;=40</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>ar</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101142807</td>\n",
       "      <td>8487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13456</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.821053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>anakaman</td>\n",
       "      <td>392</td>\n",
       "      <td>False</td>\n",
       "      <td>19-29</td>\n",
       "      <td>False</td>\n",
       "      <td>un</td>\n",
       "      <td>ar</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101177558</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>210</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.105263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>anakaman</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>19-29</td>\n",
       "      <td>False</td>\n",
       "      <td>un</td>\n",
       "      <td>ar</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10118982</td>\n",
       "      <td>3117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11782</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.010526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>anakaman</td>\n",
       "      <td>42</td>\n",
       "      <td>False</td>\n",
       "      <td>&gt;=40</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>ar</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1012146463</td>\n",
       "      <td>4284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6419</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.784211</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>anakaman</td>\n",
       "      <td>98</td>\n",
       "      <td>False</td>\n",
       "      <td>30-39</td>\n",
       "      <td>True</td>\n",
       "      <td>es</td>\n",
       "      <td>ar</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109521</th>\n",
       "      <td>99408825</td>\n",
       "      <td>228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1190</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.442308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>나도당했다</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>30-39</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>ko</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109522</th>\n",
       "      <td>995054521</td>\n",
       "      <td>274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.057692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>나도당했다</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "      <td>&lt;=18</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>ko</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109523</th>\n",
       "      <td>995341885</td>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.211538</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>나도당했다</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>19-29</td>\n",
       "      <td>True</td>\n",
       "      <td>un</td>\n",
       "      <td>ko</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109524</th>\n",
       "      <td>997420128</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>나도당했다</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>19-29</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>ko</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109525</th>\n",
       "      <td>99784623</td>\n",
       "      <td>1468</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1697</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.317308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>나도당했다</td>\n",
       "      <td>26</td>\n",
       "      <td>False</td>\n",
       "      <td>30-39</td>\n",
       "      <td>True</td>\n",
       "      <td>es</td>\n",
       "      <td>ko</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109526 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         author_id  int_pre_peak  act_pre_peak  norm_act_pre_peak  \\\n",
       "0       1005675566          1229           0.0               2972   \n",
       "1        101142807          8487           0.0              13456   \n",
       "2        101177558            19           0.0                210   \n",
       "3         10118982          3117           0.0              11782   \n",
       "4       1012146463          4284           0.0               6419   \n",
       "...            ...           ...           ...                ...   \n",
       "109521    99408825           228           0.0               1190   \n",
       "109522   995054521           274           0.0                214   \n",
       "109523   995341885            22           0.0                 22   \n",
       "109524   997420128            13           0.0                 50   \n",
       "109525    99784623          1468           0.0               1697   \n",
       "\n",
       "        act_pre_peak_avg  norm_act_pre_peak_avg  act_post_peak  \\\n",
       "0                    0.0              15.642105            0.0   \n",
       "1                    0.0              70.821053            0.0   \n",
       "2                    0.0               1.105263            0.0   \n",
       "3                    0.0              62.010526            0.0   \n",
       "4                    0.0              33.784211            0.0   \n",
       "...                  ...                    ...            ...   \n",
       "109521               0.0              11.442308            0.0   \n",
       "109522               0.0               2.057692            0.0   \n",
       "109523               0.0               0.211538            0.0   \n",
       "109524               0.0               0.480769            0.0   \n",
       "109525               0.0              16.317308            0.0   \n",
       "\n",
       "        act_post_peak_avg        ht  likes_std    org    age  gender lang  \\\n",
       "0                     0.0  anakaman         20  False   >=40    True   en   \n",
       "1                     0.0  anakaman        392  False  19-29   False   un   \n",
       "2                     0.0  anakaman          1   True  19-29   False   un   \n",
       "3                     0.0  anakaman         42  False   >=40    True   en   \n",
       "4                     0.0  anakaman         98  False  30-39    True   es   \n",
       "...                   ...       ...        ...    ...    ...     ...  ...   \n",
       "109521                0.0     나도당했다          2  False  30-39    True   en   \n",
       "109522                0.0     나도당했다         16   True   <=18   False   en   \n",
       "109523                0.0     나도당했다          2  False  19-29    True   un   \n",
       "109524                0.0     나도당했다          1  False  19-29    True   en   \n",
       "109525                0.0     나도당했다         26  False  30-39    True   es   \n",
       "\n",
       "       hashtag_lang  lang_diff  \n",
       "0                ar       True  \n",
       "1                ar       True  \n",
       "2                ar       True  \n",
       "3                ar       True  \n",
       "4                ar       True  \n",
       "...             ...        ...  \n",
       "109521           ko       True  \n",
       "109522           ko       True  \n",
       "109523           ko       True  \n",
       "109524           ko       True  \n",
       "109525           ko       True  \n",
       "\n",
       "[109526 rows x 16 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stats_df = stats_df.merge(int_cluster_df_by_author, on=['ht', 'author_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>int_pre_peak</th>\n",
       "      <th>act_pre_peak</th>\n",
       "      <th>norm_act_pre_peak</th>\n",
       "      <th>act_pre_peak_avg</th>\n",
       "      <th>norm_act_pre_peak_avg</th>\n",
       "      <th>act_post_peak</th>\n",
       "      <th>act_post_peak_avg</th>\n",
       "      <th>likes_std</th>\n",
       "      <th>same_cluster_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>109526.000000</td>\n",
       "      <td>109526.000000</td>\n",
       "      <td>109526.000000</td>\n",
       "      <td>109526.000000</td>\n",
       "      <td>109526.000000</td>\n",
       "      <td>109526.000000</td>\n",
       "      <td>109526.000000</td>\n",
       "      <td>109526.000000</td>\n",
       "      <td>3203.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>887.690311</td>\n",
       "      <td>3.151224</td>\n",
       "      <td>1656.165038</td>\n",
       "      <td>0.029625</td>\n",
       "      <td>20.325156</td>\n",
       "      <td>2.186896</td>\n",
       "      <td>0.021486</td>\n",
       "      <td>14.535718</td>\n",
       "      <td>-0.217395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1630.119353</td>\n",
       "      <td>77.912486</td>\n",
       "      <td>2889.517207</td>\n",
       "      <td>0.686748</td>\n",
       "      <td>27.906952</td>\n",
       "      <td>29.403709</td>\n",
       "      <td>0.277051</td>\n",
       "      <td>31.122847</td>\n",
       "      <td>0.429901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>53.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.097222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>274.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>574.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.245614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1003.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1866.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.903762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>26378.000000</td>\n",
       "      <td>20189.000000</td>\n",
       "      <td>25099.000000</td>\n",
       "      <td>177.096491</td>\n",
       "      <td>655.400000</td>\n",
       "      <td>5389.000000</td>\n",
       "      <td>40.626506</td>\n",
       "      <td>2013.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        int_pre_peak   act_pre_peak  norm_act_pre_peak  act_pre_peak_avg  \\\n",
       "count  109526.000000  109526.000000      109526.000000     109526.000000   \n",
       "mean      887.690311       3.151224        1656.165038          0.029625   \n",
       "std      1630.119353      77.912486        2889.517207          0.686748   \n",
       "min         1.000000       0.000000           0.000000          0.000000   \n",
       "25%        53.000000       0.000000         132.000000          0.000000   \n",
       "50%       274.000000       0.000000         574.000000          0.000000   \n",
       "75%      1003.000000       0.000000        1866.000000          0.000000   \n",
       "max     26378.000000   20189.000000       25099.000000        177.096491   \n",
       "\n",
       "       norm_act_pre_peak_avg  act_post_peak  act_post_peak_avg      likes_std  \\\n",
       "count          109526.000000  109526.000000      109526.000000  109526.000000   \n",
       "mean               20.325156       2.186896           0.021486      14.535718   \n",
       "std                27.906952      29.403709           0.277051      31.122847   \n",
       "min                 0.000000       0.000000           0.000000       0.000000   \n",
       "25%                 3.097222       0.000000           0.000000       1.000000   \n",
       "50%                10.245614       0.000000           0.000000       4.000000   \n",
       "75%                25.903762       0.000000           0.000000      16.000000   \n",
       "max               655.400000    5389.000000          40.626506    2013.000000   \n",
       "\n",
       "       same_cluster_change  \n",
       "count          3203.000000  \n",
       "mean             -0.217395  \n",
       "std               0.429901  \n",
       "min              -1.000000  \n",
       "25%              -0.500000  \n",
       "50%               0.000000  \n",
       "75%               0.000000  \n",
       "max               1.000000  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_stats_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = new_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = stats_df.merge(int_df[['author_id', 'ht', 'weight', 'reciprocal', 'percent_reciprocal']], on=['ht', 'author_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>int_pre_peak</th>\n",
       "      <th>act_pre_peak</th>\n",
       "      <th>norm_act_pre_peak</th>\n",
       "      <th>act_pre_peak_avg</th>\n",
       "      <th>norm_act_pre_peak_avg</th>\n",
       "      <th>act_post_peak</th>\n",
       "      <th>act_post_peak_avg</th>\n",
       "      <th>ht</th>\n",
       "      <th>likes_std</th>\n",
       "      <th>org</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>lang</th>\n",
       "      <th>hashtag_lang</th>\n",
       "      <th>lang_diff</th>\n",
       "      <th>same_cluster_change</th>\n",
       "      <th>weight</th>\n",
       "      <th>reciprocal</th>\n",
       "      <th>percent_reciprocal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1005675566</td>\n",
       "      <td>1229</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2972</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.642105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>anakaman</td>\n",
       "      <td>20</td>\n",
       "      <td>False</td>\n",
       "      <td>&gt;=40</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>ar</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101142807</td>\n",
       "      <td>8487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13456</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.821053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>anakaman</td>\n",
       "      <td>392</td>\n",
       "      <td>False</td>\n",
       "      <td>19-29</td>\n",
       "      <td>False</td>\n",
       "      <td>un</td>\n",
       "      <td>ar</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101177558</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>210</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.105263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>anakaman</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>19-29</td>\n",
       "      <td>False</td>\n",
       "      <td>un</td>\n",
       "      <td>ar</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10118982</td>\n",
       "      <td>3117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11782</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.010526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>anakaman</td>\n",
       "      <td>42</td>\n",
       "      <td>False</td>\n",
       "      <td>&gt;=40</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>ar</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1012146463</td>\n",
       "      <td>4284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6419</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.784211</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>anakaman</td>\n",
       "      <td>98</td>\n",
       "      <td>False</td>\n",
       "      <td>30-39</td>\n",
       "      <td>True</td>\n",
       "      <td>es</td>\n",
       "      <td>ar</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109521</th>\n",
       "      <td>99408825</td>\n",
       "      <td>228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1190</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.442308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>나도당했다</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>30-39</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>ko</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109522</th>\n",
       "      <td>995054521</td>\n",
       "      <td>274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.057692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>나도당했다</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "      <td>&lt;=18</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>ko</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109523</th>\n",
       "      <td>995341885</td>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.211538</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>나도당했다</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>19-29</td>\n",
       "      <td>True</td>\n",
       "      <td>un</td>\n",
       "      <td>ko</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109524</th>\n",
       "      <td>997420128</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>나도당했다</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>19-29</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>ko</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109525</th>\n",
       "      <td>99784623</td>\n",
       "      <td>1468</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1697</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.317308</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>나도당했다</td>\n",
       "      <td>26</td>\n",
       "      <td>False</td>\n",
       "      <td>30-39</td>\n",
       "      <td>True</td>\n",
       "      <td>es</td>\n",
       "      <td>ko</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109526 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         author_id  int_pre_peak  act_pre_peak  norm_act_pre_peak  \\\n",
       "0       1005675566          1229           0.0               2972   \n",
       "1        101142807          8487           0.0              13456   \n",
       "2        101177558            19           0.0                210   \n",
       "3         10118982          3117           0.0              11782   \n",
       "4       1012146463          4284           0.0               6419   \n",
       "...            ...           ...           ...                ...   \n",
       "109521    99408825           228           0.0               1190   \n",
       "109522   995054521           274           0.0                214   \n",
       "109523   995341885            22           0.0                 22   \n",
       "109524   997420128            13           0.0                 50   \n",
       "109525    99784623          1468           0.0               1697   \n",
       "\n",
       "        act_pre_peak_avg  norm_act_pre_peak_avg  act_post_peak  \\\n",
       "0                    0.0              15.642105            0.0   \n",
       "1                    0.0              70.821053            0.0   \n",
       "2                    0.0               1.105263            0.0   \n",
       "3                    0.0              62.010526            0.0   \n",
       "4                    0.0              33.784211            0.0   \n",
       "...                  ...                    ...            ...   \n",
       "109521               0.0              11.442308            0.0   \n",
       "109522               0.0               2.057692            0.0   \n",
       "109523               0.0               0.211538            0.0   \n",
       "109524               0.0               0.480769            0.0   \n",
       "109525               0.0              16.317308            0.0   \n",
       "\n",
       "        act_post_peak_avg        ht  likes_std    org    age  gender lang  \\\n",
       "0                     0.0  anakaman         20  False   >=40    True   en   \n",
       "1                     0.0  anakaman        392  False  19-29   False   un   \n",
       "2                     0.0  anakaman          1   True  19-29   False   un   \n",
       "3                     0.0  anakaman         42  False   >=40    True   en   \n",
       "4                     0.0  anakaman         98  False  30-39    True   es   \n",
       "...                   ...       ...        ...    ...    ...     ...  ...   \n",
       "109521                0.0     나도당했다          2  False  30-39    True   en   \n",
       "109522                0.0     나도당했다         16   True   <=18   False   en   \n",
       "109523                0.0     나도당했다          2  False  19-29    True   un   \n",
       "109524                0.0     나도당했다          1  False  19-29    True   en   \n",
       "109525                0.0     나도당했다         26  False  30-39    True   es   \n",
       "\n",
       "       hashtag_lang  lang_diff  same_cluster_change  weight  reciprocal  \\\n",
       "0                ar       True                  NaN     NaN         NaN   \n",
       "1                ar       True                  NaN     NaN         NaN   \n",
       "2                ar       True                  NaN     NaN         NaN   \n",
       "3                ar       True                  NaN     NaN         NaN   \n",
       "4                ar       True                  NaN     NaN         NaN   \n",
       "...             ...        ...                  ...     ...         ...   \n",
       "109521           ko       True                  NaN     NaN         NaN   \n",
       "109522           ko       True                  NaN     NaN         NaN   \n",
       "109523           ko       True                  NaN     NaN         NaN   \n",
       "109524           ko       True                  NaN     NaN         NaN   \n",
       "109525           ko       True                  NaN     NaN         NaN   \n",
       "\n",
       "        percent_reciprocal  \n",
       "0                      NaN  \n",
       "1                      NaN  \n",
       "2                      NaN  \n",
       "3                      NaN  \n",
       "4                      NaN  \n",
       "...                    ...  \n",
       "109521                 NaN  \n",
       "109522                 NaN  \n",
       "109523                 NaN  \n",
       "109524                 NaN  \n",
       "109525                 NaN  \n",
       "\n",
       "[109526 rows x 20 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>int_pre_peak</th>\n",
       "      <th>act_pre_peak</th>\n",
       "      <th>norm_act_pre_peak</th>\n",
       "      <th>act_pre_peak_avg</th>\n",
       "      <th>norm_act_pre_peak_avg</th>\n",
       "      <th>act_post_peak</th>\n",
       "      <th>act_post_peak_avg</th>\n",
       "      <th>likes_std</th>\n",
       "      <th>same_cluster_change</th>\n",
       "      <th>weight</th>\n",
       "      <th>reciprocal</th>\n",
       "      <th>percent_reciprocal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>109526.000000</td>\n",
       "      <td>109526.000000</td>\n",
       "      <td>109526.000000</td>\n",
       "      <td>109526.000000</td>\n",
       "      <td>109526.000000</td>\n",
       "      <td>109526.000000</td>\n",
       "      <td>109526.000000</td>\n",
       "      <td>109526.000000</td>\n",
       "      <td>3203.000000</td>\n",
       "      <td>3207.000000</td>\n",
       "      <td>3207.000000</td>\n",
       "      <td>3207.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>887.690311</td>\n",
       "      <td>3.151224</td>\n",
       "      <td>1656.165038</td>\n",
       "      <td>0.029625</td>\n",
       "      <td>20.325156</td>\n",
       "      <td>2.186896</td>\n",
       "      <td>0.021486</td>\n",
       "      <td>14.535718</td>\n",
       "      <td>-0.217395</td>\n",
       "      <td>6.815404</td>\n",
       "      <td>1.330839</td>\n",
       "      <td>0.180657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1630.119353</td>\n",
       "      <td>77.912486</td>\n",
       "      <td>2889.517207</td>\n",
       "      <td>0.686748</td>\n",
       "      <td>27.906952</td>\n",
       "      <td>29.403709</td>\n",
       "      <td>0.277051</td>\n",
       "      <td>31.122847</td>\n",
       "      <td>0.429901</td>\n",
       "      <td>37.282380</td>\n",
       "      <td>6.682254</td>\n",
       "      <td>1.444011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>53.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.097222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>274.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>574.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.245614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1003.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1866.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.903762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>26378.000000</td>\n",
       "      <td>20189.000000</td>\n",
       "      <td>25099.000000</td>\n",
       "      <td>177.096491</td>\n",
       "      <td>655.400000</td>\n",
       "      <td>5389.000000</td>\n",
       "      <td>40.626506</td>\n",
       "      <td>2013.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1969.000000</td>\n",
       "      <td>229.000000</td>\n",
       "      <td>76.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        int_pre_peak   act_pre_peak  norm_act_pre_peak  act_pre_peak_avg  \\\n",
       "count  109526.000000  109526.000000      109526.000000     109526.000000   \n",
       "mean      887.690311       3.151224        1656.165038          0.029625   \n",
       "std      1630.119353      77.912486        2889.517207          0.686748   \n",
       "min         1.000000       0.000000           0.000000          0.000000   \n",
       "25%        53.000000       0.000000         132.000000          0.000000   \n",
       "50%       274.000000       0.000000         574.000000          0.000000   \n",
       "75%      1003.000000       0.000000        1866.000000          0.000000   \n",
       "max     26378.000000   20189.000000       25099.000000        177.096491   \n",
       "\n",
       "       norm_act_pre_peak_avg  act_post_peak  act_post_peak_avg      likes_std  \\\n",
       "count          109526.000000  109526.000000      109526.000000  109526.000000   \n",
       "mean               20.325156       2.186896           0.021486      14.535718   \n",
       "std                27.906952      29.403709           0.277051      31.122847   \n",
       "min                 0.000000       0.000000           0.000000       0.000000   \n",
       "25%                 3.097222       0.000000           0.000000       1.000000   \n",
       "50%                10.245614       0.000000           0.000000       4.000000   \n",
       "75%                25.903762       0.000000           0.000000      16.000000   \n",
       "max               655.400000    5389.000000          40.626506    2013.000000   \n",
       "\n",
       "       same_cluster_change       weight   reciprocal  percent_reciprocal  \n",
       "count          3203.000000  3207.000000  3207.000000         3207.000000  \n",
       "mean             -0.217395     6.815404     1.330839            0.180657  \n",
       "std               0.429901    37.282380     6.682254            1.444011  \n",
       "min              -1.000000     1.000000     0.000000            0.000000  \n",
       "25%              -0.500000     1.000000     0.000000            0.000000  \n",
       "50%               0.000000     2.000000     0.000000            0.000000  \n",
       "75%               0.000000     6.000000     1.000000            0.041667  \n",
       "max               1.000000  1969.000000   229.000000           76.333333  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_stats_df[~new_stats_df['lang_diff'].isnull() & ~new_stats_df['same_cluster_change'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_stats_df = new_stats_df[~new_stats_df['lang_diff']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save progress\n",
    "# combined_analysis_save_file = '/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/06_reporting/combined_analysis.hdf5'\n",
    "\n",
    "# if os.path.isfile(combined_analysis_save_file):\n",
    "#     with h5py.File(combined_analysis_save_file, 'a') as f:\n",
    "#         check = f'group_{group_num}' in f.keys()\n",
    "#         if check and overwrite:\n",
    "#             print('overwriting')\n",
    "#             del f[f'group_{group_num}']\n",
    "#             stats_df.to_hdf(combined_analysis_save_file,key=f'group_{group_num}', format='table')\n",
    "#         elif check and read_in:\n",
    "#             print('reading in')\n",
    "#             stats_df = pd.read_hdf(combined_analysis_save_file, f'group_{group_num}')\n",
    "#         else:\n",
    "#             print('key does not yet exist. Writing...')\n",
    "#             stats_df.to_hdf(combined_analysis_save_file,key=f'group_{group_num}', format='table')\n",
    "\n",
    "# else:\n",
    "#     print('saving to new file')\n",
    "#     stats_df.to_hdf(combined_analysis_save_file,key=f'group_{group_num}', format='table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overwriting\n"
     ]
    }
   ],
   "source": [
    "stats_df_save_dir = '/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/06_reporting/'\n",
    "if for_abm:\n",
    "    save_filename = os.path.join(stats_df_save_dir, f'ABM_stats_df_group_{group_num}.obj')\n",
    "else:\n",
    "    save_filename = os.path.join(stats_df_save_dir, f'stats_df_group_{group_num}.obj')\n",
    "\n",
    "if os.path.isfile(save_filename) and overwrite:\n",
    "    with open(save_filename, 'wb') as f:\n",
    "        pickle.dump(stats_df, f)\n",
    "    print('overwriting')\n",
    "elif os.path.isfile(save_filename) and read_in:\n",
    "    with open(save_filename, 'rb') as f:\n",
    "        stats_df = pickle.load(f)\n",
    "    print('reading in')\n",
    "else:\n",
    "    with open(save_filename, 'wb') as f:\n",
    "        pickle.dump(stats_df, f)\n",
    "    print('saving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overwriting\n"
     ]
    }
   ],
   "source": [
    "# save df raw\n",
    "stats_df_save_dir = '/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/06_reporting/'\n",
    "if for_abm:\n",
    "    save_filename = os.path.join(stats_df_save_dir, f'ABM_raw_df_group_{group_num}.obj')\n",
    "else:\n",
    "    save_filename = os.path.join(stats_df_save_dir, f'raw_df_group_{group_num}.obj')\n",
    "\n",
    "if os.path.isfile(save_filename) and overwrite:\n",
    "    with open(save_filename, 'wb') as f:\n",
    "        pickle.dump(df, f)\n",
    "    print('overwriting')\n",
    "else:\n",
    "    with open(save_filename, 'wb') as f:\n",
    "        pickle.dump(df, f)\n",
    "    print('saving')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_selected(data, response, family=sm.families.family.NegativeBinomial(alpha=2)):\n",
    "    '''Linear model designed by forward selection.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas DataFrame with all possible predictors and response\n",
    "\n",
    "    response: string, name of response column in data\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    model: an \"optimal\" fitted statsmodels linear model\n",
    "           with an intercept\n",
    "           selected by forward selection\n",
    "           evaluated by adjusted R-squared\n",
    "    '''\n",
    "\n",
    "    remaining = set(data.columns)\n",
    "    remaining.remove(response)\n",
    "    selected = []\n",
    "    current_score, best_new_score = 0.0, 0.0\n",
    "    while remaining and current_score == best_new_score:\n",
    "        scores_with_candidates = []\n",
    "        for candidate in remaining:\n",
    "            formula = \"{} ~ {} + 1\".format(response,\n",
    "                                           ' + '.join(selected + [candidate]))\n",
    "            try:\n",
    "                score = smf.glm(formula, data, family=family).fit().aic\n",
    "            except ValueError:\n",
    "                continue\n",
    "            scores_with_candidates.append((score, candidate))\n",
    "        scores_with_candidates.sort()\n",
    "        best_new_score, best_candidate = scores_with_candidates.pop()\n",
    "        if current_score < best_new_score:\n",
    "            remaining.remove(best_candidate)\n",
    "            selected.append(best_candidate)\n",
    "            current_score = best_new_score\n",
    "    formula = \"{} ~ {} + 1\".format(response,\n",
    "                                   ' + '.join(selected))\n",
    "    model = smf.glm(formula, data, family=family).fit()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further features:\n",
    "\n",
    "Revisiting Chabot (2010):\n",
    "\n",
    "![Diffusion](diffusion.png)\n",
    "\n",
    "* Awareness is hypothesised by Chabot to be through mass media, for example. Not operationalisable here and not really of interest either.\n",
    "* Translation is the 'meaningful' dialogue between potential transmitters and receivers. This is the key thing to operationalise and why I wanted clustering in the first place\n",
    "    * perhaps manual coding of key phrases\n",
    "    * feature measuring the \n",
    "* Experimentation\n",
    "    * This is extremely difficult to model - how can I infer from tweets whether they are 'testing' something out?\n",
    "    * Theory needs developing here for internet age - Chabot writes '[experimentation] begins with small-scale direct action campaigns and implies intensive dialogue among adoption pioneers'\n",
    "        * potential operationalisation - measuring more intense periods of interaction WITHIN a hashtag version before peak\n",
    "        * but small-scale direct action campaigns are not really operationalisable in this context.\n",
    "    * or another mechanism is likes on one tweet in one interaction.\n",
    "* Movement Application -> this is my outcome variable, operationalised as amount of integration into target hashtag version\n",
    "    * another measure - interval before first tweet in another language?\n",
    "    * make lots of scatter plots\n",
    "    * scattermatrix pandas\n",
    "    * more GLMs\n",
    "\n",
    "* Over and above all of this, who is an activist?\n",
    "    * but should we even make that distinction? and is that one for me to make? no\n",
    "    * maybe it made sense in collective action models to differentiate between those who participate in protests, for example, but doing this online is less clear.\n",
    "    * That's not to say we can't make meaningful segmentations, though. Could do Verified/not verified (there's literature to suggest that's a meaningful difference), and perhaps\n",
    "    * **think about separating by organisation**\n",
    "    * just draw line on activity? threshold. Can vary parameter and check robustness obviously.\n",
    "    * That would be an honest way to separate the users\n",
    "\n",
    "Next steps:\n",
    "\n",
    "* expand measure of activity to not be a simple aggregate of before and after a peak. Perhaps a few days before and a few days after? Segment time into weeks?\n",
    "    * or some measure of density of interactions\n",
    "* cluster without hashtags?\n",
    "\n",
    "\n",
    "From Earl (2010):\n",
    "\n",
    "![Earl](Earl-2010.png)\n",
    "\n",
    "Earl contends that there social movement scholars haven't studied second order effects of diffusion, but doesn't really discuss the diffusion mechanics itself\n",
    "\n",
    "From Penney and Dadas (2014) -> rapid distribution of text through Twitter allowed protestors to buidl a geeographically dispersed, netwroked counterpublic. This is captured by the timed clusters.\n",
    "\n",
    "However, their interviews also stress the difficulty of using Twitter to engage in discussion as it gets frustrating and debate isn't meaningful.\n",
    "\n",
    "They also mention how Twiter helped strengthen ties but not necessarily across countries, and Occupy also had more of an offline component. \n",
    "\n",
    "NOTE TO SELF: USE TWITTER PLATFORM AFFORANCES TO GENERATE HYPOTHESIS\n",
    "* e.g. think of Massanari (2017).\n",
    "* Draw from harassment literature (ask Alexa). Women who get harassed online -> less likely to interact or to participate?\n",
    "\n",
    "\n",
    "From Vasi Suh (2016):\n",
    "* they also do stats models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36cf16204b8548560b1c020c4e8fb5b57f0e4c58016f52f2d4be01e192833930"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
