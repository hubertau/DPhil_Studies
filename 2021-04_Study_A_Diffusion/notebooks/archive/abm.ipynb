{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99601858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import datetime\n",
    "import tqdm\n",
    "from typing import NamedTuple\n",
    "import h5py\n",
    "\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import Counter, defaultdict\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import bernoulli, binom\n",
    "sns.set('talk')\n",
    "sns.set_style('ticks')\n",
    "\n",
    "#load in search hashtags\n",
    "with open('../references/search_hashtags.txt', 'r') as f:\n",
    "    search_hashtags = f.readlines()\n",
    "    search_hashtags = [i.replace('\\n', '') for i in search_hashtags]\n",
    "    search_hashtags = [i.replace('#', '') for i in search_hashtags]\n",
    "    search_hashtags = [i.lower() for i in search_hashtags]\n",
    "    search_hashtags.remove('وأناكمان')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5610e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/06_reporting/ABM_output_group_2_batch_2.hdf5', 'r') as f:\n",
    "    for i in tqdm.tqdm(range(12000)):\n",
    "        x = f['batch_result'][24,i,:].sum()\n",
    "        if x > 0:\n",
    "            print(f'found. {x}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbd68bf",
   "metadata": {},
   "source": [
    "# 0. Import real data and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29044dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Set Parameters\n",
    "################################################################################\n",
    "group_num = 3\n",
    "hashtag_split = True\n",
    "ngram_range = '34'\n",
    "min_user = 100\n",
    "\n",
    "overwrite = False\n",
    "read_in = True\n",
    "\n",
    "################################################################################\n",
    "# Set relevant file paths\n",
    "################################################################################\n",
    "\n",
    "plot_save_path = f'/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/results/0{group_num}_group/'\n",
    "\n",
    "follows_dir = f'/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/01_raw/0{group_num}_group/'\n",
    "\n",
    "activity_file = '/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/03_processed/activity_counts.hdf5'\n",
    "\n",
    "peak_analysis_file = '/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/02_intermediate/FAS_peak_analysis.hdf5'\n",
    "\n",
    "# abm df save path.\n",
    "abm_processed_df_savepath = f'/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/06_reporting/ABM_processed_df_group_{group_num}.obj'\n",
    "\n",
    "graph_savepath = f'/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/06_reporting/ABM_graph_group_{group_num}.obj'\n",
    "\n",
    "################################################################################\n",
    "# Define useful functions\n",
    "################################################################################\n",
    "\n",
    "def unit_conv(val):\n",
    "    return datetime.datetime.strptime('2017-10-16', '%Y-%m-%d') + datetime.timedelta(days=int(val))\n",
    "\n",
    "def reverse_unit_conv(date):\n",
    "    return (datetime.datetime.strptime(date, '%Y-%m-%d') - datetime.datetime.strptime('2017-10-16', '%Y-%m-%d')).days\n",
    "\n",
    "class daterange(NamedTuple):\n",
    "    start: str\n",
    "    end: str\n",
    "\n",
    "def date_to_array_index(date, daterange):\n",
    "    return (datetime.datetime.strptime(date, '%Y-%m-%d') - datetime.datetime.strptime(daterange.start, '%Y-%m-%d')).days\n",
    "\n",
    "def group_peaks_and_daterange(peak_analysis_file, group_num):\n",
    "\n",
    "    #obtain peak times again\n",
    "    with h5py.File(peak_analysis_file, 'r') as f:\n",
    "        FAS_peaks = f['peak_detections']\n",
    "        x = f['segments']['selected_ranges'][int(group_num)-1]\n",
    "        group_date_range = daterange(\n",
    "            start = x[0].decode(),\n",
    "            end = x[1].decode()\n",
    "        )\n",
    "\n",
    "        # group_start_index = reverse_unit_conv(group_date_range.start)\n",
    "        # group_end_index = reverse_unit_conv(group_date_range.end)\n",
    "\n",
    "        most_prominent_peaks = {}\n",
    "        for name, h5obj in FAS_peaks.items():\n",
    "\n",
    "            peak_locations = h5obj['peak_locations']\n",
    "            peak_locations = [(i,e) for i,e in enumerate(h5obj['peak_locations']) if (unit_conv(e) > datetime.datetime.strptime(group_date_range.start, '%Y-%m-%d')) and (unit_conv(e) < datetime.datetime.strptime(group_date_range.end, '%Y-%m-%d'))]\n",
    "            peak_indices = [i[0] for i in peak_locations]\n",
    "            prominences = [element for index, element in enumerate(h5obj['prominences']) if index in peak_indices]\n",
    "            if len(prominences) == 0:\n",
    "                continue\n",
    "            max_prominence = np.argmax(prominences)\n",
    "            most_prominent_peaks[name] = unit_conv(peak_locations[max_prominence][1])\n",
    "\n",
    "    daterange_length = (datetime.datetime.strptime(group_date_range.end, '%Y-%m-%d') - datetime.datetime.strptime(group_date_range.start, '%Y-%m-%d')).days\n",
    "\n",
    "    return most_prominent_peaks, group_date_range, daterange_length\n",
    "\n",
    "################################################################################\n",
    "# Read in peaks\n",
    "################################################################################\n",
    "\n",
    "most_prominent_peaks, group_date_range, daterange_length = group_peaks_and_daterange(peak_analysis_file, group_num)\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# Determine if ABM df has already been processed.\n",
    "################################################################################\n",
    "\n",
    "if (os.path.isfile(abm_processed_df_savepath) and overwrite) or not os.path.isfile(abm_processed_df_savepath):\n",
    "\n",
    "    print('overwriting or writing for the first time')\n",
    "\n",
    "    # read df raw for ABM\n",
    "    stats_df_save_dir = '/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/06_reporting/'\n",
    "    df_filename = os.path.join(stats_df_save_dir, f'ABM_raw_df_group_{group_num}.obj')\n",
    "    stats_filename = os.path.join(stats_df_save_dir, f'ABM_stats_df_group_{group_num}.obj')\n",
    "\n",
    "    if os.path.isfile(df_filename):\n",
    "        print('reading in df')\n",
    "        with open(df_filename, 'rb') as f:\n",
    "            df = pickle.load(f)\n",
    "    if os.path.isfile(stats_filename):\n",
    "        print('reading in stats_df')\n",
    "        with open(stats_filename, 'rb') as f:\n",
    "            stats_df = pickle.load(f)\n",
    "\n",
    "    print('N.B. users are not included in stats df because in creating the activity counts users were split into before and after peak interactions')\n",
    "\n",
    "    print(f'Length of df: {len(df)}')\n",
    "    unique_author_stats_df_count = len(stats_df['author_id'].unique())\n",
    "    print(f'Number of unique authors in stats df: {unique_author_stats_df_count}')\n",
    "    unique_author_df_count = len(df['author_id'].unique())\n",
    "    print(f'Number of unique authors in df: {unique_author_df_count}')\n",
    "\n",
    "    # generate ht column\n",
    "    df_colnames = df.columns\n",
    "    vocab_colnames = [i for i in df_colnames if i.startswith('vocab')][::-1]\n",
    "    def process_row_ht(row):\n",
    "        for col in vocab_colnames:\n",
    "            if col == 'vocab:#timesup':\n",
    "                continue\n",
    "            if row[col] == 1:\n",
    "                return col.split('#')[-1]\n",
    "        return None\n",
    "\n",
    "    df['ht'] = df.apply(process_row_ht, axis=1)\n",
    "    df['ht'] = df['ht'].fillna('metoo')\n",
    "\n",
    "    df = df.merge(stats_df, on=['author_id','ht'], how='right')\n",
    "\n",
    "    # incorporate primary ht\n",
    "    with open(f'/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/03_processed/primary_ht_global.obj', 'rb') as f:\n",
    "        user_order, res = pickle.load(f)\n",
    "\n",
    "    unknown_count = 0\n",
    "    def process_primary_res(author_id):\n",
    "        global unknown_count\n",
    "        if author_id not in user_order:\n",
    "            # print(f'{author_id} not in users')\n",
    "            unknown_count += 1\n",
    "            return 'metoo'\n",
    "        return search_hashtags[np.argmax(res[user_order.index(author_id),:])]\n",
    "\n",
    "\n",
    "    df['primary_ht'] = df['author_id'].map(df.groupby('author_id').apply(lambda x: process_primary_res(x.name)))\n",
    "    print(f'Number of unknown primary hashtags for users: {unknown_count}')\n",
    "\n",
    "    df['interacted_users'].loc[df['interacted_users'].isnull()] = df['interacted_users'].loc[df['interacted_users'].isnull()].apply(lambda x: [])\n",
    "\n",
    "    with open(abm_processed_df_savepath, 'wb') as f:\n",
    "        pickle.dump(df, f)\n",
    "\n",
    "\n",
    "elif os.path.isfile(abm_processed_df_savepath) and read_in:\n",
    "    print('reading in')\n",
    "    with open(abm_processed_df_savepath, 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "\n",
    "################################################################################\n",
    "# Generate the \n",
    "################################################################################\n",
    "\n",
    "temp_df = df.groupby('author_id').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5c4fd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# archive:\n",
    "\n",
    "# df[['text','lang']]\n",
    "\n",
    "#### Language detection for individual tweets - possibly something to pass to ARC.\n",
    "# from langdetect import detect\n",
    "# import langdetect\n",
    "\n",
    "# def proper_detect(text):\n",
    "#     try:\n",
    "#         return detect(text)\n",
    "#     except:\n",
    "#         return None\n",
    "\n",
    "# df['text'].apply(proper_detect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3b3174",
   "metadata": {},
   "source": [
    "# 1. Make the user network\n",
    "\n",
    "Grow a graph using the Barabási-Albert preferential attachment model.\n",
    "\n",
    "A graph of `n` nodes is grown by attaching new nodes each with `m` edges that are preferentially attached to existing nodes with high degree.\n",
    "\n",
    "https://networkx.org/documentation/networkx-1.9.1/reference/generated/networkx.generators.random_graphs.barabasi_albert_graph.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a782516f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet_id', 'author_id', 'tweet_lang', 'text', 'likes', 'created_at',\n",
       "       'in_reply_to', 'mentions', 'quotes', 'replies', 'contains_hashtags',\n",
       "       'quoted_user_id', 'internal', 'vocab:#metoo', 'vocab:#balancetonporc',\n",
       "       'vocab:#moiaussi', 'vocab:#نه_یعنی_نه', 'vocab:#米兔', 'vocab:#我也是',\n",
       "       'vocab:#gamani', 'vocab:#tôicũngvậy', 'vocab:#私も', 'vocab:#watashimo',\n",
       "       'vocab:#나도', 'vocab:#나도당했다', 'vocab:#גםאנחנו', 'vocab:#ятоже',\n",
       "       'vocab:#ricebunny', 'vocab:#enazeda', 'vocab:#anakaman',\n",
       "       'vocab:#yotambien', 'vocab:#sendeanlat', 'vocab:#kutoo',\n",
       "       'vocab:#withyou', 'vocab:#wetoo', 'vocab:#cuentalo',\n",
       "       'vocab:#quellavoltache', 'vocab:#niunamenos', 'vocab:#woyeshi',\n",
       "       'vocab:#myharveyweinstein', 'vocab:#noustoutes',\n",
       "       'vocab:#stilleforopptak', 'vocab:#nårdansenstopper',\n",
       "       'vocab:#nårmusikkenstilner', 'vocab:#memyös', 'vocab:#timesup',\n",
       "       'vocab:#niere', 'vocab:#jotambe', 'author_total_hashtags', 'peak_kutoo',\n",
       "       'peak_metoo', 'peak_moiaussi', 'peak_niunamenos', 'peak_tôicũngvậy',\n",
       "       'peak_watashimo', 'peak_ятоже', 'peak_나도당했다', 'interacted_users',\n",
       "       'likes_deviation', 'likes_std_2', 'ht', 'int_pre_peak', 'act_pre_peak',\n",
       "       'norm_act_pre_peak', 'act_pre_peak_avg', 'norm_act_pre_peak_avg',\n",
       "       'act_post_peak', 'act_post_peak_avg', 'likes_std', 'org', 'age',\n",
       "       'gender', 'lang', 'hashtag_lang', 'lang_diff', 'same_cluster_change',\n",
       "       'weight', 'reciprocal', 'percent_reciprocal', 'primary_ht'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head().columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08753981",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graph_savepath' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_54587/25741222.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_savepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loading in'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_savepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'graph_savepath' is not defined"
     ]
    }
   ],
   "source": [
    "def generate_network(df, follows_dir = None):\n",
    "\n",
    "    # construct graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # add nodes to graph\n",
    "    unique_users = df['author_id'].unique()\n",
    "\n",
    "    # for the most common hasthags used by an interacted user\n",
    "    filtered_ht = df.groupby('author_id')[['ht','gender','age','int_pre_peak','act_pre_peak','norm_act_pre_peak','org','lang', 'primary_ht']].agg(pd.Series.mode)\n",
    "\n",
    "    # attributes\n",
    "    # G = nx.path_graph(3)\n",
    "    # attrs = {0: {\"attr1\": 20, \"attr2\": \"nothing\"}, 1: {\"attr2\": 3}}\n",
    "    # nx.set_node_attributes(G, attrs)\n",
    "\n",
    "    # switch correct type of graph being constructed:\n",
    "\n",
    "    # OLD AS OF 11 MAY 2022\n",
    "    # if gen_from == 'interaction':\n",
    "\n",
    "    #     # first add all nodes\n",
    "    #     for row in df.itertuples(index=False):\n",
    "    #         G.add_node(row.author_id)\n",
    "    #         if type(row.interacted_users) == float and np.isnan(row.interacted_users):\n",
    "    #             continue\n",
    "    #         for interacted_user in row.interacted_users:\n",
    "    #             if interacted_user not in unique_users:\n",
    "    #                 continue\n",
    "    #             G.add_edge(row.author_id, interacted_user)\n",
    "\n",
    "    #     print(2*G.number_of_edges() / float(G.number_of_nodes()))\n",
    "\n",
    "    # sanity check: followers data is collected\n",
    "    assert os.path.isdir(follows_dir)\n",
    "\n",
    "    successful_follows=0\n",
    "    # extract users whom a user follows and following\n",
    "    for user_id in tqdm.tqdm(unique_users):\n",
    "\n",
    "        # add user to graph as a node with attributes.\n",
    "        G.add_node(\n",
    "            user_id,\n",
    "            gender       = filtered_ht.loc[user_id]['gender'],\n",
    "            age          = filtered_ht.loc[user_id]['age'],\n",
    "            int_pre_peak = filtered_ht.loc[user_id]['int_pre_peak'],\n",
    "            act_pre_peak = filtered_ht.loc[user_id]['act_pre_peak'],\n",
    "            norm_act_pre_peak = filtered_ht.loc[user_id]['norm_act_pre_peak'],\n",
    "            org               = filtered_ht.loc[user_id]['org'],\n",
    "            lang              = filtered_ht.loc[user_id]['lang'],\n",
    "            primary_ht        = filtered_ht.loc[user_id]['primary_ht']\n",
    "        )\n",
    "\n",
    "        # print(f'Processing {user_id}')\n",
    "        follows_filepath = os.path.join(follows_dir, f'following_{user_id}.txt')\n",
    "        try:\n",
    "            total_edges_to_add = df[df['author_id'] == user_id]['interacted_users'].sum()\n",
    "        except:\n",
    "            print(user_id)\n",
    "        if isinstance(total_edges_to_add, int):\n",
    "            total_edges_to_add = np.array([])\n",
    "        total_edges_to_add = np.intersect1d(total_edges_to_add, unique_users)\n",
    "\n",
    "        if os.path.isfile(follows_filepath):\n",
    "            try:\n",
    "                edges_to = pd.read_table(follows_filepath).values.flatten().astype(str)\n",
    "                successful_follows+=1\n",
    "                new_total_edges_to_add = np.union1d(total_edges_to_add, edges_to)\n",
    "                new_total_edges_to_add = np.intersect1d(new_total_edges_to_add, unique_users)\n",
    "                length_diff = len(new_total_edges_to_add) - len(total_edges_to_add)\n",
    "                assert length_diff >= 0\n",
    "                # print(f'Length diff: {length_diff} for {user_id}')\n",
    "                total_edges_to_add = new_total_edges_to_add\n",
    "            except pd.errors.EmptyDataError:\n",
    "                pass\n",
    "\n",
    "        for interacted_user in total_edges_to_add:\n",
    "            ht = filtered_ht.loc[interacted_user]['ht']\n",
    "            G.add_edge(user_id, interacted_user, ht=ht)\n",
    "\n",
    "    print(f'Total successfully read follows: {successful_follows}')\n",
    "\n",
    "    # then add attributes:\n",
    "\n",
    "    # for k,v in user_edges.items():\n",
    "\n",
    "    #     # add k\n",
    "    #     G.add_node(str(k))\n",
    "\n",
    "    #     v_counter = Counter(v)\n",
    "    #     v_edge_list = [(str(k), str(node), count) for node,count in v_counter.items() if str(node) in user_edges_keys]\n",
    "\n",
    "    #     G.add_weighted_edges_from(v_edge_list)\n",
    "\n",
    "    # with open(graph_object_file, 'wb') as f:\n",
    "    #     pickle.dump(G, f)\n",
    "    return G\n",
    "\n",
    "\n",
    "if os.path.isfile(graph_savepath):\n",
    "    print('loading in')\n",
    "    with open(graph_savepath, 'rb') as f:\n",
    "        G = pickle.load(f)\n",
    "else:\n",
    "    print('generating network')\n",
    "    G = generate_network(df, follows_dir)\n",
    "    with open(graph_savepath, 'wb') as f:\n",
    "        pickle.dump(G,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a104193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['author_id']=='16996244']['interacted_users']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f822967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positions = nx.spring_layout(G)\n",
    "# nx.draw(G, node_size=25, pos=positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c067ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# degrees        = dict(G.degree()).values()\n",
    "# sorted_degrees = sorted(degrees)[::-1]\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(20,5))\n",
    "\n",
    "# ax.bar(x=range(len(sorted_degrees)), height=sorted_degrees)\n",
    "\n",
    "# plt.xlabel('Nodes')\n",
    "# plt.ylabel('Node degree')\n",
    "# plt.xlim(-5,len(degrees)+5)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb13da1",
   "metadata": {},
   "source": [
    "## Get statistics from data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862de33f",
   "metadata": {},
   "source": [
    "# 2. Make the agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fab1f10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object): \n",
    "\n",
    "    # initialise internal variables\n",
    "    def __init__(self, ID, df, search_hashtags):\n",
    "        self.ID = ID\n",
    "        self.supporting_metoo = False    # initial assumption.\n",
    "        self.supporting_metoo_dict = {i:0 for i in search_hashtags}\n",
    "        self.interacts_with   = []       # I wrote this to represent a symmetric interaction, rather than following.\n",
    "        # self.interaction_counter = defaultdict(int)    # counts interactions\n",
    "        self.forget_all_interactions()\n",
    "        # print(ID)\n",
    "        # row = df[df['author_id'] == ID].iloc[0,:]\n",
    "        # self.total_hashtags = row.author_total_hashtags\n",
    "        self.primary_ht = df.loc[ID,'primary_ht']\n",
    "        self.support_tracker = np.zeros(shape=(35,1))\n",
    "        self.activity_tracker = np.zeros(shape=(35,1))\n",
    "        self.individual_propensity = np.zeros(shape=(35,1))\n",
    "        self.experimentation_count = 0\n",
    "\n",
    "    def update_tracker(self):\n",
    "\n",
    "        # support_update_array = np.zeros(shape=(35,1))\n",
    "        # individual_prop_update_array = np.zeros(shape=(35,1)) \n",
    "        # print('processing propensities')\n",
    "\n",
    "        # index=0\n",
    "        # for k,v in self.propensity_params.items():\n",
    "        #     support_update_array[index,0] = self.supporting_metoo_dict[k]\n",
    "        #     individual_prop_update_array[index,0] = (1-v)**(support_update_array[index,0])\n",
    "        #     index += 1\n",
    "\n",
    "        # start_time = time.time()\n",
    "\n",
    "        support_update_array = np.array(list(self.supporting_metoo_dict.values())).reshape(-1,1)\n",
    "        # individual_prop_update_array = (1-np.array(list(self.propensity_params.values())))**support_update_array\n",
    "\n",
    "        # individual_prop_update_array = individual_prop_update_array.reshape(-1,1)\n",
    "\n",
    "        self.support_tracker = np.hstack((self.support_tracker, support_update_array))\n",
    "        # print(f'support array shape for {self.ID}: {self.support_tracker.shape}')\n",
    "        # self.individual_propensity = np.hstack((self.individual_propensity, individual_prop_update_array))\n",
    "\n",
    "        # timetaken = time.time() - start_time\n",
    "        # print(f'end sim. Time taken: {timetaken}')\n",
    "\n",
    "    def simulate(self, search_hashtag_propensity):\n",
    "\n",
    "        keys = list(self.supporting_metoo_dict.keys())\n",
    "        propensities = np.array([search_hashtag_propensity[i] for i in keys])\n",
    "\n",
    "        self.probability_matrix = np.power(1-propensities.reshape(-1,1),self.support_tracker)\n",
    "\n",
    "        self.simulated = np.random.binomial(1, self.probability_matrix)\n",
    "\n",
    "        # return self.probability_matrix\n",
    "\n",
    "        # update_array = np.zeros(shape=(35,1))\n",
    "        # for index, v in enumerate(self.individual_propensity[:,-1]):\n",
    "        #     b = bernoulli(v)\n",
    "        #     update_array[index, 0] = 1-b.rvs(1)[0]\n",
    "        # update_array = update_array.reshape(-1,1)\n",
    "        # self.activity_tracker = np.hstack((self.activity_tracker,update_array))\n",
    "\n",
    "    def interact(self, other, experimentation_success_chance):\n",
    "\n",
    "        # Keep track of interactions with others.\n",
    "        # This step is asymmetric: 'self' keeps track, but 'other' does not.\n",
    "\n",
    "        if other.ID not in self.interaction_counter:\n",
    "            self.interaction_counter[other.ID]  = 1\n",
    "        else:\n",
    "            self.interaction_counter[other.ID] += 1\n",
    "\n",
    "        # For later models implementing likes\n",
    "        experimentation_trial = np.random.uniform()\n",
    "        if experimentation_trial <= experimentation_success_chance:\n",
    "            self.experimentation_success = True\n",
    "        else:\n",
    "            self.experimentation_success = False\n",
    "        self.experimentation_count += self.experimentation_success\n",
    "\n",
    "    # How do they behave?\n",
    "    # How do agents change?\n",
    "    def maybe_join(self, other, filtered_ht_dict, interact_threshold = 1, model_num = None, verbose=False):\n",
    "\n",
    "\n",
    "        # Model num. This is just for ease of reproducibility. model_num being None means take the latest (probably most complex) ABM.\n",
    "\n",
    "        if model_num == 1:\n",
    "\n",
    "            # Simplest model. No \n",
    "            if self.supporting_metoo == False and \\\n",
    "                other.supporting_metoo == True and \\\n",
    "                self.interaction_counter[other.ID] > interact_threshold:\n",
    "\n",
    "                self.supporting_metoo = True\n",
    "\n",
    "                if verbose:\n",
    "                    print(f'Agent {self.ID} has spoken a lot to Agent {other.ID} and now supports {other.primary_ht}')\n",
    "\n",
    "        elif model_num == 2:\n",
    "\n",
    "            # Model 2:\n",
    "            # still have singular 'metoo' supporting\n",
    "            # ADD different primary language requirement.\n",
    "            if  self.supporting_metoo == False and \\\n",
    "                other.supporting_metoo == True and \\\n",
    "                other.primary_ht != self.primary_ht and \\\n",
    "                self.interaction_counter[other.ID] > interact_threshold:\n",
    "\n",
    "                self.supporting_metoo = True\n",
    "\n",
    "                if verbose:\n",
    "                    print(f'Agent {self.ID} has spoken a lot to Agent {other.ID} and now supports {other.primary_ht}')\n",
    "\n",
    "        elif model_num == 3:\n",
    "\n",
    "            # Model 3:\n",
    "            # still have singular 'metoo' supporting\n",
    "            # and different language requirement.\n",
    "            # ADD minimum reciprocal interaction\n",
    "            if  self.supporting_metoo == False and \\\n",
    "                other.supporting_metoo == True and \\\n",
    "                other.primary_ht != self.primary_ht and \\\n",
    "                self.interaction_counter[other.ID] > interact_threshold and \\\n",
    "                other.interaction_counter[self.ID] > interact_threshold:\n",
    "\n",
    "                self.supporting_metoo = True\n",
    "\n",
    "                if verbose:\n",
    "                    print(f'Agent {self.ID} has spoken a lot to Agent {other.ID} and now supports {other.primary_ht}')\n",
    "\n",
    "        elif model_num == 4:\n",
    "\n",
    "            # Model 4:\n",
    "            # ADD metoodict\n",
    "            # and different language requirement.\n",
    "            # and minimum reciprocal interaction.\n",
    "            #\n",
    "            # Now there is the possibility of those who are 'supporting metoo' already in one language to be influenced to support another language.\n",
    "            #\n",
    "            # This model only allows for one user to influence another on their primary hashtag.\n",
    "\n",
    "            # if  (self.supporting_metoo is False) and \\\n",
    "            if  (self.interaction_counter[other.ID] > interact_threshold) and \\\n",
    "                (other.primary_ht != self.primary_ht) and \\\n",
    "                (self.supporting_metoo_dict[other.primary_ht] == 0) and \\\n",
    "                (other.interaction_counter[self.ID] > interact_threshold):\n",
    "\n",
    "                self.supporting_metoo_dict[other.primary_ht] += 1\n",
    "\n",
    "                if verbose:\n",
    "                    print(f'Agent {self.ID} has spoken a lot to Agent {other.ID} and now supports {other.primary_ht}')\n",
    "\n",
    "        elif model_num == 5:\n",
    "\n",
    "            # Model 5:\n",
    "            # use metoodict\n",
    "            # and different language requirement.\n",
    "            # and minimum reciprocal interaction.\n",
    "            #\n",
    "            # ADD experimentation success\n",
    "            #\n",
    "            # This model only allows for one user to influence another on their primary hashtag.\n",
    "\n",
    "            # if  (self.supporting_metoo is False) and \\\n",
    "            if  (self.interaction_counter[other.ID] > interact_threshold) and \\\n",
    "                (other.primary_ht != self.primary_ht) and \\\n",
    "                (self.supporting_metoo_dict[other.primary_ht] == 0) and \\\n",
    "                (other.interaction_counter[self.ID] > interact_threshold or self.experimentation_success):\n",
    "\n",
    "                self.supporting_metoo_dict[other.primary_ht] += 1\n",
    "\n",
    "                if verbose:\n",
    "                    print(f'Agent {self.ID} has spoken a lot to Agent {other.ID} and now supports {other.primary_ht}')\n",
    "\n",
    "        elif model_num == 6:\n",
    "\n",
    "            # Model 6:\n",
    "            # use metoodict\n",
    "            # and different language requirement.\n",
    "            # and minimum reciprocal interaction.\n",
    "            #\n",
    "            # ADD ability to influence other users within your community too.\n",
    "            #\n",
    "            # This model only allows for one user to influence another on their primary hashtag.\n",
    "\n",
    "            # if  (self.supporting_metoo is False) and \\\n",
    "            if  (self.interaction_counter[other.ID] > interact_threshold) and \\\n",
    "                (other.primary_ht != self.primary_ht) and \\\n",
    "                (self.supporting_metoo_dict[other.primary_ht] == 0) and \\\n",
    "                (other.interaction_counter[self.ID] > interact_threshold):\n",
    "\n",
    "                self.supporting_metoo_dict[other.primary_ht] += 1\n",
    "\n",
    "                if verbose:\n",
    "                    print(f'Agent {self.ID} has spoken a lot to Agent {other.ID} and now supports {other.primary_ht}')\n",
    "\n",
    "            elif (self.interaction_counter[other.ID] > interact_threshold) and \\\n",
    "                 (other.primary_ht == self.primary_ht) and \\\n",
    "                 (other.interaction_counter[self.ID] > interact_threshold):\n",
    "\n",
    "                samplelist = [k for k,v in other.supporting_metoo_dict.items() if v>0]\n",
    "\n",
    "                if samplelist:\n",
    "                    # select a random value within\n",
    "                    sampled_ht_for_influence = random.choices(\n",
    "                        samplelist,\n",
    "                        [v for _,v in other.supporting_metoo_dict.items() if v>0],\n",
    "                        k=1\n",
    "                    )\n",
    "\n",
    "                    self.supporting_metoo_dict[sampled_ht_for_influence[0]] += 1\n",
    "\n",
    "                if verbose:\n",
    "                    print(f'Agent {self.ID} has influenced someone of their own primary ht community.')\n",
    "\n",
    "        elif model_num is None:\n",
    "            pass\n",
    "\n",
    "    def forget_all_interactions(self):\n",
    "        self.interaction_counter = defaultdict(int)\n",
    "\n",
    "    def forget_support_metoo(self):\n",
    "        self.supporting_metoo = False\n",
    "        self.supporting_metoo_dict = {i:0 for i in search_hashtags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23d59fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists and overwriting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12120/12120 [00:00<00:00, 53721.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating agents complete\n"
     ]
    }
   ],
   "source": [
    "# Assemble network of agents\n",
    "# agents = { ID: Agent(ID, df) for ID in df['author_id'].unique() }\n",
    "# this is extremely slow\n",
    "# %load_ext line_profiler\n",
    "\n",
    "# temp_groupby_for_agent_creation\n",
    "# temp_df = df.groupby('author_id').first()\n",
    "\n",
    "def produce_agents():\n",
    "    agents = {}\n",
    "    for user in tqdm.tqdm(list(df['author_id'].unique())):\n",
    "        agents[user] = Agent(user,temp_df, search_hashtags)\n",
    "\n",
    "    return agents\n",
    "\n",
    "# %lprun -f Agent.__init__ produce_agents()\n",
    "\n",
    "agents_overwrite = True\n",
    "agents_read_in = True\n",
    "agents_savepath = f'/home/hubert/DPhil_Studies/2021-04_Study_A_Diffusion/data/06_reporting/ABM_agents_group_{group_num}.obj'\n",
    "\n",
    "if os.path.isfile(agents_savepath) and agents_overwrite:\n",
    "    print('File exists and overwriting')\n",
    "    agents = produce_agents()\n",
    "    with open(agents_savepath, 'wb') as f:\n",
    "        pickle.dump(agents, f)\n",
    "elif os.path.isfile(agents_savepath) and agents_read_in:\n",
    "    print('reading in')\n",
    "    with open(agents_savepath, 'rb') as f:\n",
    "        agents = pickle.load(f)\n",
    "elif not os.path.isfile(agents_savepath):\n",
    "    print('producing agents for the first time')\n",
    "    agents = produce_agents()\n",
    "    with open(agents_savepath, 'wb') as f:\n",
    "        pickle.dump(agents, f)\n",
    "\n",
    "print('creating agents complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5602e0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, e in enumerate(list(agents[list(agents.keys())[0]].supporting_metoo_dict.keys())):\n",
    "    assert e == search_hashtags[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eec85731",
   "metadata": {},
   "outputs": [],
   "source": [
    "for user_id, agent in agents.items():\n",
    "    agent.forget_all_interactions()\n",
    "    agent.forget_support_metoo()\n",
    "for edge in G.edges():\n",
    "    i,j = edge\n",
    "    agents[i].interacts_with.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94f0bc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For instance, agent 12 interacts with ['2188044039', '266215308', '3023497329', '473782165', '4897361049', '80820758', '815500888634785792', '89784381', '932933255258615809', '947601090']\n"
     ]
    }
   ],
   "source": [
    "print( 'For instance, agent 12 interacts with', agents[list(agents.keys())[11]].interacts_with )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceb0e0f",
   "metadata": {},
   "source": [
    "# 2.5: Intermediate checks with likes std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f067d1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metoo': datetime.datetime(2018, 12, 12, 0, 0),\n",
       " 'moiaussi': datetime.datetime(2018, 12, 5, 0, 0),\n",
       " 'niunamenos': datetime.datetime(2018, 11, 25, 0, 0),\n",
       " 'noustoutes': datetime.datetime(2018, 11, 24, 0, 0),\n",
       " 'wetoo': datetime.datetime(2018, 12, 12, 0, 0),\n",
       " 'ятоже': datetime.datetime(2018, 11, 29, 0, 0),\n",
       " '米兔': datetime.datetime(2018, 11, 26, 0, 0)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_prominent_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c59d43f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check rt\n",
    "\n",
    "# df['likes_deviation'] = df.groupby('author_id')['likes'].transform(lambda x: abs(x - x.mean()) / x.std())\n",
    "# df['likes_std_2'] = df['likes_deviation'] > 2\n",
    "\n",
    "likes_df_temp = df.sort_values(by='created_at', ascending=True).groupby('author_id')['likes_std_2'].sum()\n",
    "\n",
    "# likes_df_temp_moiaussi = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4cc592",
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c357de2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intermediate: check correlation between likes std and activity tout court:\n",
    "\n",
    "# goal: correlation between activity in x days after first likes std above certain threshold\n",
    "\n",
    "act_val = {}\n",
    "for user_id, agent in df.itertuples():\n",
    "    # obtain user activity\n",
    "\n",
    "    act_val[user_id] = {}\n",
    "    with h5py.File(activity_file, 'r') as f:\n",
    "        activity = f[f'group_{group_num}'][user_id]['hashtagged'][:]\n",
    "        feature_order = f[f'group_{group_num}'][user_id]['hashtagged'].attrs['feature_order']\n",
    "        feature_order = feature_order.split(';')\n",
    "        # act_val[user_id] = np.sum(activity[:,-int(daterange_length/2):])\n",
    "\n",
    "        for hashtag_in_period in most_prominent_peaks:\n",
    "            hashtag_in_period_index = feature_order.index(hashtag_in_period)\n",
    "\n",
    "            # obtain the index offset from the detected peak of the hashtag to collect initial time window.\n",
    "            peak_index_index = (datetime.datetime.strptime(group_date_range.end, '%Y-%m-%d')-most_prominent_peaks[hashtag_in_period]).days\n",
    "            # offset_index -= peak_delta_init\n",
    "            # offset_index = max(0,offset_index)+1\n",
    "            # print(f'Offset for {hashtag_in_period} is {offset_index}')\n",
    "\n",
    "            act_val[user_id][hashtag_in_period_index]= np.sum(activity[hashtag_in_period_index,-peak_index_index-1:])\n",
    "\n",
    "act_val = pd.DataFrame.from_dict(act_val, orient='index').reset_index()\n",
    "act_val.columns = ['user_id'] + list(most_prominent_peaks.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669055a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_val['sum'] = act_val.iloc[:,1:].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7943f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_val_combined = act_val.merge(likes_df_temp, left_on='user_id', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ad7053",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_val_combined['likes_std_2'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3660635c",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_val_combined = act_val_combined[act_val_combined['sum']<1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde81374",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_val_combined['likes_std_2'].astype('float').corr(act_val_combined['sum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6351749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(act_val_combined['likes_std_2'].astype('float')+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910b22f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(np.log10(act_val_combined['likes_std_2'].astype('float')+1), np.log10(act_val_combined['sum']+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889d0cbe",
   "metadata": {},
   "source": [
    "# 3. Run the model\n",
    "\n",
    "First model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01672f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model parameters:\n",
    "\n",
    "# N.B. these thresholds are > not >=\n",
    "params = {\n",
    "    'interact_threshold': 1,\n",
    "    'initial_activity_threshold': 2,\n",
    "    'peak_delta_init': 7,\n",
    "    'model_num': 5,\n",
    "    'interact_prob': 0.7,\n",
    "    'interact_prob_multiplier': 1.1,\n",
    "    'experimentation_chance': 0.1,\n",
    "}\n",
    "\n",
    "\n",
    "search_hashtag_propensity = {\n",
    "    'metoo': 0.1,\n",
    "    'balancetonporc': 0.1,\n",
    "    'moiaussi': 0.1,\n",
    "    'نه_یعنی_نه': 0.1,\n",
    "    '米兔': 0.1,\n",
    "    '我也是': 0.1,\n",
    "    'gamani': 0.1,\n",
    "    'tôicũngvậy': 0.1,\n",
    "    '私も': 0.1,\n",
    "    'watashimo': 0.1,\n",
    "    '나도': 0.1,\n",
    "    '나도당했다': 0.1,\n",
    "    'גםאנחנו': 0.1,\n",
    "    'ятоже': 0.1,\n",
    "    'ricebunny': 0.1,\n",
    "    'enazeda': 0.1,\n",
    "    'anakaman': 0.1,\n",
    "    'yotambien': 0.1,\n",
    "    'sendeanlat': 0.1,\n",
    "    'kutoo': 0.1,\n",
    "    'withyou': 0.1,\n",
    "    'wetoo': 0.1,\n",
    "    'cuentalo': 0.1,\n",
    "    'quellavoltache': 0.1,\n",
    "    'niunamenos': 0.1,\n",
    "    'woyeshi': 0.1,\n",
    "    'myharveyweinstein': 0.1,\n",
    "    'noustoutes': 0.1,\n",
    "    'stilleforopptak': 0.1,\n",
    "    'nårdansenstopper': 0.1,\n",
    "    'nårmusikkenstilner': 0.1,\n",
    "    'memyös': 0.1,\n",
    "    'timesup': 0.1,\n",
    "    'niere': 0.1,\n",
    "    'jotambe': 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2236a809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 group \"/group_2\" (12586 members)>\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(activity_file, 'r') as f:\n",
    "    activity = f[f'group_{group_num}']\n",
    "    print(activity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f34397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_abm(agents, initial_activity_threshold, search_hashtag_propensity, peak_delta_init = 10):\n",
    "\n",
    "    # First, reset everyone's memory of their interactions\n",
    "    for user_id, agent in agents.items():\n",
    "        agent.forget_all_interactions()\n",
    "        agent.forget_support_metoo()\n",
    "        agent.propensity_params = search_hashtag_propensity\n",
    "\n",
    "\n",
    "    # Alternate second step: activate everyone above a certain activity threshold before the daterange. This activation will be done by primary language they have expressed some \n",
    "    pre_val = {}\n",
    "    for user_id, agent in agents.items():\n",
    "\n",
    "        pre_val[user_id] = {}\n",
    "\n",
    "        # obtain user activity\n",
    "        with h5py.File(activity_file, 'r') as f:\n",
    "            activity = f[f'group_{group_num}'][user_id]['hashtagged'][:]\n",
    "            feature_order = f[f'group_{group_num}'][user_id]['hashtagged'].attrs['feature_order']\n",
    "            feature_order = feature_order.split(';')\n",
    "\n",
    "            # obtain values for the hashtags that have peaks in this time period\n",
    "            for hashtag_in_period in most_prominent_peaks:\n",
    "                hashtag_in_period_index = feature_order.index(hashtag_in_period)\n",
    "\n",
    "                # obtain the index offset from the detected peak of the hashtag to collect initial time window.\n",
    "                offset_index = (most_prominent_peaks[hashtag_in_period] - datetime.datetime.strptime(group_date_range.start, '%Y-%m-%d')).days\n",
    "                offset_index -= peak_delta_init\n",
    "                offset_index = max(0,offset_index)+1\n",
    "                # print(f'Offset for {hashtag_in_period} is {offset_index}')\n",
    "\n",
    "                pre_val[user_id][hashtag_in_period_index]= np.sum(activity[hashtag_in_period_index,:offset_index])\n",
    "\n",
    "    pre_val = pd.DataFrame.from_dict(pre_val, orient='index').reset_index()\n",
    "    pre_val.columns = ['user_id'] + list(most_prominent_peaks.keys())\n",
    "\n",
    "    init_count = set()\n",
    "    for _, row in pre_val.iterrows():\n",
    "        for hashtag_in_period in most_prominent_peaks:\n",
    "            if row[hashtag_in_period] > initial_activity_threshold:\n",
    "                init_count.add(row['user_id'])\n",
    "                agent_in_question = agents[row['user_id']]\n",
    "                agent_in_question.supporting_metoo=True\n",
    "                agent_in_question.supporting_metoo_dict[hashtag_in_period] = True\n",
    "\n",
    "    print(f'Total initally set to support: {len(init_count)}')\n",
    "\n",
    "    return pre_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "655a64ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total initally set to support: 337\n"
     ]
    }
   ],
   "source": [
    "pre_val = reset_abm(agents,\n",
    "    initial_activity_threshold=params['initial_activity_threshold'],\n",
    "    search_hashtag_propensity=search_hashtag_propensity,\n",
    "    peak_delta_init=params['peak_delta_init']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a268febe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metoo         4862.0\n",
       "moiaussi        69.0\n",
       "niunamenos      42.0\n",
       "noustoutes     177.0\n",
       "wetoo          287.0\n",
       "ятоже            0.0\n",
       "米兔               0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_val.iloc[:,1:].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fb5b5448",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:29<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating...\n"
     ]
    }
   ],
   "source": [
    "def run_model(\n",
    "        agents,\n",
    "        params,\n",
    "        verbose = False\n",
    "    ):\n",
    "\n",
    "    for time in tqdm.tqdm(range(daterange_length)):\n",
    "        if verbose:\n",
    "            print(f'Starting interactions on day {time+1}')\n",
    "        for _, agent in agents.items():\n",
    "\n",
    "            # pick a random person that the agent interacts with\n",
    "            try:\n",
    "\n",
    "                other_agent = agents[np.random.choice(agent.interacts_with)]\n",
    "\n",
    "                # interact with them\n",
    "                if np.random.uniform()<=(params['interact_prob'])*params['interact_prob_multiplier']**(agent.interaction_counter[other_agent.ID]):\n",
    "                    agent.interact(other_agent, params['experimentation_chance'])\n",
    "\n",
    "                    # if you've interacted with them many times recently, say something\n",
    "                    agent.maybe_join(\n",
    "                        other_agent,\n",
    "                        filtered_ht_dict,\n",
    "                        params['interact_threshold'],\n",
    "                        model_num = params['model_num'],\n",
    "                        verbose=verbose)\n",
    "\n",
    "                agent.update_tracker()\n",
    "\n",
    "            except ValueError:\n",
    "\n",
    "                agent.update_tracker()\n",
    "                continue\n",
    "\n",
    "    return agents\n",
    "\n",
    "\n",
    "filtered_ht_dict = df.groupby('author_id')['ht'].agg(pd.Series.mode).to_dict()\n",
    "\n",
    "# some have multiple for mode, just take first one as an approximation now:\n",
    "for k,v in filtered_ht_dict.items():\n",
    "    if isinstance(v, np.ndarray):\n",
    "        filtered_ht_dict[k] = v[0]\n",
    "\n",
    "modelled_agents = run_model(\n",
    "            agents,\n",
    "            params,\n",
    "            verbose = False\n",
    "        )\n",
    "\n",
    "print('Simulating...')\n",
    "for _, agent in modelled_agents.items():\n",
    "    agent.simulate(search_hashtag_propensity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb12eef6",
   "metadata": {},
   "source": [
    "# 3.5 Checking simulation part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "359ec171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4516\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "nonzero = []\n",
    "for agent_id, agent in modelled_agents.items():\n",
    "    if agent.support_tracker.sum() > 0:\n",
    "        counter +=1\n",
    "        nonzero.append(agent_id)\n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "100f459d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1000030383969316865'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonzero[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "836540dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., 16.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0., 35.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelled_agents['1000030383969316865'].support_tracker.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1de6264",
   "metadata": {},
   "source": [
    "# 4. Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f58e1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_abm_results(agents_dict, model_num=None):\n",
    "\n",
    "    output_dict={}\n",
    "\n",
    "    total = len(agents_dict)\n",
    "    # num_supporting = 0\n",
    "    # num_not_supporting = 0\n",
    "\n",
    "    for user_id, agent in agents_dict.items():\n",
    "        output_dict[user_id] = agent.supporting_metoo_dict\n",
    "\n",
    "    output_df = pd.DataFrame.from_dict(output_dict, orient='index').reset_index()\n",
    "\n",
    "    num_supporting = output_df.iloc[:,1:].sum(axis=0)\n",
    "    num_supporting = num_supporting[num_supporting>0]\n",
    "    num_not_supporting = (output_df==False).sum(axis=0)\n",
    "\n",
    "    print(num_supporting)\n",
    "    # print(num_not_supporting)\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b7d124",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = print_abm_results(modelled_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239df559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with actual activity\n",
    "\n",
    "# define active at the end as having above 10? or somewhere along the distribution?\n",
    "\n",
    "act_val = {}\n",
    "for user_id, agent in agents.items():\n",
    "    # obtain user activity\n",
    "\n",
    "    act_val[user_id] = {}\n",
    "    with h5py.File(activity_file, 'r') as f:\n",
    "        activity = f[f'group_{group_num}'][user_id]['hashtagged'][:]\n",
    "        feature_order = f[f'group_{group_num}'][user_id]['hashtagged'].attrs['feature_order']\n",
    "        feature_order = feature_order.split(';')\n",
    "        # act_val[user_id] = np.sum(activity[:,-int(daterange_length/2):])\n",
    "\n",
    "        for hashtag_in_period in most_prominent_peaks:\n",
    "            hashtag_in_period_index = feature_order.index(hashtag_in_period)\n",
    "\n",
    "            # obtain the index offset from the detected peak of the hashtag to collect initial time window.\n",
    "            peak_index_index = (datetime.datetime.strptime(group_date_range.end, '%Y-%m-%d')-most_prominent_peaks[hashtag_in_period]).days\n",
    "            # offset_index -= peak_delta_init\n",
    "            # offset_index = max(0,offset_index)+1\n",
    "            # print(f'Offset for {hashtag_in_period} is {offset_index}')\n",
    "\n",
    "            act_val[user_id][hashtag_in_period_index]= np.sum(activity[hashtag_in_period_index,-peak_index_index-1:])\n",
    "\n",
    "act_val = pd.DataFrame.from_dict(act_val, orient='index').reset_index()\n",
    "act_val.columns = ['user_id'] + list(most_prominent_peaks.keys())\n",
    "\n",
    "# act_val = pd.DataFrame.from_dict(act_val, orient='index').reset_index()\n",
    "# act_val.columns = ['user_id', 'val']\n",
    "\n",
    "# (act_val['val']>10).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e02c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(act_val.iloc[:,1:]>0).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b672997",
   "metadata": {},
   "source": [
    "# 5. Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16e5bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[res['kutoo']>0]['kutoo'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415978dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[res['metoo']>0]['metoo'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf54878",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[res['kutoo']>0]['kutoo'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a5c7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[res['niunamenos']>0]['niunamenos'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85acf156",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents['92263198'].support_tracker[19,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cefca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_hashtags.index('kutoo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4229c314",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6470b2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ht, _ in most_prominent_peaks.items():\n",
    "    abm_res = res[res[ht]>0][['index', ht]]\n",
    "    act_res = act_val[act_val[ht]>3][['user_id',ht]]\n",
    "\n",
    "    overlap = set(abm_res['index']) & set(act_res['user_id'])\n",
    "\n",
    "    print(f'For {ht}: overlap = {len(overlap)}, total = {len(act_res)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9e980e",
   "metadata": {},
   "source": [
    "# 6. Graph Attributes\n",
    "\n",
    "* to help answer questions about whether or not paritcular users were key brokers of knowledge. cf. Sandra's work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebfedfa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36cf16204b8548560b1c020c4e8fb5b57f0e4c58016f52f2d4be01e192833930"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
