{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from typing import DefaultDict\n",
    "\n",
    "import pickle\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotnine\n",
    "from sklearn.metrics import cluster\n",
    "from sklearn.metrics import normalized_mutual_info_score as nmi_score\n",
    "\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "from time import time\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../src/d02_intermediate/')\n",
    "from generate_user_to_hashtag_matrix import TweetVocabVectorizer\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import SpectralCoclustering\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.cluster import v_measure_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/02_intermediate/01_group/'\n",
    "bsc_results = '../data/05_model_output/01_group/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_output_files = glob.glob(os.path.join(bsc_results, 'bsc_python*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hubert/.local/lib/python3.9/site-packages/sklearn/base.py:324: UserWarning: Trying to unpickle estimator SpectralCoclustering from version 0.24.2 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "/home/hubert/.local/lib/python3.9/site-packages/sklearn/base.py:324: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.24.2 when using version 1.0.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n"
     ]
    }
   ],
   "source": [
    "with open(python_output_files[0], 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(data_dir, 'vectorizer_ngram_23.obj'), 'rb') as f:\n",
    "    vectorizer = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'user_count_mat_ngram_23.obj'), 'rb') as f:\n",
    "    csr = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136377560"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_names.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "729"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(model.row_labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary is non-zero. Check OK.\n",
      "At least #MeToo hashtag is in vocab and has count 284920. Check OK.\n",
      "No eot_tokens found in vocabulary. Check OK.\n"
     ]
    }
   ],
   "source": [
    "def assert_helper(condition, true_msg='Condition Fulfilled', false_msg='Error'):\n",
    "    assert(condition), false_msg\n",
    "    print(true_msg)\n",
    "\n",
    "assert_helper(len(features_names)>0, 'Vocabulary is non-zero. Check OK.')\n",
    "\n",
    "metoosum = np.sum(['#metoo' in i for i in features_names])\n",
    "assert_helper(metoosum>0, 'At least #MeToo hashtag is in vocab and has count {}. Check OK.'.format(metoosum))\n",
    "\n",
    "elements_with_eot_token = np.sum(['eottoken' in i for i in features_names])\n",
    "assert_helper(elements_with_eot_token==0, 'No eot_tokens found in vocabulary. Check OK.', '{} counts of eot_token found in voabulary! Check NOT OK.'.format(elements_with_eot_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0160920023918152, 'gigabytes')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_bytes(size):\n",
    "    # 2**10 = 1024\n",
    "    power = 2**10\n",
    "    n = 0\n",
    "    power_labels = {0 : '', 1: 'kilo', 2: 'mega', 3: 'giga', 4: 'tera'}\n",
    "    while size > power:\n",
    "        size /= power\n",
    "        n += 1\n",
    "    return size, power_labels[n]+'bytes'\n",
    "\n",
    "\n",
    "format_bytes(features_names.itemsize*features_names.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bicluster_ncut(cocluster, csr, i):\n",
    "    rows, cols = cocluster.get_indices(i)\n",
    "    if not (np.any(rows) and np.any(cols)):\n",
    "        import sys\n",
    "\n",
    "        return sys.float_info.max\n",
    "    row_complement = np.nonzero(np.logical_not(cocluster.rows_[i]))[0]\n",
    "    col_complement = np.nonzero(np.logical_not(cocluster.columns_[i]))[0]\n",
    "    # Note: the following is identical to X[rows[:, np.newaxis],\n",
    "    # cols].sum() but much faster in scipy <= 0.16\n",
    "    weight = csr[rows][:, cols].sum()\n",
    "    # weight = csr[rows[:, np.newaxis],cols].sum()\n",
    "    cut = csr[row_complement][:, cols].sum() + csr[rows][:, col_complement].sum()\n",
    "    return cut / weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2007.2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bicluster_ncut(model, csr, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_normalizer(tokens):\n",
    "    \"\"\"Map all numeric tokens to a placeholder.\n",
    "\n",
    "    For many applications, tokens that begin with a number are not directly\n",
    "    useful, but the fact that such a token exists can be relevant.  By applying\n",
    "    this form of dimensionality reduction, some methods may perform better.\n",
    "    \"\"\"\n",
    "    return (\"#NUMBER\" if token[0].isdigit() else token for token in tokens)\n",
    "\n",
    "class NumberNormalizingVectorizer(TfidfVectorizer):\n",
    "    def build_tokenizer(self):\n",
    "        tokenize = super().build_tokenizer()\n",
    "        return lambda doc: list(number_normalizer(tokenize(doc)))\n",
    "\n",
    "# # exclude 'comp.os.ms-windows.misc'\n",
    "# categories = [\n",
    "#     \"alt.atheism\",\n",
    "#     \"comp.graphics\",\n",
    "#     \"comp.sys.ibm.pc.hardware\",\n",
    "#     \"comp.sys.mac.hardware\",\n",
    "#     \"comp.windows.x\",\n",
    "#     \"misc.forsale\",\n",
    "#     \"rec.autos\",\n",
    "#     \"rec.motorcycles\",\n",
    "#     \"rec.sport.baseball\",\n",
    "#     \"rec.sport.hockey\",\n",
    "#     \"sci.crypt\",\n",
    "#     \"sci.electronics\",\n",
    "#     \"sci.med\",\n",
    "#     \"sci.space\",\n",
    "#     \"soc.religion.christian\",\n",
    "#     \"talk.politics.guns\",\n",
    "#     \"talk.politics.mideast\",\n",
    "#     \"talk.politics.misc\",\n",
    "#     \"talk.religion.misc\",\n",
    "# ]\n",
    "# newsgroups = fetch_20newsgroups(categories=categories)\n",
    "# y_true = newsgroups.target\n",
    "\n",
    "# vectorizer = NumberNormalizingVectorizer(stop_words=\"english\", min_df=5)\n",
    "# cocluster = SpectralCoclustering(\n",
    "#     n_clusters=len(categories), svd_method=\"arpack\", random_state=0\n",
    "# )\n",
    "# kmeans = MiniBatchKMeans(n_clusters=len(categories), batch_size=20000, random_state=0)\n",
    "\n",
    "# print(\"Vectorizing...\")\n",
    "# X = vectorizer.fit_transform(newsgroups.data)\n",
    "\n",
    "# print(\"Coclustering...\")\n",
    "# start_time = time()\n",
    "# cocluster.fit(X)\n",
    "# y_cocluster = cocluster.row_labels_\n",
    "# print(\n",
    "#     \"Done in {:.2f}s. V-measure: {:.4f}\".format(\n",
    "#         time() - start_time, v_measure_score(y_cocluster, y_true)\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# print(\"MiniBatchKMeans...\")\n",
    "# start_time = time()\n",
    "# y_kmeans = kmeans.fit_predict(X)\n",
    "# print(\n",
    "#     \"Done in {:.2f}s. V-measure: {:.4f}\".format(\n",
    "#         time() - start_time, v_measure_score(y_kmeans, y_true)\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# feature_names = vectorizer.get_feature_names_out()\n",
    "# document_names = list(newsgroups.target_names[i] for i in newsgroups.target)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def most_common(d):\n",
    "    \"\"\"Items of a defaultdict(int) with the highest values.\n",
    "\n",
    "    Like Counter.most_common in Python >=2.7.\n",
    "    \"\"\"\n",
    "    return sorted(d.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "\n",
    "bicluster_ncuts = list(bicluster_ncut(i) for i in range()))\n",
    "best_idx = np.argsort(bicluster_ncuts)[:5]\n",
    "\n",
    "print()\n",
    "print(\"Best biclusters:\")\n",
    "print(\"----------------\")\n",
    "for idx, cluster in enumerate(best_idx):\n",
    "    n_rows, n_cols = model.get_shape(cluster)\n",
    "    cluster_docs, cluster_words = model.get_indices(cluster)\n",
    "    if not len(cluster_docs) or not len(cluster_words):\n",
    "        continue\n",
    "\n",
    "    # words\n",
    "    out_of_cluster_docs = model.row_labels_ != cluster\n",
    "    out_of_cluster_docs = np.where(out_of_cluster_docs)[0]\n",
    "    word_col = csr[:, cluster_words]\n",
    "    word_scores = np.array(\n",
    "        word_col[cluster_docs, :].sum(axis=0)\n",
    "        - word_col[out_of_cluster_docs, :].sum(axis=0)\n",
    "    )\n",
    "    word_scores = word_scores.ravel()\n",
    "    important_words = list(\n",
    "        features_names[cluster_words[i]] for i in word_scores.argsort()[:-11:-1]\n",
    "    )\n",
    "\n",
    "    print(\"bicluster {} : {} documents, {} words\".format(idx, n_rows, n_cols))\n",
    "    print(\"words        : {}\\n\".format(\", \".join(important_words)))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36cf16204b8548560b1c020c4e8fb5b57f0e4c58016f52f2d4be01e192833930"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
