# 2021-06-14, 2021-06-15, 2021-06-16
* BSC eval and graphs are working fine. Now working on some of the stuff relating to the 2021-06-08 TODOs
* Realisation after checking the BSC pdf results: hashtags collected and used in vocab also count non-hashtag use of them.
* Also TODO:
  - [x] Fix https removal
	  - [x] attempt made at 15.09 to replace regex with ''\shttps?:\/\/\S*"
  - [x] Fix ellipses removal 
  - [ ] Add method to BSC class to generate top words per cluster exmaination
  - [ ] merge users and summaries bsc attributes to allow for sorting of phrases by usage.
  - [x] properly do vocab to not pick up other instances or hashtags
  - [ ] Omit clusters with only one user
  - [x] Put data into a networkx graph?
	  - [x] as of today, not worth it. It would be better to figure out how to skip the csv writing stage and just get bsc R script to just handle an adjacency matrix.
  - [x] Add title to ggplots
  - [ ] Allow BSC class to store points to collect clusters of interests
  - [ ] Allow BSC class to generate list of users and stats based on selected clusters
  - [ ] Plot usage of phrases across timeframe
  - [ ] Basic metrics of interaction between users in a cluster in the given timeframe.
  - [ ] How to subsequently model the diffusion
  - [ ] How to scale up to full data collection??
    -    How to sample users?
    -    Limiting parameters?
  - [x] stop hashtag removal in the R script.
  - [x] plot distribution of clusters by users
  - [ ] Simply dont count the ngrams crossing tweets by using a custom analyzer
  - [ ] zoom in on a cluster of users 
  - [x] clear out #................................. stuff fromvocab
		* '#......' is not in the hashtag set of vocabulary, because it's an encoding error with ggplot writing to pdfs (and plots more generally probably)
  - [ ] 
# 2021-06-08, 2021-06-10

* Today I am trying to first get rid of the bug where the vocab is not properly rendering
  * CountVectorizer is not properly counting eot_token for example. Confirmed when tokens (generated by the first CountVectorizer instance, remember!) for a column corresponding to an eot_token.
  * Tried lowercasing the vocabulary but that didn't seem to work.
  * Tried setting lowercase to True in second CountVectorier and also removing < and > surrounding eot_token
  * It looks like from https://stackoverflow.com/questions/24007812/can-i-control-the-way-the-countvectorizer-vectorizes-the-corpus-in-scikit-learn?rq=1 that the issue is the second vectorizer needs to have ngram_range too. Going to re-add < > around eot_token and remove lowercase=True from the second CountVectorizer.
  * this should have worked
* Next, work on BSC R script. Desiderata:
  - [x] apply NMI in python search script (normalised mutual information) like the BSC script has
    * see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.normalized_mutual_info_score.html
  - [x] How to get the material I want into Python from R
  - [x] How do I want to visualise these?
  - [ ] Applying on a subset?
* Also TODO
  - [ ] How to qualitatively pick out phrases
  - [ ] How to subsequently model the diffusion
  - [ ]  How to scale up to full data collection??
    - [ ]  How to sample users?
    - [ ]  Limiting parameters?
  - [x]  Resolve issue about bsc warnings about non-english labelling errors (encoding error)
    * error has been suppressed, in util.R
  - [x] 'null device' printing also suppressed from dev.off()
  - [x] attempted to make parallel the bsc gridsearch but all subprocesses are already parallel, make sense!

# 2021-06-09

* BSC eval is working, but original work had (I think) user NMI score but we can probably also do the same with phrase assignment which will also be useful measure
  * working on this early aft
* now need to add material on getting the max index kind of thing.
- [x] Adding comments to BSC functions

