

# 2021-08-03
Things to control for in statistical analysis:
* activity, average tweeting per day.
* usage of certain 'neutral' phrases, like 'tout le monde'
* matched users
* really useful to have matched users in tracing something up to a point in time.
	* Null hypothesis is not just a flat line. But what kind of a drop off?
	* Generate expectation based on matched users. Tenure, level of activity (on Wiki)
	* just significant activiation in peak in subsequent. compared to matched userset
	* just as I don't have to mass media or prior twitter or social media influences
* burstiness - Barabasi (read) - Nature paper. Also his book - Bursts. Need to take this into account
	* don't expect sometime to pick up a term and S curve. But it's bursty. Power law. Fat-tailed. Need to expect that. Probably a lot of log transform. Statistically difficult, however.


Thoughts
* Since last time
	* created better plots
	* corrected clustering implementation which will take significantly longer, which is fine
	* working on network viz. I think I can get distance measure on the connections between users/phrases across clusters that will exist. The more connections the closer in 2d space.
	* But need to filter out nodes because there are just way too many to plot meaningfully. thinking I will drop the tail end of frequency within each cluster and plot.
	* full archive search collection is still happening. Up to early 2020 now, want to get to Oct 2020 for a neat 3 years.
* Phil mentioned about Twitter person possibly being in contact about data dump, but no response from Phil yet.
* Question: for any qualitative analyses I want to pick out, I'll probably need someone to consult on with the phrasing. What's the best way to go about doing this? I think Siân would be excellent, for example.
* need longer than a week. need longer timespans, maybe fewer users
* need to somehow incorporate interaction anaylsis. Seems clustering on language usage is not often returning internal retweets. Need to check this
* Also interesting that within clusters retweets are not common. Perhaps I need to take this into account directly more.

# 2021-07-21 Logs
* Realisation: even if #metoo itself is only contained within one cluster, a lot of ngrams containing it will appear in other clusters.

# 2021-07-12 Problems with python implementation
* in order to get a decent graphviz we want a graph object. Right? But the R implementation obviously doesn't allow for that.
* But it might - there are matrices involved in there for sure, and how can we visualise these?
* uhtMat is the last matrix object that is then kmeans clustered. I can change what the outputs of these are.
* Idea 2021-07-13: take matrix out of function and then visualise using that matrix.
* Q: how is it that there are only 832 items/phrases?
* THE FUCKING GRAPH ADJACENCY MATRIX THEY HAD DID NOT TRANSFER EDGE WEIGHT
* python: getting a NaN warning from sklearn. Does input need to be dense?
	* nope.
	* perhaps need to drop rows that are 0?
	* adding small diagonal can help better condition the matrix https://stackoverflow.com/questions/18754324/improving-a-badly-conditioned-matrix

* see this for bipartite viz: https://stackoverflow.com/questions/60100006/visualize-bipartite-network-graph-created-using-pandas-dataframe

* also, how to convert between unipartite and bipartite graph type in R:
	* just add a $type attribute

* also, error and warning capture included now for bispec search

# 2021-06-30 Graph Visualisation
* The key problem here is that the graph, when partititioned, does not have multiple assignment. So there will be no overlap in users and phrases assigned across clusters, and a visualisation should have some kind of distance measure. What to do with this?
	* Yesterday I found https://github.com/Nicola17/High-Dimensional-Inspector
	* Perhaps links between users and phrases across clusters, those will still exist
	* but still need to make appropriate adjacency matrix after clustering. How to do?
	* the clustering _is_ a form of rearranging rows of an adjacency matrix. So perhaps don't use the R implementation? Found a python one, but was written in Python 2. need to check if still working.
	* from [@steinbockCasualVisualExploration2018], Common strategies to visualize large graphs – and large data sets in general – are ﬁltering (i.e., removing items) and aggregation (i.e., grouping items)
	* https://scikit-learn.org/stable/modules/biclustering.html biclustering is implemented in sklearn!!!


# 2021-06-21 meeting with Balazs

* FAS collection used this month - 10m for 6 months of tweets.
* BSC results up and running, I can send you the files
* Things suggested to work on:
	* I now have a couple months of FAS on a fuller set of collection data
	* I can start more systematically looking through accounts with clustering information to (a) control for word use to decode who is an activist or not
		* (b) prepare sampling of users for July collection.
---
* use individal plots of course
* strength of dyad
	* follow collaborator
	* normalise histories where a bot was introduced and where a bot was not introduced.
	* counterfactual comparison
	* marginals like account same age, collaboration intensity, normalise out seasonality dynamics, approximate true effect of introduction of bot.
	* seasonality probably not relevant here.

* we do expect an increase across metoo but activist should but compared to what?
* DO VISUALISATION on SPACE for sense of regions
	* correspondence analysis
	* pca, non metric muiltidimensional scaling MDS, correspondnece (bipartite)
	* e.g. measure distance from one cluster to another cluster
	* same measure generating clusters can be used to get distance

* baselines comparisons IMPORTANT!
	* need COUNTERFACTUALS

* distance metric between clusters!
* hashtag pool use
* hashtag cofrequency
* aggregate cofrequency link between clusters